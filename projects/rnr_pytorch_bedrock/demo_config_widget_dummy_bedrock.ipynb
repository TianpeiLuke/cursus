{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Pipeline Configuration with DAGConfigFactory\n",
    "\n",
    "This notebook demonstrates the new interactive approach to pipeline configuration using the DAGConfigFactory.\n",
    "Instead of manually creating 500+ lines of static configuration, we use a guided step-by-step process.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Define Pipeline DAG** - Create the pipeline structure\n",
    "2. **Initialize DAGConfigFactory** - Set up the interactive factory\n",
    "3. **Configure Base Settings** - Set shared pipeline configuration\n",
    "4. **Configure Processing Settings** - Set shared processing configuration\n",
    "5. **Configure Individual Steps** - Set step-specific configurations\n",
    "6. **Generate Final Configurations** - Create config instances\n",
    "7. **Save to JSON** - Export unified configuration file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines\n",
      "add project root /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines into system\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date\n",
    "import logging\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get parent directory of current notebook\n",
    "project_root = str(Path().absolute().parent)\n",
    "print(f\"project root {project_root}\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"add project root {project_root} into system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:00,623 - INFO - Note: NumExpr detected 48 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2025-11-05 22:49:00,624 - INFO - NumExpr defaulting to 8 threads.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "2025-11-05 22:49:01,759 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# SageMaker and SAIS imports\n",
    "from sagemaker import Session\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:02,105 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default\n"
     ]
    }
   ],
   "source": [
    "print(f\"Role: {PipelineSession().get_caller_identity_arn()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: buyer-seller-messaging-reversal\n"
     ]
    }
   ],
   "source": [
    "bucket = \"buyer-seller-messaging-reversal\"\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Pipeline DAG\n",
    "\n",
    "First, we define the pipeline structure using a DAG (Directed Acyclic Graph).\n",
    "This replaces the hardcoded pipeline structure from the legacy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:02,633 - WARNING - Could not import constants from mods_workflow_core, using local definitions\n",
      "2025-11-05 22:49:02,993 - INFO - Added node: BedrockPromptTemplateGeneration\n",
      "2025-11-05 22:49:02,994 - INFO - Created Bedrock Batch data processing DAG with 1 nodes and 0 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline DAG created with 1 steps:\n",
      "  - BedrockPromptTemplateGeneration\n"
     ]
    }
   ],
   "source": [
    "from cursus.api.dag.base_dag import PipelineDAG\n",
    "\n",
    "\n",
    "def create_bedrock_batch_data_processing_dag() -> PipelineDAG:\n",
    "    \"\"\"\n",
    "    Create a DAG for Bedrock Batch data processing pipeline.\n",
    "\n",
    "    This DAG represents the simplest possible workflow that includes\n",
    "    cost-efficient Bedrock batch LLM enhancement for pure data processing\n",
    "    without any training, calibration, packaging, registration, or evaluation steps.\n",
    "    Perfect for data enhancement and annotation workflows.\n",
    "\n",
    "    Returns:\n",
    "        PipelineDAG: The directed acyclic graph for the pipeline\n",
    "    \"\"\"\n",
    "    dag = PipelineDAG()\n",
    "\n",
    "    # Add minimal data processing nodes with Bedrock batch enhancement\n",
    "    # dag.add_node(\"DummyDataLoading_training\")  # Dummy data load\n",
    "    # dag.add_node(\"TabularPreprocessing_training\")  # Tabular preprocessing\n",
    "    dag.add_node(\n",
    "        \"BedrockPromptTemplateGeneration\"\n",
    "    )  # Bedrock prompt template generation\n",
    "    # dag.add_node(\"BedrockBatchProcessing_training\")  # Bedrock batch processing step\n",
    "\n",
    "    # Simple data processing flow with Bedrock batch enhancement\n",
    "    # dag.add_edge(\"DummyDataLoading_training\", \"TabularPreprocessing_training\")\n",
    "\n",
    "    # Bedrock batch processing flow - two inputs to BedrockBatchProcessing\n",
    "    # dag.add_edge(\"TabularPreprocessing_training\", \"BedrockBatchProcessing_training\")  # Data input\n",
    "    # dag.add_edge(\"BedrockPromptTemplateGeneration\", \"BedrockBatchProcessing_training\")  # Template input\n",
    "\n",
    "    logger.info(\n",
    "        f\"Created Bedrock Batch data processing DAG with {len(dag.nodes)} nodes and {len(dag.edges)} edges\"\n",
    "    )\n",
    "    return dag\n",
    "\n",
    "\n",
    "# Create the pipeline DAG\n",
    "dag = create_bedrock_batch_data_processing_dag()\n",
    "\n",
    "print(f\"Pipeline DAG created with {len(dag.nodes)} steps:\")\n",
    "for node in dag.nodes:\n",
    "    print(f\"  - {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize DAGConfigFactory\n",
    "\n",
    "Now we initialize the DAGConfigFactory with our DAG. This will automatically:\n",
    "- Map DAG nodes to configuration classes\n",
    "- Set up the interactive workflow\n",
    "- Prepare for step-by-step configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,000 - INFO - üîß BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-05 22:49:03,001 - INFO - üîß BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-05 22:49:03,002 - INFO - ‚úÖ BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-05 22:49:03,002 - INFO - ‚úÖ Registry info loaded: 34 steps\n",
      "2025-11-05 22:49:03,002 - INFO - üéâ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-05 22:49:03,003 - INFO - üîç ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-05 22:49:03,003 - INFO - üîç ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-05 22:49:03,003 - INFO - üîç ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-05 22:49:03,004 - INFO - ‚úÖ Registry info loaded: 34 steps\n",
      "2025-11-05 22:49:03,004 - INFO - üéâ ScriptAutoDiscovery initialization completed successfully\n",
      "2025-11-05 22:49:03,118 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:03,135 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:03,163 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:03,163 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:03,163 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:03,164 - INFO - ‚úÖ Mapped 'BedrockPromptTemplateGeneration' -> 'BedrockPromptTemplateGeneration' -> BedrockPromptTemplateGenerationConfig\n",
      "2025-11-05 22:49:03,164 - INFO - Successfully mapped 1/1 DAG nodes to config classes\n",
      "2025-11-05 22:49:03,165 - INFO - Initialized DAGConfigFactory for DAG with 1 steps using robust step detection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAG Node to Config Class Mapping:\n",
      "==================================================\n",
      "  BedrockPromptTemplateGeneration     -> BedrockPromptTemplateGenerationConfig\n",
      "\n",
      "Successfully mapped 1 steps to configuration classes.\n"
     ]
    }
   ],
   "source": [
    "from cursus.api.factory.dag_config_factory import DAGConfigFactory\n",
    "\n",
    "# Initialize the factory with our DAG\n",
    "factory = DAGConfigFactory(dag)\n",
    "\n",
    "# Get the config class mapping\n",
    "config_map = factory.get_config_class_map()\n",
    "\n",
    "print(\"DAG Node to Config Class Mapping:\")\n",
    "print(\"=\" * 50)\n",
    "for node_name, config_class in config_map.items():\n",
    "    print(f\"  {node_name:<35} -> {config_class.__name__}\")\n",
    "\n",
    "print(f\"\\nSuccessfully mapped {len(config_map)} steps to configuration classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Base Pipeline Settings\n",
    "\n",
    "These settings are shared across ALL pipeline steps. Instead of repeating them\n",
    "in every step configuration, we set them once here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Pipeline Configuration Requirements:\n",
      "==================================================\n",
      "* author                    (str)\n",
      "    Author or owner of the pipeline.\n",
      "\n",
      "* bucket                    (str)\n",
      "    S3 bucket name for pipeline artifacts and data.\n",
      "\n",
      "* role                      (str)\n",
      "    IAM role for pipeline execution.\n",
      "\n",
      "* region                    (str)\n",
      "    Custom region code (NA, EU, FE) for internal logic.\n",
      "\n",
      "* service_name              (str)\n",
      "    Service name for the pipeline.\n",
      "\n",
      "* pipeline_version          (str)\n",
      "    Version string for the SageMaker Pipeline.\n",
      "\n",
      "  model_class               (str) (default: xgboost)\n",
      "    Model class (e.g., XGBoost, PyTorch).\n",
      "\n",
      "  current_date              (str) (default: PydanticUndefined)\n",
      "    Current date, typically used for versioning or pathing.\n",
      "\n",
      "  framework_version         (str) (default: 2.1.0)\n",
      "    Default framework version (e.g., PyTorch).\n",
      "\n",
      "  py_version                (str) (default: py310)\n",
      "    Default Python version.\n",
      "\n",
      "  source_dir                (Optional) (default: None)\n",
      "    Common source directory for scripts if applicable. Can be overridden by step configs.\n",
      "\n",
      "  enable_caching            (bool) (default: False)\n",
      "    Enable caching for pipeline steps.\n",
      "\n",
      "* project_root_folder       (str)\n",
      "    Root folder name for the user's project (required for hybrid resolution)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get base configuration requirements\n",
    "base_requirements = factory.get_base_config_requirements()\n",
    "\n",
    "print(\"Base Pipeline Configuration Requirements:\")\n",
    "print(\"=\" * 50)\n",
    "for req in base_requirements:\n",
    "    marker = \"*\" if req[\"required\"] else \" \"\n",
    "    default_info = (\n",
    "        f\" (default: {req.get('default')})\"\n",
    "        if not req[\"required\"] and \"default\" in req\n",
    "        else \"\"\n",
    "    )\n",
    "    print(f\"{marker} {req['name']:<25} ({req['type']}){default_info}\")\n",
    "    print(f\"    {req['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,194 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "2025-11-05 22:49:03,545 - INFO - Base configuration set successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base pipeline configuration set successfully!\n",
      "   Region: NA (us-east-1)\n",
      "   Service: BuyerAbuseRnR\n",
      "   Author: lukexie\n",
      "   Pipeline Version: 0.0.1\n"
     ]
    }
   ],
   "source": [
    "# Set up basic configuration values\n",
    "region_list = [\"NA\", \"EU\", \"FE\"]\n",
    "region_selection = 0\n",
    "region = region_list[region_selection]\n",
    "\n",
    "# Map region to AWS region\n",
    "region_mapping = {\"NA\": \"us-east-1\", \"EU\": \"eu-west-1\", \"FE\": \"us-west-2\"}\n",
    "aws_region = region_mapping[region]\n",
    "\n",
    "service_name = \"BuyerAbuseRnR\"\n",
    "pipeline_version = \"0.0.1\"\n",
    "author = \"lukexie\"\n",
    "model_class = \"pytorch\"\n",
    "\n",
    "# Get current directory and set up paths\n",
    "current_dir = Path.cwd()\n",
    "package_root = Path(current_dir).resolve()\n",
    "source_dir = Path(\"docker\")\n",
    "project_root_folder = \"rnr_pytorch_bedrock\"\n",
    "\n",
    "# Set base configuration\n",
    "factory.set_base_config(\n",
    "    # Infrastructure settings\n",
    "    bucket=bucket,\n",
    "    role=PipelineSession().get_caller_identity_arn(),\n",
    "    region=region,\n",
    "    aws_region=aws_region,\n",
    "    # Project identification\n",
    "    author=author,\n",
    "    service_name=service_name,\n",
    "    pipeline_version=pipeline_version,\n",
    "    model_class=model_class,\n",
    "    # Framework settings\n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py310\",\n",
    "    source_dir=str(source_dir),\n",
    "    project_root_folder=project_root_folder,\n",
    "    # Date settings\n",
    "    current_date=date.today().strftime(\"%Y-%m-%d\"),\n",
    "    # Enable Cache\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Base pipeline configuration set successfully!\")\n",
    "print(f\"   Region: {region} ({aws_region})\")\n",
    "print(f\"   Service: {service_name}\")\n",
    "print(f\"   Author: {author}\")\n",
    "print(f\"   Pipeline Version: {pipeline_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Base Processing Settings\n",
    "\n",
    "These settings are shared across all PROCESSING steps (data loading, preprocessing, etc.)\n",
    "but not training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Processing Configuration Requirements:\n",
      "==================================================\n",
      "  processing_instance_count      (int) (default: 1)\n",
      "    Instance count for processing jobs\n",
      "\n",
      "  processing_volume_size         (int) (default: 500)\n",
      "    Volume size for processing jobs in GB\n",
      "\n",
      "  processing_instance_type_large (str) (default: ml.m5.4xlarge)\n",
      "    Large instance type for processing step.\n",
      "\n",
      "  processing_instance_type_small (str) (default: ml.m5.2xlarge)\n",
      "    Small instance type for processing step.\n",
      "\n",
      "  use_large_processing_instance  (bool) (default: False)\n",
      "    Set to True to use large instance type, False for small instance type.\n",
      "\n",
      "  processing_source_dir          (Optional) (default: None)\n",
      "    Source directory for processing scripts. Falls back to base source_dir if not provided.\n",
      "\n",
      "  processing_entry_point         (Optional) (default: None)\n",
      "    Entry point script for processing, must be relative to source directory. Can be overridden by derived classes.\n",
      "\n",
      "  processing_script_arguments    (Optional) (default: None)\n",
      "    Optional arguments for the processing script.\n",
      "\n",
      "  processing_framework_version   (str) (default: 1.2-1)\n",
      "    Version of the scikit-learn framework to use in SageMaker Processing. Format: '<sklearn-version>-<build-number>'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get base processing configuration requirements\n",
    "processing_requirements = factory.get_base_processing_config_requirements()\n",
    "\n",
    "if processing_requirements:\n",
    "    print(\"Base Processing Configuration Requirements:\")\n",
    "    print(\"=\" * 50)\n",
    "    for req in processing_requirements:\n",
    "        marker = \"*\" if req[\"required\"] else \" \"\n",
    "        default_info = (\n",
    "            f\" (default: {req.get('default')})\"\n",
    "            if not req[\"required\"] and \"default\" in req\n",
    "            else \"\"\n",
    "        )\n",
    "        print(f\"{marker} {req['name']:<30} ({req['type']}){default_info}\")\n",
    "        print(f\"    {req['description']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No base processing configuration required for this pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,559 - INFO - Package location discovery succeeded (bundled): /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers\n",
      "2025-11-05 22:49:03,559 - INFO - Hybrid resolution completed successfully via Package Location Discovery: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers\n",
      "2025-11-05 22:49:03,560 - INFO - Base processing configuration set successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base processing configuration set successfully!\n",
      "   Processing source: dockers/scripts\n"
     ]
    }
   ],
   "source": [
    "# Set base processing configuration if needed\n",
    "if processing_requirements:\n",
    "    processing_source_dir = source_dir / \"scripts\"\n",
    "\n",
    "    factory.set_base_processing_config(\n",
    "        # Processing infrastructure\n",
    "        processing_source_dir=str(processing_source_dir),\n",
    "        processing_instance_type_large=\"ml.m5.12xlarge\",\n",
    "        processing_instance_type_small=\"ml.m5.4xlarge\",\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Base processing configuration set successfully!\")\n",
    "    print(f\"   Processing source: {processing_source_dir}\")\n",
    "else:\n",
    "    print(\"‚úÖ No base processing configuration needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Check Configuration Status\n",
    "\n",
    "Let's see which steps still need configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Status:\n",
      "==============================\n",
      "Base config set: ‚úÖ\n",
      "Processing config set: ‚úÖ\n",
      "Total steps: 1\n",
      "Pending steps: 1\n",
      "\n",
      "Steps needing configuration:\n",
      "  - BedrockPromptTemplateGeneration\n"
     ]
    }
   ],
   "source": [
    "# Check current status\n",
    "status = factory.get_configuration_status()\n",
    "pending_steps = factory.get_pending_steps()\n",
    "\n",
    "print(\"Configuration Status:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Base config set: {'‚úÖ' if status['base_config'] else '‚ùå'}\")\n",
    "print(f\"Processing config set: {'‚úÖ' if status['base_processing_config'] else '‚ùå'}\")\n",
    "print(f\"Total steps: {len(config_map)}\")\n",
    "print(f\"Pending steps: {len(pending_steps)}\")\n",
    "print()\n",
    "\n",
    "if pending_steps:\n",
    "    print(\"Steps needing configuration:\")\n",
    "    for step in pending_steps:\n",
    "        print(f\"  - {step}\")\n",
    "else:\n",
    "    print(\"‚úÖ All steps configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Individual Steps\n",
    "\n",
    "Now we configure each step with its specific requirements. The factory will show us\n",
    "only the fields that are unique to each step (not inherited from base configs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1 Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BedrockPromptTemplateGeneration Config Requirements:\n",
      "==================================================\n",
      "* input_placeholders             (List)\n",
      "    List of input field names to include in the template (e.g., ['input_data', 'context', 'metadata'])\n",
      "\n",
      "* prompt_configs_path            (str)\n",
      "    Path to prompt configuration directory containing system_prompt.json, output_format.json, instruction.json, and category_definitions.json files, relative to processing source directory\n",
      "\n",
      "  template_task_type             (str) (default: classification)\n",
      "    Type of task for template generation (classification, sentiment_analysis, content_moderation)\n",
      "\n",
      "  template_style                 (str) (default: structured)\n",
      "    Style of template generation (structured, conversational, technical)\n",
      "\n",
      "  validation_level               (str) (default: standard)\n",
      "    Level of template validation (basic, standard, comprehensive)\n",
      "\n",
      "  output_format_type             (str) (default: structured_json)\n",
      "    Type of output format (structured_json, formatted_text, hybrid)\n",
      "\n",
      "  required_output_fields         (List) (default: ['category', 'confidence', 'key_evidence', 'reasoning'])\n",
      "    List of required fields in the output format\n",
      "\n",
      "  include_examples               (bool) (default: True)\n",
      "    Include examples in the generated template\n",
      "\n",
      "  generate_validation_schema     (bool) (default: True)\n",
      "    Generate JSON validation schema for downstream use\n",
      "\n",
      "  template_version               (str) (default: 1.0)\n",
      "    Version identifier for the generated template\n",
      "\n",
      "  system_prompt_settings         (Optional) (default: None)\n",
      "    System prompt configuration with comprehensive defaults\n",
      "\n",
      "  output_format_settings         (Optional) (default: None)\n",
      "    Output format configuration with comprehensive defaults\n",
      "\n",
      "  instruction_settings           (Optional) (default: None)\n",
      "    Instruction configuration with comprehensive defaults\n",
      "\n",
      "  category_definitions           (Optional) (default: None)\n",
      "    List of category definitions for template generation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_requirements = factory.get_step_requirements(\"BedrockPromptTemplateGeneration\")\n",
    "\n",
    "if config_requirements:\n",
    "    print(\"BedrockPromptTemplateGeneration Config Requirements:\")\n",
    "    print(\"=\" * 50)\n",
    "    for req in config_requirements:\n",
    "        marker = \"*\" if req[\"required\"] else \" \"\n",
    "        default_info = (\n",
    "            f\" (default: {req.get('default')})\"\n",
    "            if not req[\"required\"] and \"default\" in req\n",
    "            else \"\"\n",
    "        )\n",
    "        print(f\"{marker} {req['name']:<30} ({req['type']}){default_info}\")\n",
    "        print(f\"    {req['description']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No base processing configuration required for this pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cursus.steps.configs.config_bedrock_prompt_template_generation_step import (\n",
    "    BedrockPromptTemplateGenerationConfig,\n",
    "    SystemPromptConfig,\n",
    "    OutputFormatConfig,\n",
    "    InstructionConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_definitions_path = package_root / \"dockers\" / \"prompt_configs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_definitions_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_settings = SystemPromptConfig(\n",
    "    role_definition=\"expert in analyzing buyer-seller messaging conversations and shipping logistics\",\n",
    "    expertise_areas=[\n",
    "        \"buyer-seller messaging analysis\",\n",
    "        \"shipping logistics\",\n",
    "        \"delivery timing analysis\",\n",
    "        \"e-commerce dispute resolution\",\n",
    "        \"classification and categorization\",\n",
    "    ],\n",
    "    responsibilities=[\n",
    "        \"classify interactions based on message content\",\n",
    "        \"analyze shipping events and delivery timing\",\n",
    "        \"categorize into predefined dispute categories\",\n",
    "        \"provide evidence-based reasoning for classifications\",\n",
    "    ],\n",
    "    behavioral_guidelines=[\n",
    "        \"be precise in classification decisions\",\n",
    "        \"be objective in evidence evaluation\",\n",
    "        \"be thorough in timeline analysis\",\n",
    "        \"follow exact formatting requirements\",\n",
    "        \"consider all available evidence sources\",\n",
    "    ],\n",
    "    tone=\"professional\",  # Options: \"professional\", \"casual\", \"technical\", \"formal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_format_settings = OutputFormatConfig(\n",
    "    format_type=\"structured_text\",\n",
    "    header_text=\"**CRITICAL: Follow this exact format for automated parsing**\",\n",
    "    structured_text_sections=[\n",
    "        {\n",
    "            \"number\": 1,\n",
    "            \"header\": \"Category\",\n",
    "            \"format\": \"single_value\",\n",
    "            \"placeholder\": \"${category_enum}\",  # Auto-resolved from category definitions\n",
    "            \"placeholder_source\": \"schema_enum\",\n",
    "        },\n",
    "        {\n",
    "            \"number\": 2,\n",
    "            \"header\": \"Confidence Score\",\n",
    "            \"format\": \"single_value\",\n",
    "            \"placeholder\": \"${numeric_range}\",  # Auto-resolved from schema constraints\n",
    "            \"placeholder_source\": \"schema_range\",\n",
    "        },\n",
    "        {\n",
    "            \"number\": 3,\n",
    "            \"header\": \"Key Evidence\",\n",
    "            \"format\": \"subsections\",\n",
    "            \"item_prefix\": \"- \",\n",
    "            \"indent\": \"   \",\n",
    "            \"subsections\": [\n",
    "                {\n",
    "                    \"name\": \"Message Evidence\",\n",
    "                    \"example_items\": [\n",
    "                        \"- [BUYER]: Example message\",\n",
    "                        \"- [SELLER]: Example response\",\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Shipping Evidence\",\n",
    "                    \"example_items\": [\"- [Event]: Delivered\"],\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Timeline Evidence\",\n",
    "                    \"example_items\": [\"- Delivery on 2025-02-21\"],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"number\": 4,\n",
    "            \"header\": \"Reasoning\",\n",
    "            \"format\": \"subsections\",\n",
    "            \"item_prefix\": \"- \",\n",
    "            \"indent\": \"   \",\n",
    "            \"subsections\": [\n",
    "                {\"name\": \"Primary Factors\", \"example_items\": [\"- Main reason\"]},\n",
    "                {\n",
    "                    \"name\": \"Supporting Evidence\",\n",
    "                    \"example_items\": [\"- Supporting detail\"],\n",
    "                },\n",
    "                {\"name\": \"Contradicting Evidence\", \"example_items\": [\"- None\"]},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    field_descriptions={\n",
    "        \"Category\": \"Exactly one category from the predefined list (case-sensitive match required)\",\n",
    "        \"Confidence Score\": \"Decimal number between 0.00 and 1.00 indicating classification certainty\",\n",
    "        \"Key Evidence\": \"Three subsections: Message Evidence, Shipping Evidence, Timeline Evidence - each with [sep] token separators\",\n",
    "        \"Reasoning\": \"Three subsections: Primary Factors, Supporting Evidence, Contradicting Evidence - each with [sep] token separators\",\n",
    "    },\n",
    "    formatting_rules=[\n",
    "        \"Use exact section headers with numbers and colons\",\n",
    "        \"No semicolons (;) anywhere in response\",\n",
    "    ],\n",
    "    validation_requirements=[\n",
    "        \"Category must match exactly from predefined list\",\n",
    "        \"Confidence score must be decimal format (e.g., 0.85, not 85%)\",\n",
    "        \"Each evidence item must start with '[sep] ' token\",\n",
    "    ],\n",
    "    evidence_validation_rules=[\n",
    "        \"Message Evidence must include direct quotes with speaker identification\",\n",
    "        \"Shipping Evidence must include tracking events with timestamps\",\n",
    "        \"Timeline Evidence must show chronological sequence of events\",\n",
    "        \"All evidence must reference specific content from input data\",\n",
    "    ],\n",
    "    example_output=(\n",
    "        \"1. Category: TrueDNR\\n\\n\"\n",
    "        \"2. Confidence Score: 0.92\\n\\n\"\n",
    "        \"3. Key Evidence:\\n\"\n",
    "        \"   * Message Evidence:\\n\"\n",
    "        \"     - [BUYER]: Hello, I have not received my package\\n\"\n",
    "        \"     - [BUYER]: But I did not find any package, please refund me\\n\"\n",
    "        \"   * Shipping Evidence:\\n\"\n",
    "        \"     - [Event Time]: 2025-02-21T17:40:49.323Z [Event]: Delivered\\n\"\n",
    "        \"     - No further shipping events after delivery confirmation\\n\"\n",
    "        \"   * Timeline Evidence:\\n\"\n",
    "        \"     - Delivery confirmation on 2025-02-21 17:40\\n\"\n",
    "        \"     - Buyer reports non-receipt starting 2025-02-25 07:14\\n\\n\"\n",
    "        \"4. Reasoning:\\n\"\n",
    "        \"   * Primary Factors:\\n\"\n",
    "        \"     - Tracking shows package was delivered successfully\\n\"\n",
    "        \"     - Buyer explicitly states they did not receive the package\\n\"\n",
    "        \"   * Supporting Evidence:\\n\"\n",
    "        \"     - Buyer requests refund due to missing package\\n\"\n",
    "        \"     - No evidence of buyer receiving wrong/defective item\\n\"\n",
    "        \"   * Contradicting Evidence:\\n\"\n",
    "        \"     - None\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_settings = InstructionConfig(\n",
    "    include_analysis_steps=True,\n",
    "    include_decision_criteria=True,\n",
    "    include_reasoning_requirements=True,\n",
    "    step_by_step_format=True,\n",
    "    include_evidence_validation=True,\n",
    "    classification_guidelines={\n",
    "        \"sections\": [\n",
    "            {\n",
    "                \"title\": \"## Classification Guidelines\",\n",
    "                \"subsections\": [\n",
    "                    {\n",
    "                        \"title\": \"### 1. Output Format Requirements\",\n",
    "                        \"content\": [\n",
    "                            \"**Category Selection:**\",\n",
    "                            \"- Choose exactly ONE category from the provided list\",\n",
    "                            \"- Category name must match exactly (case-sensitive)\",\n",
    "                            \"\",\n",
    "                            \"**Confidence Score:**\",\n",
    "                            \"- Provide as decimal number between 0.00 and 1.00 (e.g., 0.95)\",\n",
    "                            \"- Base confidence for complete data: 0.7-1.0\",\n",
    "                            \"- Missing one field: reduce by 0.1-0.2\",\n",
    "                            \"- Missing two fields: reduce by 0.2-0.3\",\n",
    "                            \"- Minimum confidence threshold: 0.5\",\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"title\": \"### 2. Shiptrack Parsing Rules\",\n",
    "                        \"content\": [\n",
    "                            \"**Multiple Shipment Structure:**\",\n",
    "                            \"- Multiple shipment sequences separated by shipment IDs\",\n",
    "                            '- Each sequence starts with \"[bom] [Shipment ID]:* [eom]\"',\n",
    "                            \"\",\n",
    "                            \"**Analysis Approach:**\",\n",
    "                            \"- Process each shipment sequence separately\",\n",
    "                            \"- Compare delivery events (EVENT_301) across all sequences\",\n",
    "                            \"\",\n",
    "                            \"**Key Event Codes:**\",\n",
    "                            \"- EVENT_301: Delivery confirmation\",\n",
    "                            \"- EVENT_302: Out for delivery\",\n",
    "                            \"- EVENT_201: Arrival at facility\",\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"title\": \"### 3. Missing Data Handling\",\n",
    "                        \"content\": [\n",
    "                            \"**When Dialogue is Empty but Shiptrack Exists:**\",\n",
    "                            \"- Focus on shipping events and timeline\",\n",
    "                            \"- Reduce confidence score by 0.1-0.2\",\n",
    "                            \"\",\n",
    "                            \"**When Shiptrack is Empty but Dialogue Exists:**\",\n",
    "                            \"- Focus on message content and reported issues\",\n",
    "                            \"- Reduce confidence score by 0.1-0.2\",\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"title\": \"### 4. Category Priority Hierarchy\",\n",
    "                        \"content\": [\n",
    "                            \"**Tier 1: Abuse Pattern Categories (Highest Priority)**\",\n",
    "                            \"- PDA_Undeliverable: Verify no delivery + refund given\",\n",
    "                            \"- PDA_Early_Refund: Verify refund before delivery\",\n",
    "                            \"\",\n",
    "                            \"**Tier 2: Delivery Status Categories**\",\n",
    "                            \"- TrueDNR: Delivered but disputed\",\n",
    "                            \"- Confirmed_Delay: External factors confirmed\",\n",
    "                        ],\n",
    "                    },\n",
    "                    {\n",
    "                        \"title\": \"### 5. Evidence Requirements\",\n",
    "                        \"content\": [\n",
    "                            \"**Message Evidence Must Include:**\",\n",
    "                            \"- Direct quotes from dialogue with speaker identification\",\n",
    "                            \"\",\n",
    "                            \"**Shipping Evidence Must Include:**\",\n",
    "                            \"- All tracking events listed chronologically\",\n",
    "                            \"\",\n",
    "                            \"**Timeline Evidence Must Show:**\",\n",
    "                            \"- Clear chronological sequence of events\",\n",
    "                        ],\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_definitions_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,624 - INFO - Generated system prompt config: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs/system_prompt.json\n",
      "2025-11-05 22:49:03,625 - INFO - Generated output format config: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs/output_format.json\n",
      "2025-11-05 22:49:03,625 - INFO - Generated instruction config: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs/instruction.json\n",
      "2025-11-05 22:49:03,626 - INFO - Skipping category_definitions.json generation (no category definitions available)\n",
      "2025-11-05 22:49:03,626 - INFO - Generated prompt configuration bundle in: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs\n",
      "2025-11-05 22:49:03,626 - INFO - Bundle contains 3 JSON configuration files: system_prompt.json, output_format.json, instruction.json\n",
      "2025-11-05 22:49:03,627 - INFO - Auto-generated prompt configuration bundle at: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs\n",
      "2025-11-05 22:49:03,627 - INFO - ‚úÖ BedrockPromptTemplateGeneration configured successfully using BedrockPromptTemplateGenerationConfig\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BedrockPromptTemplateGeneration configured\n"
     ]
    }
   ],
   "source": [
    "step_name = \"BedrockPromptTemplateGeneration\"\n",
    "factory.set_step_config(\n",
    "    step_name,\n",
    "    # input\n",
    "    input_placeholders=[\n",
    "        \"dialogue\",\n",
    "        \"shiptrack_event_history_by_order\",\n",
    "        \"shiptrack_max_estimated_arrival_date_by_order\",\n",
    "    ],\n",
    "    prompt_configs_path=str(category_definitions_path),\n",
    "    # basic setting\n",
    "    template_task_type=\"buyer_seller_classification\",\n",
    "    template_style=\"structured\",\n",
    "    validation_level=\"comprehensive\",\n",
    "    template_version=\"2.0\",\n",
    "    # Output configuration\n",
    "    output_format_type=\"structured_text\",\n",
    "    required_output_fields=[\n",
    "        \"Category\",\n",
    "        \"Confidence Score\",\n",
    "        \"Key Evidence\",\n",
    "        \"Reasoning\",\n",
    "    ],\n",
    "    # Template features\n",
    "    include_examples=True,\n",
    "    generate_validation_schema=True,\n",
    "    # Sub-configurations (Pydantic models)\n",
    "    system_prompt_settings=system_prompt_settings,\n",
    "    output_format_settings=output_format_settings,\n",
    "    instruction_settings=instruction_settings,\n",
    "    processing_entry_point=\"bedrock_prompt_template_generation.py\",\n",
    ")\n",
    "print(f\"‚úÖ {step_name} configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Configure Dummy Data Loading Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dummy data loading\n",
    "if \"DummyDataLoading_training\" in pending_steps:\n",
    "    step_name = \"DummyDataLoading_training\"\n",
    "\n",
    "    data_source = (\n",
    "        \"s3://buyer-seller-messaging-reversal/production-pipeline/raw-input/2025-11-03\"\n",
    "    )\n",
    "\n",
    "    factory.set_step_config(\n",
    "        step_name,\n",
    "        job_type=\"training\",\n",
    "        data_source=data_source,\n",
    "        processing_entry_point=\"dummy_data_loading.py\",\n",
    "        use_large_processing_instance=True,\n",
    "        output_format=\"PARQUET\",\n",
    "    )\n",
    "    print(f\"‚úÖ {step_name} configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: Configure Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BedrockPromptTemplateGeneration']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pending_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure training preprocessing\n",
    "if \"TabularPreprocessing_training\" in pending_steps:\n",
    "    step_name = \"TabularPreprocessing_training\"\n",
    "\n",
    "    factory.set_step_config(\n",
    "        step_name,\n",
    "        job_type=\"training\",\n",
    "        label_name=\"reversal_flag\",\n",
    "        processing_entry_point=\"tabular_preprocessing.py\",\n",
    "        use_large_processing_instance=True,\n",
    "    )\n",
    "    print(f\"‚úÖ {step_name} configured\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.4: Configure Remaining Steps\n",
    "\n",
    "**USER INPUT BLOCK**: Fill in the essential fields for each remaining step.\n",
    "The factory has identified the required fields for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining steps to configure:\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Get current pending steps\n",
    "current_pending = factory.get_pending_steps()\n",
    "\n",
    "print(\"Remaining steps to configure:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for step_name in current_pending:\n",
    "    requirements = factory.get_step_requirements(step_name)\n",
    "    essential_reqs = [req for req in requirements if req[\"required\"]]\n",
    "\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"  Essential fields ({len(essential_reqs)}):\")\n",
    "    for req in essential_reqs:\n",
    "        print(f\"    * {req['name']} ({req['type']}) - {req['description']}\")\n",
    "\n",
    "    if len(requirements) > len(essential_reqs):\n",
    "        optional_count = len(requirements) - len(essential_reqs)\n",
    "        print(f\"  Optional fields: {optional_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,673 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "bedrock_batch_role_arn = PipelineSession().get_caller_identity_arn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_primary_model_id = \"anthropic.claude-sonnet-4-5-20250929-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_inference_profile_arn = \"arn:aws:bedrock:us-east-1:178936618742:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Model Evaluation\n",
    "if \"BedrockBatchProcessing_training\" in current_pending:\n",
    "    factory.set_step_config(\n",
    "        \"BedrockBatchProcessing_training\",\n",
    "        job_type=\"training\",\n",
    "        processing_entry_point=\"bedrock_batch_processing.py\",\n",
    "        bedrock_batch_role_arn=bedrock_batch_role_arn,\n",
    "        bedrock_primary_model_id=bedrock_primary_model_id,\n",
    "        bedrock_inference_profile_arn=bedrock_inference_profile_arn,\n",
    "    )\n",
    "    print(f\"‚úÖ BedrockBatchProcessing_training configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Final Configurations\n",
    "\n",
    "Now that all steps are configured, we can generate the final configuration instances.\n",
    "The factory will validate that all essential fields are provided and create the config objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Configuration Status:\n",
      "========================================\n",
      "Base config: ‚úÖ\n",
      "Processing config: ‚úÖ\n",
      "Pending steps: 0\n",
      "\n",
      "‚úÖ All steps configured! Ready to generate configurations.\n"
     ]
    }
   ],
   "source": [
    "# Check final status\n",
    "final_status = factory.get_configuration_status()\n",
    "final_pending = factory.get_pending_steps()\n",
    "\n",
    "print(\"Final Configuration Status:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Base config: {'‚úÖ' if final_status['base_config'] else '‚ùå'}\")\n",
    "print(f\"Processing config: {'‚úÖ' if final_status['base_processing_config'] else '‚ùå'}\")\n",
    "print(f\"Pending steps: {len(final_pending)}\")\n",
    "\n",
    "if final_pending:\n",
    "    print(\"\\nStill pending:\")\n",
    "    for step in final_pending:\n",
    "        print(f\"  - {step}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please configure remaining steps before generating configs.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All steps configured! Ready to generate configurations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,901 - INFO - ‚úÖ Returning 1 pre-validated configuration instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final configurations...\n",
      "\n",
      "‚úÖ Successfully generated 1 configuration instances:\n",
      "   1. BedrockPromptTemplateGenerationConfig\n",
      "\n",
      "üéâ Configuration generation complete!\n"
     ]
    }
   ],
   "source": [
    "# Generate final configurations\n",
    "if not final_pending:\n",
    "    try:\n",
    "        print(\"Generating final configurations...\")\n",
    "        configs = factory.generate_all_configs()\n",
    "\n",
    "        print(f\"\\n‚úÖ Successfully generated {len(configs)} configuration instances:\")\n",
    "        for i, config in enumerate(configs, 1):\n",
    "            print(f\"  {i:2d}. {config.__class__.__name__}\")\n",
    "\n",
    "        print(\"\\nüéâ Configuration generation complete!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Configuration generation failed: {e}\")\n",
    "        print(\"\\nPlease check that all required fields are provided.\")\n",
    "        configs = None\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Cannot generate configs - some steps are still pending configuration.\")\n",
    "    configs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save to JSON\n",
    "\n",
    "Finally, we save the generated configurations to a unified JSON file using the existing\n",
    "`merge_and_save_configs` utility. This creates the same format as the legacy approach\n",
    "but with much less effort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:03,987 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:03,994 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:04,021 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:04,022 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:04,022 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:04,022 - WARNING - Config SystemPromptConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,023 - WARNING - Config OutputFormatConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,024 - WARNING - Config InstructionConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,025 - INFO - Merging and saving 1 configs to /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-05 22:49:04,026 - INFO - Collecting field information for 1 configs (1 processing configs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving configurations to: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:04,098 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:04,105 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:04,327 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:04,327 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:04,328 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:04,328 - WARNING - Config SystemPromptConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,329 - WARNING - Config OutputFormatConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,329 - WARNING - Config InstructionConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,331 - INFO - Collected information for 49 unique fields\n",
      "2025-11-05 22:49:04,403 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:04,410 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:04,438 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:04,439 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:04,439 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:04,439 - WARNING - Config SystemPromptConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,440 - WARNING - Config OutputFormatConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,440 - WARNING - Config InstructionConfig has no categorize_fields method\n",
      "2025-11-05 22:49:04,441 - INFO - Populated specific fields for 1 configs\n",
      "2025-11-05 22:49:04,442 - INFO - Shared fields: 0\n",
      "2025-11-05 22:49:04,443 - INFO - Specific steps: 1\n",
      "2025-11-05 22:49:04,515 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:04,522 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:04,549 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:04,550 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:04,550 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:04,551 - INFO - Saving merged configuration to /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-05 22:49:04,552 - INFO - Successfully saved merged configuration to /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-05 22:49:04,553 - INFO - Successfully saved merged configs to /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Configuration saved successfully!\n",
      "   File: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "   Size: 26.8 KB\n"
     ]
    }
   ],
   "source": [
    "if configs:\n",
    "    # Set up output directory and filename\n",
    "    MODEL_CLASS = \"pytorch\"\n",
    "    service_name = \"BuyerAbuseRnR\"\n",
    "\n",
    "    config_dir = Path(current_dir) / \"pipeline_config\"\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_file_name = f\"config.json\"\n",
    "    config_path = config_dir / config_file_name\n",
    "\n",
    "    print(f\"Saving configurations to: {config_path}\")\n",
    "\n",
    "    # Use the existing merge_and_save_configs utility\n",
    "    from cursus.steps.configs.utils import merge_and_save_configs\n",
    "\n",
    "    try:\n",
    "        merged_config = merge_and_save_configs(configs, str(config_path))\n",
    "\n",
    "        print(f\"\\n‚úÖ Configuration saved successfully!\")\n",
    "        print(f\"   File: {config_path}\")\n",
    "        print(f\"   Size: {config_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "        # Also save hyperparameters separately (for compatibility)\n",
    "        # hyperparam_path = source_dir / 'hyperparams' / f'hyperparameters.json'\n",
    "        # with open(hyperparam_path, 'w') as f:\n",
    "        #    json.dump(xgb_hyperparams.model_dump(), f, indent=2, sort_keys=True)\n",
    "\n",
    "        # print(f\"   Hyperparameters: {hyperparam_path}\")\n",
    "\n",
    "        # print(f\"\\nüéâ Interactive configuration complete!\")\n",
    "        # print(f\"\\nüìä Comparison with legacy approach:\")\n",
    "        # print(f\"   Legacy: 500+ lines of manual configuration\")\n",
    "        # print(f\"   Interactive: Guided step-by-step process\")\n",
    "        # print(f\"   Time saved: ~20-25 minutes\")\n",
    "        # print(f\"   Error reduction: Validation at each step\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Failed to save configurations: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No configurations to save. Please generate configs first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if we can load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cursus.steps.configs.config_dummy_data_loading_step import DummyDataLoadingConfig\n",
    "from cursus.steps.configs.config_tabular_preprocessing_step import (\n",
    "    TabularPreprocessingConfig,\n",
    ")\n",
    "from cursus.steps.configs.config_bedrock_prompt_template_generation_step import (\n",
    "    BedrockPromptTemplateGenerationConfig,\n",
    ")\n",
    "from cursus.steps.configs.config_bedrock_batch_processing_step import (\n",
    "    BedrockBatchProcessingConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cursus.steps.configs.utils import load_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_CLASSES = {\n",
    "    \"DummyDataLoadingConfig\": DummyDataLoadingConfig,\n",
    "    \"BedrockPromptTemplateGenerationConfig\": BedrockPromptTemplateGenerationConfig,\n",
    "    \"BedrockBatchProcessingConfig\": BedrockBatchProcessingConfig,\n",
    "    \"TabularPreprocessingConfig\": TabularPreprocessingConfig,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:49:04,576 - INFO - Loading configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-05 22:49:04,577 - INFO - Loading configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-05 22:49:04,578 - WARNING - Could not find class InstructionConfig\n",
      "2025-11-05 22:49:04,578 - WARNING - Could not find class OutputFormatConfig\n",
      "2025-11-05 22:49:04,578 - WARNING - Could not find class SystemPromptConfig\n",
      "2025-11-05 22:49:04,579 - INFO - Successfully loaded configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-05 22:49:04,579 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json with 1 specific configs\n",
      "2025-11-05 22:49:04,580 - INFO - Creating additional config instance for BedrockPromptTemplateGeneration (BedrockPromptTemplateGenerationConfig)\n",
      "2025-11-05 22:49:04,653 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:04,660 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:04,688 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:04,688 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:04,688 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:04,895 - INFO - Discovered 47 core config classes\n",
      "2025-11-05 22:49:04,903 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-05 22:49:04,931 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-05 22:49:04,931 - INFO - Built complete config classes: 58 total (47 config + 11 hyperparameter auto-discovered)\n",
      "2025-11-05 22:49:04,931 - INFO - Discovered 58 config classes via step catalog\n",
      "2025-11-05 22:49:04,933 - INFO - Generated system prompt config: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs/system_prompt.json\n",
      "2025-11-05 22:49:04,934 - INFO - Generated output format config: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs/output_format.json\n",
      "2025-11-05 22:49:04,934 - INFO - Generated instruction config: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs/instruction.json\n",
      "2025-11-05 22:49:04,935 - INFO - Skipping category_definitions.json generation (no category definitions available)\n",
      "2025-11-05 22:49:04,935 - INFO - Generated prompt configuration bundle in: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs\n",
      "2025-11-05 22:49:04,935 - INFO - Bundle contains 3 JSON configuration files: system_prompt.json, output_format.json, instruction.json\n",
      "2025-11-05 22:49:04,936 - INFO - Auto-generated prompt configuration bundle at: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/dockers/prompt_configs\n",
      "2025-11-05 22:49:04,936 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n"
     ]
    }
   ],
   "source": [
    "loaded_configs = load_configs(str(config_path), CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the **DAGConfigFactory** approach to pipeline configuration:\n",
    "\n",
    "### ‚úÖ **Benefits Achieved**\n",
    "\n",
    "1. **Reduced Complexity**: From 500+ lines of manual config to guided workflow\n",
    "2. **Base Config Inheritance**: Set common fields once, inherit everywhere\n",
    "3. **Step-by-Step Guidance**: Clear requirements for each configuration step\n",
    "4. **Validation**: Comprehensive validation prevents configuration errors\n",
    "5. **Reusable DAG**: Pipeline structure defined once, reused across environments\n",
    "\n",
    "### üîÑ **Workflow Comparison**\n",
    "\n",
    "| Aspect | Legacy Approach | Interactive Approach |\n",
    "|--------|----------------|---------------------|\n",
    "| **Lines of Code** | 500+ manual lines | Guided step-by-step |\n",
    "| **Time Required** | 30+ minutes | 10-15 minutes |\n",
    "| **Error Rate** | High (manual entry) | Low (validation) |\n",
    "| **Reusability** | Copy-paste heavy | DAG-driven |\n",
    "| **Maintenance** | Manual updates | Automatic inheritance |\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "\n",
    "The generated configuration file can now be used with the existing pipeline compiler:\n",
    "\n",
    "```python\n",
    "# Use with pipeline compiler (from demo_pipeline.ipynb)\n",
    "from cursus.core.compiler.dag_compiler import PipelineDAGCompiler\n",
    "\n",
    "dag_compiler = PipelineDAGCompiler(\n",
    "    config_path=config_path,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "# Compile DAG to pipeline\n",
    "template_pipeline, report = dag_compiler.compile_with_report(dag=dag)\n",
    "```\n",
    "\n",
    "The interactive configuration approach transforms the user experience from complex manual setup to an intuitive, guided workflow while maintaining full compatibility with the existing cursus infrastructure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
