{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "299b07c0-e087-4bc8-8a81-46e3710c12fb",
   "metadata": {},
   "source": [
    "# Cursus: Automatic SageMaker (MODS) Pipeline Compiler\n",
    "\n",
    "The main contribution of this work is **Cursus**, a **compiler** that automatically generate **[MODS (Model Training Workflow Operation and Development System) Pipeline](https://w.amazon.com/bin/view/CMLS/Overview/MODS/)** base on two set of user inputs\n",
    "* The **Pipeline DAG (Directed Acylic Graph)**, which describe pipeline as a graph\n",
    "* The **Unified Config JSON**, which provides a central hub to extract all user inputs and their associated step information\n",
    "    * Run [demo_config](./demo_config.ipynb) first to generate the Unified Config JSON\n",
    "    * The config json will be saved in `./pipeling_config/xxx/` folder\n",
    "\n",
    "![mods_pipeline_train_eval_calib](./demo/mods_pipeline_train_eval_calib.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe50590-45a5-42e0-ba46-db3182c95c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefa46ee-5c6e-4376-b315-596daf503e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, model_validator, field_validator\n",
    "from typing import List, Optional, Dict, Any, Type, Union, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94496431-af1a-4689-888d-4e5dc62a2c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import (\n",
    "    defaultdict,\n",
    "    deque\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92563607-e4f2-417f-9115-d8549711e7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cb59bf-9f68-4b0c-b699-a48961d40f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8109385d-fa21-4b1d-8b0c-52a0394f0b1e",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbf0cf9-05ca-4e17-9725-a02e1ac26975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:10,890 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7aae78b-4ba8-4498-95b2-2049fea6f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6efb08-ca57-43fe-9a92-ade36d21fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name='buyer-seller-messaging-reversal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e3f0529-ee5d-468b-a891-e9c1c0075835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:11,249 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "pipeline_session = PipelineSession(default_bucket=bucket_name) # IMPORTANT now the session uses the generated sagemaker_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e9385d-eccd-46ec-9813-08997aaa0c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:11,511 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role=PipelineSession().get_caller_identity_arn()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6850b94d-eabe-4bcf-a509-37d67f17a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add project root /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines into system\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Get parent directory of current notebook\n",
    "project_root = str(Path().absolute().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)  \n",
    "    print(f\"add project root {project_root} into system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9961bb-ae6a-45eb-9e12-5fb79b25f71c",
   "metadata": {},
   "source": [
    "## Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c50cd1-074c-47a5-acfe-5bbcf9500cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_list = [\n",
    "    'NA',\n",
    "    'EU',\n",
    "    'FE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9387ac-df3b-47c8-b869-6c748ed55cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_selection = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac12ddc2-68c8-4d10-ab58-b9285365515c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NA'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region = region_list[region_selection]\n",
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "151164f4-717a-4750-bebb-1d4e5406812d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_CLASS='pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e69d0db9-52a9-46e2-a090-0ff91670591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name=\"BuyerAbuseRnR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c62dd-0e54-491a-b399-1ff7e3e8de42",
   "metadata": {},
   "source": [
    "#### Config and Hyperparameter Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "113f0787-3986-4bd9-a087-8c3ef3247fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path.cwd()\n",
    "config_dir = Path(current_dir) / 'pipeline_config'\n",
    "print(config_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4038ad40-95c0-4858-8de4-24aa96a0a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyparam_filename = f'hyperparameters_{region}_{MODEL_CLASS}.json' #'hyperparameters.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94c03bfe-a3b2-4c3c-bfac-ae097ef3ef42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config.json'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_config_name = f'config.json'  #f'config_{region}.json'\n",
    "pipeline_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41fe835b-937f-4f17-a71f-2d0deb0483ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = config_dir / pipeline_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf788034-e479-4cac-8274-98b33937569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa969016-7024-423f-9784-5c4145d276bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9282a90-2ed4-43fe-9725-2c3ec53b4f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00e1fa95-7379-4ab9-8d25-8f57654e5734",
   "metadata": {},
   "source": [
    "## Pipeline Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d32cbbf4-f448-42f4-a54b-56f7947f8c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02cf5a-e2d5-49af-8f55-aca2c4bec315",
   "metadata": {},
   "source": [
    "## [Optional]: Test Config Load Functionality\n",
    "\n",
    "Please skip this section if you are not concern about the config information loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79af0dc-1b46-4809-a0b2-8eb534e4d365",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee5eb858-99b6-4937-a3d7-265e53f95af8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from cursus.steps.hyperparams.hyperparameters_xgboost import XGBoostModelHyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26e2d72b-1a34-42ca-9aee-a8430d6b8706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hyparam_path = config_dir / hyparam_filename\n",
    "#with open(hyparam_path, 'r') as file:\n",
    "#    hyperparam_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3f00717-87c5-4270-85d4-f81c34f7deb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hyperparams = XGBoostModelHyperparameters(**hyperparam_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e74fb0c-7a5f-4792-8123-fa0142ff15ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hyperparams.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f57e6d2-3012-4746-90a3-9fd7c2559b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hyperparams.is_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23851f5a-be88-4aa8-a17e-95ca42f282ce",
   "metadata": {},
   "source": [
    "### Import Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07614f39-3683-42b1-baf0-dca78d21b6eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:12,158 - WARNING - Could not import constants from mods_workflow_core, using local definitions\n"
     ]
    }
   ],
   "source": [
    "from cursus.core.base.config_base import BasePipelineConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c507be6b-d6f4-42ff-99d3-3f5050589715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from cursus.steps.configs.config_cradle_data_loading_step import (CradleDataLoadingConfig,\n",
    "#                                                    MdsDataSourceConfig,\n",
    "#                                                    EdxDataSourceConfig,\n",
    "#                                                    DataSourceConfig,\n",
    "#                                                    DataSourcesSpecificationConfig,\n",
    "#                                                    JobSplitOptionsConfig,\n",
    "#                                                    TransformSpecificationConfig,\n",
    "#                                                    OutputSpecificationConfig,\n",
    "#                                                    CradleJobSpecificationConfig\n",
    "#                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2df6703-df32-4145-8571-e70b1740cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cursus.steps.configs.config_dummy_data_loading_step import DummyDataLoadingConfig\n",
    "from cursus.steps.configs.config_tabular_preprocessing_step import TabularPreprocessingConfig\n",
    "from cursus.steps.configs.config_bedrock_prompt_template_generation_step import BedrockPromptTemplateGenerationConfig\n",
    "from cursus.steps.configs.config_bedrock_batch_processing_step import BedrockBatchProcessingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbb7c6-7f29-48bf-8940-86cf69df9c73",
   "metadata": {},
   "source": [
    "### Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "715f7a1c-a2c2-4809-b629-35d268b343e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cursus.steps.configs.utils import serialize_config, merge_and_save_configs, load_configs, verify_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a9787f3-493e-4631-9efc-eceeb0c2bc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG_CLASSES = {\n",
    "        'DummyDataLoadingConfig':                     DummyDataLoadingConfig,\n",
    "        'BedrockPromptTemplateGenerationConfig':      BedrockPromptTemplateGenerationConfig,\n",
    "        'BedrockBatchProcessingConfig':               BedrockBatchProcessingConfig,\n",
    "        'TabularPreprocessingConfig':                 TabularPreprocessingConfig,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed47146d-6e3b-4254-aeae-21bae57e43d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dc47699-ed18-46e6-a9bc-ae8eb890feeb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:12,416 - INFO - Loading configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:12,416 - INFO - Loading configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:12,418 - WARNING - Could not find class InstructionConfig\n",
      "2025-11-08 05:08:12,418 - WARNING - Could not find class OutputFormatConfig\n",
      "2025-11-08 05:08:12,418 - WARNING - Could not find class SystemPromptConfig\n",
      "2025-11-08 05:08:12,419 - INFO - Successfully loaded configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:12,419 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json with 4 specific configs\n",
      "2025-11-08 05:08:12,420 - INFO - Creating additional config instance for BedrockBatchProcessing_training (BedrockBatchProcessingConfig)\n",
      "2025-11-08 05:08:12,421 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:12,421 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:12,422 - INFO - âœ… BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-08 05:08:12,422 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:12,422 - INFO - ðŸŽ‰ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:12,423 - INFO - ðŸ” ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:12,423 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:12,424 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-08 05:08:12,424 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:12,424 - INFO - ðŸŽ‰ ScriptAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:12,521 - INFO - Discovered 49 core config classes\n",
      "2025-11-08 05:08:12,538 - INFO - Discovered 4 core hyperparameter classes\n",
      "2025-11-08 05:08:12,565 - INFO - Discovered 7 base hyperparameter classes from core/base\n",
      "2025-11-08 05:08:12,566 - INFO - Creating additional config instance for BedrockPromptTemplateGeneration (BedrockPromptTemplateGenerationConfig)\n",
      "2025-11-08 05:08:12,568 - INFO - Generated system prompt config: docker/scripts/prompt_configs/system_prompt.json\n",
      "2025-11-08 05:08:12,569 - INFO - Generated output format config: docker/scripts/prompt_configs/output_format.json\n",
      "2025-11-08 05:08:12,569 - INFO - Generated instruction config: docker/scripts/prompt_configs/instruction.json\n",
      "2025-11-08 05:08:12,570 - INFO - Skipping category_definitions.json generation (no category definitions available)\n",
      "2025-11-08 05:08:12,570 - INFO - Generated prompt configuration bundle in: docker/scripts/prompt_configs\n",
      "2025-11-08 05:08:12,570 - INFO - Bundle contains 3 JSON configuration files: system_prompt.json, output_format.json, instruction.json\n",
      "2025-11-08 05:08:12,571 - INFO - Auto-generated prompt configuration bundle at: docker/scripts/prompt_configs\n",
      "2025-11-08 05:08:12,571 - INFO - Creating additional config instance for DummyDataLoading_training (DummyDataLoadingConfig)\n",
      "2025-11-08 05:08:12,572 - INFO - Creating additional config instance for TabularPreprocessing_training (TabularPreprocessingConfig)\n",
      "2025-11-08 05:08:12,573 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n"
     ]
    }
   ],
   "source": [
    "# Load configs\n",
    "loaded_configs = load_configs(config_path, CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8f2aee4-a039-42e6-a89b-86c0084f6c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BedrockBatchProcessing_training': BedrockBatchProcessingConfig(author='lukexie', bucket='buyer-seller-messaging-reversal', role='arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default', region='NA', service_name='BuyerAbuseRnR', pipeline_version='0.0.1', model_class='pytorch', current_date='2025-11-08', framework_version='2.1.0', py_version='py310', source_dir='docker', enable_caching=False, use_secure_pypi=False, project_root_folder='rnr_pytorch_bedrock', processing_instance_count=1, processing_volume_size=500, processing_instance_type_large='ml.m5.12xlarge', processing_instance_type_small='ml.m5.4xlarge', use_large_processing_instance=False, processing_source_dir='docker/scripts', processing_entry_point='bedrock_batch_processing.py', processing_script_arguments=None, processing_framework_version='1.2-1', bedrock_batch_role_arn='arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default', job_type='training', bedrock_primary_model_id='anthropic.claude-sonnet-4-5-20250929-v1:0', bedrock_fallback_model_id='anthropic.claude-sonnet-4-20250514-v1:0', bedrock_inference_profile_arn='arn:aws:bedrock:us-east-1:178936618742:inference-profile/us.anthropic.claude-sonnet-4-20250514-v1:0', bedrock_inference_profile_required_models=['anthropic.claude-opus-4-1-20250805-v1:0', 'anthropic.claude-sonnet-4-5-20250929-v1:0', 'anthropic.claude-sonnet-4-20250514-v1:0'], bedrock_max_tokens=32768, bedrock_temperature=1.0, bedrock_top_p=0.999, bedrock_batch_size=10, bedrock_max_retries=3, bedrock_output_column_prefix='llm_', bedrock_concurrency_mode='sequential', bedrock_max_concurrent_workers=5, bedrock_rate_limit_per_second=10, bedrock_batch_mode='auto', bedrock_batch_threshold=1000, bedrock_batch_timeout_hours=24, bedrock_max_records_per_job=30000, bedrock_max_concurrent_batch_jobs=20),\n",
       " 'BedrockPromptTemplateGeneration': BedrockPromptTemplateGenerationConfig(author='lukexie', bucket='buyer-seller-messaging-reversal', role='arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default', region='NA', service_name='BuyerAbuseRnR', pipeline_version='0.0.1', model_class='pytorch', current_date='2025-11-08', framework_version='2.1.0', py_version='py310', source_dir='docker', enable_caching=False, use_secure_pypi=False, project_root_folder='rnr_pytorch_bedrock', processing_instance_count=1, processing_volume_size=500, processing_instance_type_large='ml.m5.12xlarge', processing_instance_type_small='ml.m5.4xlarge', use_large_processing_instance=False, processing_source_dir='docker/scripts', processing_entry_point='bedrock_prompt_template_generation.py', processing_script_arguments=None, processing_framework_version='1.2-1', input_placeholders=['dialogue', 'shiptrack_event_history_by_order', 'shiptrack_max_estimated_arrival_date_by_order'], prompt_configs_path='prompt_configs', template_task_type='buyer_seller_classification', template_style='structured', validation_level='comprehensive', output_format_type='structured_json', required_output_fields=['Category', 'Confidence Score', 'Key Evidence', 'Reasoning'], include_examples=True, generate_validation_schema=True, template_version='2.0', system_prompt_settings=SystemPromptConfig(role_definition='expert in analyzing buyer-seller messaging conversations and shipping logistics', expertise_areas=['buyer-seller messaging analysis', 'shipping logistics', 'delivery timing analysis', 'e-commerce dispute resolution', 'classification and categorization'], responsibilities=['classify interactions based on message content', 'analyze shipping events and delivery timing', 'categorize into predefined dispute categories', 'provide evidence-based reasoning for classifications'], behavioral_guidelines=['be precise in classification decisions', 'be objective in evidence evaluation', 'be thorough in timeline analysis', 'follow exact formatting requirements', 'consider all available evidence sources'], tone='professional'), output_format_settings=OutputFormatConfig(format_type='structured_json', required_fields=['category', 'confidence_score', 'key_evidence', 'reasoning'], field_descriptions={'category': 'Exactly one category from the predefined list (case-sensitive match required)', 'confidence_score': 'Decimal number between 0.00 and 1.00 indicating classification certainty', 'key_evidence': 'Object containing three arrays: message_evidence, shipping_evidence, timeline_evidence', 'reasoning': 'Object containing three arrays: primary_factors, supporting_evidence, contradicting_evidence'}, json_schema={'additionalProperties': False, 'properties': {'category': {'description': 'Exactly one category from the predefined list (case-sensitive match required)', 'enum': [], 'type': 'string'}, 'confidence_score': {'description': 'Decimal number between 0.00 and 1.00 indicating classification certainty', 'maximum': 1.0, 'minimum': 0.0, 'type': 'number'}, 'key_evidence': {'description': 'Object containing three arrays of evidence from different sources', 'properties': {'message_evidence': {'description': 'Direct quotes from dialogue with speaker identification', 'items': {'type': 'string'}, 'type': 'array'}, 'shipping_evidence': {'description': 'Tracking events with timestamps', 'items': {'type': 'string'}, 'type': 'array'}, 'timeline_evidence': {'description': 'Chronological sequence of key events', 'items': {'type': 'string'}, 'type': 'array'}}, 'required': ['message_evidence', 'shipping_evidence', 'timeline_evidence'], 'type': 'object'}, 'reasoning': {'description': 'Object containing three arrays explaining the classification decision', 'properties': {'contradicting_evidence': {'description': 'Evidence that contradicts the classification (use empty array if none)', 'items': {'type': 'string'}, 'type': 'array'}, 'primary_factors': {'description': 'Main reasons supporting the selected category', 'items': {'type': 'string'}, 'type': 'array'}, 'supporting_evidence': {'description': 'Additional evidence that strengthens the classification', 'items': {'type': 'string'}, 'type': 'array'}}, 'required': ['primary_factors', 'supporting_evidence', 'contradicting_evidence'], 'type': 'object'}}, 'required': ['category', 'confidence_score', 'key_evidence', 'reasoning'], 'type': 'object'}, validation_requirements=['Must be valid JSON format', 'Category must match exactly from predefined list', 'Confidence score must be number between 0.0 and 1.0', 'All required fields must be present', 'key_evidence and reasoning must be objects with nested arrays'], evidence_validation_rules=['Message Evidence must include direct quotes with speaker identification', 'Shipping Evidence must include tracking events with timestamps', 'Timeline Evidence must show chronological sequence of events', 'All evidence must reference specific content from input data'], header_text=None, structured_text_sections=None, formatting_rules=['Output MUST be valid, parseable JSON.', 'Do not include any text before the opening { or after the closing }.', 'CRITICAL: Do NOT wrap JSON in markdown code blocks - no ``` or ```json markers.', 'CRITICAL: Output pure JSON starting with { and ending with } - nothing else.', 'Ensure all arrays and objects are properly closed.', 'Use empty arrays [] for missing values, not null or empty strings.', 'Do not include trailing commas.', 'Ensure proper escaping of special characters in strings.', 'Quote Handling - JSON Structure: ALWAYS use ASCII double quotes (U+0022) for JSON keys and for starting/ending JSON string values.', \"Quote Handling - Cited Content (Smart Quotes): If the input text contains any non-ASCII or 'smart' quotation marks, do NOT copy those characters into the JSON. Instead, replace each of them with a simple ASCII apostrophe (').\", 'The smart quotation marks that MUST be replaced include (but are not limited to): U+201C (LEFT DOUBLE QUOTATION MARK), U+201D (RIGHT DOUBLE QUOTATION MARK), U+201E (DOUBLE LOW-9 QUOTATION MARK, used in German), U+201F, and U+2018/U+2019/U+201A/U+201B (Unicode single quotation marks).', \"Example (described, not shown verbatim): If the input shows a name surrounded by any kind of non-ASCII quote characters (for example, a German-style opening quote before a person name), you MUST output that name in JSON as 'Person Name' using ASCII apostrophes only.\", 'Inner Double Quotes - General Rule: Inside JSON string values, NEVER include a raw ASCII double quote character (\") as part of the content. The ONLY double quotes you should ever emit are the ones required by JSON syntax (around keys and around entire string values).', 'Inner Double Quotes - Rewriting Measurements: If the original text uses double quotes for measurements or sizes (for example, a rug described as 8\"x10\"), you MUST rewrite this in plain text without inner double quotes (for example, \\'8 x 10 inch rug\\' or \\'8 by 10 inch rug\\').', \"Inner Double Quotes - Rewriting Emphasized Phrases: If the original text puts a word or phrase in double quotes (for example, a buyer writing a word like impossible in quotes), you MUST rewrite it using apostrophes or plain text, such as [BUYER]: 'impossible' or [BUYER]: impossible, without any inner double quote characters.\", \"Inner Double Quotes - No Literal Copy: Do NOT copy any internal double quotes from the source text into the JSON output. When the original text uses double quotes around a phrase, output that phrase using ASCII apostrophes (') or rephrase it so that no internal double quotes appear inside the JSON string.\", \"Summary: Use the ASCII double quote character (U+0022) only for JSON structure (keys and string delimiters). Inside JSON string values, never emit smart quotation marks and never emit raw inner double quotes. When the original text uses quotes around a phrase or measurement, rewrite that phrase using ASCII apostrophes (') or plain words instead.\"], example_output={'category': 'TrueDNR', 'confidence_score': 0.92, 'key_evidence': {'message_evidence': ['[BUYER]: Hello, I have not received my package, but I see the order shows that it has been delivered, why?', '[BUYER]: But I did not find any package, please refund me, thank you'], 'shipping_evidence': ['[Event Time]: 2025-02-21T17:40:49.323Z [Ship Track Event]: Delivered to customer', 'No further shipping events after delivery confirmation'], 'timeline_evidence': ['Delivery confirmation on 2025-02-21 17:40', 'Buyer reports non-receipt starting 2025-02-25 07:14']}, 'reasoning': {'contradicting_evidence': [], 'primary_factors': ['Tracking shows package was delivered successfully', 'Buyer explicitly states they did not receive the package after delivery scan'], 'supporting_evidence': ['Buyer requests refund due to missing package', 'No evidence of buyer receiving wrong/defective item']}}), instruction_settings=InstructionConfig(include_analysis_steps=True, include_decision_criteria=True, include_reasoning_requirements=True, step_by_step_format=True, include_evidence_validation=True, classification_guidelines={'sections': [{'subsections': [{'content': ['**Category Selection:**', '- Choose exactly ONE category from the provided list', '- Category name must match exactly (case-sensitive)', '', '**Confidence Score:**', '- Provide as decimal number between 0.00 and 1.00 (e.g., 0.95)', '- Base confidence for complete data: 0.7-1.0', '- Missing one field: reduce by 0.1-0.2', '- Missing two fields: reduce by 0.2-0.3', '- Minimum confidence threshold: 0.5'], 'title': '### 1. Output Format Requirements'}, {'content': ['**Multiple Shipment Structure:**', '- Multiple shipment sequences separated by shipment IDs', '- Each sequence starts with \"[bom] [Shipment ID]:* [eom]\"', '', '**Analysis Approach:**', '- Process each shipment sequence separately', '- Compare delivery events (EVENT_301) across all sequences', '', '**Key Event Codes:**', '- EVENT_301: Delivery confirmation', '- EVENT_302: Out for delivery', '- EVENT_201: Arrival at facility'], 'title': '### 2. Shiptrack Parsing Rules'}, {'content': ['**When Dialogue is Empty but Shiptrack Exists:**', '- Focus on shipping events and timeline', '- Reduce confidence score by 0.1-0.2', '', '**When Shiptrack is Empty but Dialogue Exists:**', '- Focus on message content and reported issues', '- Reduce confidence score by 0.1-0.2'], 'title': '### 3. Missing Data Handling'}, {'content': ['**Tier 1: Abuse Pattern Categories (Highest Priority)**', '- PDA_Undeliverable: Verify no delivery + refund given', '- PDA_Early_Refund: Verify refund before delivery', '', '**Tier 2: Delivery Status Categories**', '- TrueDNR: Delivered but disputed', '- Confirmed_Delay: External factors confirmed'], 'title': '### 4. Category Priority Hierarchy'}, {'content': ['**Message Evidence Must Include:**', '- Direct quotes from dialogue with speaker identification', '', '**Shipping Evidence Must Include:**', '- All tracking events listed chronologically', '', '**Timeline Evidence Must Show:**', '- Clear chronological sequence of events'], 'title': '### 5. Evidence Requirements'}], 'title': '## Classification Guidelines'}]}), category_definitions=None),\n",
       " 'DummyDataLoading_training': DummyDataLoadingConfig(author='lukexie', bucket='buyer-seller-messaging-reversal', role='arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default', region='NA', service_name='BuyerAbuseRnR', pipeline_version='0.0.1', model_class='pytorch', current_date='2025-11-08', framework_version='2.1.0', py_version='py310', source_dir='docker', enable_caching=False, use_secure_pypi=False, project_root_folder='rnr_pytorch_bedrock', processing_instance_count=1, processing_volume_size=500, processing_instance_type_large='ml.m5.12xlarge', processing_instance_type_small='ml.m5.4xlarge', use_large_processing_instance=True, processing_source_dir='docker/scripts', processing_entry_point='dummy_data_loading.py', processing_script_arguments=None, processing_framework_version='1.2-1', data_source='s3://buyer-seller-messaging-reversal/production-pipeline/raw-input/2025-11-03', job_type='training', max_file_size_mb=1000, supported_formats=['csv', 'parquet', 'json', 'jsonl'], write_data_shards=True, shard_size=10000, output_format='PARQUET'),\n",
       " 'TabularPreprocessing_training': TabularPreprocessingConfig(author='lukexie', bucket='buyer-seller-messaging-reversal', role='arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default', region='NA', service_name='BuyerAbuseRnR', pipeline_version='0.0.1', model_class='pytorch', current_date='2025-11-08', framework_version='2.1.0', py_version='py310', source_dir='docker', enable_caching=False, use_secure_pypi=False, project_root_folder='rnr_pytorch_bedrock', processing_instance_count=1, processing_volume_size=500, processing_instance_type_large='ml.m5.12xlarge', processing_instance_type_small='ml.m5.4xlarge', use_large_processing_instance=True, processing_source_dir='docker/scripts', processing_entry_point='tabular_preprocessing.py', processing_script_arguments=None, processing_framework_version='1.2-1', job_type='training', label_name='reversal_flag', train_ratio=0.7, test_val_ratio=0.5)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bf71f23-b2af-4c1c-852a-7f53f538a896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10d3c8a6-a822-483b-be8b-2cacd1a8326f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BedrockBatchProcessing_training',\n",
       " 'BedrockPromptTemplateGeneration',\n",
       " 'DummyDataLoading_training',\n",
       " 'TabularPreprocessing_training']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(k) for k in loaded_configs.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c406dc5a-2e6e-40b4-981f-bf23c7dd1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_config = next(iter(loaded_configs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e76a613-f5a2-4008-9c08-85ec25077883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PIPELINE_VERSION = first_config.pipeline_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e42b028-085e-43d4-9fed-767174934670",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_DESCRIPTION = first_config.pipeline_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01aa4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = first_config.pipeline_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37356c7-34c6-472a-b9a0-7ccaaf87d374",
   "metadata": {},
   "source": [
    "## Parameter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca29b12b-3867-426a-adfc-d6e46e65587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:12,632 - INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "# Initialize boto3 clients\n",
    "ec2_client = boto3.client('ec2')\n",
    "kms_client = boto3.client('kms')\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Get account and region info\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578482b6-1ef2-4a30-b52e-77843816f04f",
   "metadata": {},
   "source": [
    "### Find VPC Subnet - Get default VPC subnets or list all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c915ec6a-a3b5-47bc-9153-4159b809f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2_client.describe_subnets(\n",
    "    Filters=[{'Name': 'default-for-az', 'Values': ['true']}]\n",
    ")\n",
    "vpc_subnet_id = response['Subnets'][0]['SubnetId'] if response['Subnets'] else None\n",
    "\n",
    "# OR list all subnets and choose one\n",
    "#all_subnets = ec2_client.describe_subnets()\n",
    "#for subnet in all_subnets['Subnets']:\n",
    "#    print(f\"Subnet ID: {subnet['SubnetId']}, VPC: {subnet['VpcId']}, AZ: {subnet['AvailabilityZone']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb35919",
   "metadata": {},
   "source": [
    "### Find Security Group - Get default or list all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e02ebc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ec2_client.describe_security_groups(\n",
    "    Filters=[{'Name': 'group-name', 'Values': ['default']}]\n",
    ")\n",
    "security_group_id = response['SecurityGroups'][0]['GroupId'] if response['SecurityGroups'] else None\n",
    "\n",
    "# OR list all security groups\n",
    "#all_sgs = ec2_client.describe_security_groups()\n",
    "#for sg in all_sgs['SecurityGroups']:\n",
    "#    print(f\"SG ID: {sg['GroupId']}, Name: {sg['GroupName']}, VPC: {sg.get('VpcId')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed69c5-c066-421e-bd69-11ebb2a2bacc",
   "metadata": {},
   "source": [
    "### Find KMS Key - List KMS keys for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7d27cef-15bc-4602-9de7-6d6af0ce9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = kms_client.list_aliases()\n",
    "for alias in response['Aliases']:\n",
    "    if 'sagemaker' in alias['AliasName'].lower():\n",
    "        print(f\"KMS Alias: {alias['AliasName']}, Key ID: {alias.get('TargetKeyId')}\")\n",
    "\n",
    "# OR get account's default KMS key ARN\n",
    "kms_key_id = f\"arn:aws:kms:{region}:{account_id}:alias/aws/sagemaker\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e10cdeac-719b-43cc-aa46-6ef2c1ce91cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found values:\n",
      "VPC Subnet: subnet-45db3e4b\n",
      "Security Group: sg-e116c4be\n",
      "KMS Key: arn:aws:kms:us-east-1:178936618742:alias/aws/sagemaker\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nFound values:\")\n",
    "print(f\"VPC Subnet: {vpc_subnet_id}\")\n",
    "print(f\"Security Group: {security_group_id}\")\n",
    "print(f\"KMS Key: {kms_key_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26b581",
   "metadata": {},
   "source": [
    "### Execution Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d840789",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e7b6d-8793-45e4-b228-32258643c246",
   "metadata": {},
   "source": [
    "### Define Parameter String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32b833cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.parameters import ParameterString\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bdf5691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined Pipeline Parameters\n",
    "PIPELINE_EXECUTION_TEMP_DIR = ParameterString(name=\"EXECUTION_S3_PREFIX\", default_value=f\"s3://{bucket_name}/pipeline/{PIPELINE_NAME}/{execution_id}\")\n",
    "KMS_ENCRYPTION_KEY_PARAM = ParameterString(name=\"KMS_ENCRYPTION_KEY_PARAM\", default_value=kms_key_id)\n",
    "VPC_SUBNET = ParameterString(\n",
    "    name=\"VPC_SUBNET\",\n",
    "    default_value=vpc_subnet_id\n",
    ")  # TODO: test if we can replace it with multiple subnets\n",
    "SECURITY_GROUP_ID = ParameterString(name=\"SECURITY_GROUP_ID\", default_value=security_group_id)\n",
    "PROCESSING_JOB_SHARED_NETWORK_CONFIG = NetworkConfig(\n",
    "    enable_network_isolation=False,\n",
    "    security_group_ids=[SECURITY_GROUP_ID],\n",
    "    subnets=[VPC_SUBNET],\n",
    "    encrypt_inter_container_traffic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbda68-5e79-419e-8de5-e4ce2b13d408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b04ac28-360a-4886-a447-332884b77a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66e6a4d8-728e-4376-ab5d-5c1fb1659679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Any, Optional, Type\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7cf7f72-1ff6-4614-843c-d1c8e20e90e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.parameters import Parameter\n",
    "from sagemaker.workflow.properties import Properties\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession # Crucial import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687beff8-d4b1-4eca-80cd-9921ab83e616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a27354d-00f5-4847-aae7-632d6260a4f7",
   "metadata": {},
   "source": [
    "## Demo: An End-to-End Pipeline based on PipelineDAG Compiler\n",
    "Let us use the following simpler DAG (without registration as example)\n",
    "\n",
    "\n",
    "In this demo there are several user input\n",
    "* the **Unified JSON file** in `config_path`\n",
    "* the **Registry Manager**: an object that handles the map between step logical name to `step.properties`\n",
    "* the **Dependency Resolver**: an object than handles the *automatic dependency resolution* between steps\n",
    "* the other fields\n",
    "    * `sagemaker_session`: pipelne session\n",
    "    * `role`: IAM Role\n",
    "    * `notebook_root`: track the root path \n",
    "\n",
    "\n",
    "In this pipeline template, we inherit from base class `PipelineTemplateBase`. \n",
    "\n",
    "The **major tasks** are\n",
    "* *`Config` Classes Import*\n",
    "* *Configuration Validation*\n",
    "* *Step Builder Retrieval and Step Builder Map Creation*\n",
    "* *Configuration Map Creation*\n",
    "* **Pipeline DAG Generation**: ideally, user should create this DAG and use it as input\n",
    "* **Automatic Pipeline Assemble**: Call `pipeline_assembler`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16849f-e888-4496-9737-a87f6de06288",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32afaaa-1586-410c-959a-a23912e05d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ab571ca-b049-4788-80a4-1ac03870d2bc",
   "metadata": {},
   "source": [
    "### DAG to Template Compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44a61b62-571b-4440-a3c1-17048474240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cursus.api.dag.base_dag import PipelineDAG\n",
    "from cursus.core.compiler.dag_compiler import compile_dag_to_pipeline, PipelineDAGCompiler\n",
    "from cursus.core.compiler.validation import ConversionReport\n",
    "from cursus.steps.configs.utils import load_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6045ac10-6937-461e-9611-50aa83cc824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bedrock_batch_data_processing_dag() -> PipelineDAG:\n",
    "    \"\"\"\n",
    "    Create a DAG for Bedrock Batch data processing pipeline.\n",
    "\n",
    "    This DAG represents the simplest possible workflow that includes\n",
    "    cost-efficient Bedrock batch LLM enhancement for pure data processing\n",
    "    without any training, calibration, packaging, registration, or evaluation steps.\n",
    "    Perfect for data enhancement and annotation workflows.\n",
    "\n",
    "    Returns:\n",
    "        PipelineDAG: The directed acyclic graph for the pipeline\n",
    "    \"\"\"\n",
    "    dag = PipelineDAG()\n",
    "\n",
    "    # Add minimal data processing nodes with Bedrock batch enhancement\n",
    "    dag.add_node(\"DummyDataLoading_training\")  # Dummy data load\n",
    "    dag.add_node(\"TabularPreprocessing_training\")  # Tabular preprocessing\n",
    "    dag.add_node(\"BedrockPromptTemplateGeneration\")  # Bedrock prompt template generation\n",
    "    dag.add_node(\"BedrockBatchProcessing_training\")  # Bedrock batch processing step\n",
    "\n",
    "    # Simple data processing flow with Bedrock batch enhancement\n",
    "    dag.add_edge(\"DummyDataLoading_training\", \"TabularPreprocessing_training\")\n",
    "\n",
    "    # Bedrock batch processing flow - two inputs to BedrockBatchProcessing\n",
    "    dag.add_edge(\"TabularPreprocessing_training\", \"BedrockBatchProcessing_training\")  # Data input\n",
    "    dag.add_edge(\"BedrockPromptTemplateGeneration\", \"BedrockBatchProcessing_training\")  # Template input\n",
    "\n",
    "    logger.info(\n",
    "        f\"Created Bedrock Batch data processing DAG with {len(dag.nodes)} nodes and {len(dag.edges)} edges\"\n",
    "    )\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88e0d05f-404d-4732-b4b8-e3ae866a9f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:13,426 - INFO - Added node: DummyDataLoading_training\n",
      "2025-11-08 05:08:13,427 - INFO - Added node: TabularPreprocessing_training\n",
      "2025-11-08 05:08:13,427 - INFO - Added node: BedrockPromptTemplateGeneration\n",
      "2025-11-08 05:08:13,427 - INFO - Added node: BedrockBatchProcessing_training\n",
      "2025-11-08 05:08:13,428 - INFO - Added edge: DummyDataLoading_training -> TabularPreprocessing_training\n",
      "2025-11-08 05:08:13,428 - INFO - Added edge: TabularPreprocessing_training -> BedrockBatchProcessing_training\n",
      "2025-11-08 05:08:13,428 - INFO - Added edge: BedrockPromptTemplateGeneration -> BedrockBatchProcessing_training\n",
      "2025-11-08 05:08:13,429 - INFO - Created Bedrock Batch data processing DAG with 4 nodes and 3 edges\n"
     ]
    }
   ],
   "source": [
    "dag = create_bedrock_batch_data_processing_dag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82c4b4f5-171d-4a67-9c44-d95152104154",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_parameters = [\n",
    "    PIPELINE_EXECUTION_TEMP_DIR,\n",
    "    KMS_ENCRYPTION_KEY_PARAM,\n",
    "    SECURITY_GROUP_ID,\n",
    "    VPC_SUBNET,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "afe3f43a-b759-47be-93dd-4f60249b44db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:13,438 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,439 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,439 - INFO - âœ… BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-08 05:08:13,440 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,440 - INFO - ðŸŽ‰ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,441 - INFO - ðŸ” ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,441 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,441 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-08 05:08:13,442 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,442 - INFO - ðŸŽ‰ ScriptAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,442 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,443 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,444 - INFO - âœ… BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-08 05:08:13,444 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,445 - INFO - ðŸŽ‰ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,445 - INFO - ðŸ” ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,445 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,446 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-08 05:08:13,446 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,446 - INFO - ðŸŽ‰ ScriptAutoDiscovery initialization completed successfully\n"
     ]
    }
   ],
   "source": [
    "dag_compiler = PipelineDAGCompiler(\n",
    "    config_path=config_path,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    pipeline_parameters=pipeline_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c2242-932d-43c3-8b84-b81dfc5de16e",
   "metadata": {},
   "source": [
    "### Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0def820f-b4fc-4660-85f5-f6d6c832b985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7dbeb8-f0ba-42f2-9c55-fede4f97db62",
   "metadata": {},
   "source": [
    "#### DAG Validation and Preview of Config Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f1ce473-9034-472c-81ec-1edea11b38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_only = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f50ae6a-b1af-44e4-bab2-76dadeb33453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if preview_only:\n",
    "    preview = dag_compiler.preview_resolution(dag)\n",
    "    logger.info(\"DAG node resolution preview:\")\n",
    "    for node, config_type in preview.node_config_map.items():\n",
    "        confidence = preview.resolution_confidence.get(node, 0.0)\n",
    "        logger.info(f\"  {node} â†’ {config_type} (confidence: {confidence:.2f})\")\n",
    "        \n",
    "    if preview.recommendations:\n",
    "        logger.info(\"Recommendations:\")\n",
    "        for recommendation in preview.recommendations:\n",
    "            logger.info(f\"  - {recommendation}\")\n",
    "        \n",
    "    validation = dag_compiler.validate_dag_compatibility(dag)\n",
    "    logger.info(f\"DAG validation: {'VALID' if validation.is_valid else 'INVALID'}\")\n",
    "    if not validation.is_valid:\n",
    "        if validation.missing_configs:\n",
    "            logger.warning(f\"Missing configs: {validation.missing_configs}\")\n",
    "        if validation.unresolvable_builders:\n",
    "            logger.warning(f\"Unresolvable builders: {validation.unresolvable_builders}\")\n",
    "        if validation.config_errors:\n",
    "            logger.warning(f\"Config errors: {validation.config_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15ff04-893e-41d1-a153-420405a38382",
   "metadata": {},
   "source": [
    "### Put it Together: Pipeline Generation from DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a8a68cc-a8a1-43d0-b5de-b3d473f13628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:13,463 - INFO - Converting DAG to pipeline\n",
      "2025-11-08 05:08:13,463 - INFO - Compiling DAG with detailed reporting\n",
      "2025-11-08 05:08:13,464 - INFO - Compiling DAG with 4 nodes to pipeline\n",
      "2025-11-08 05:08:13,464 - INFO - Creating template for DAG with 4 nodes\n",
      "2025-11-08 05:08:13,464 - WARNING - Could not import config_class_detector, using fallback implementation\n",
      "2025-11-08 05:08:13,465 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,465 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,466 - INFO - âœ… BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-08 05:08:13,466 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,467 - INFO - ðŸŽ‰ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,467 - INFO - ðŸ” ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,467 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,468 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-08 05:08:13,468 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,468 - INFO - ðŸŽ‰ ScriptAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,469 - INFO - Successfully discovered 60 config classes using step catalog\n",
      "2025-11-08 05:08:13,469 - INFO - Loading configs from: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,470 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,470 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,471 - INFO - âœ… BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-08 05:08:13,472 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,472 - INFO - ðŸŽ‰ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,472 - INFO - ðŸ” ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,473 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,473 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-08 05:08:13,474 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,474 - INFO - ðŸŽ‰ ScriptAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,474 - INFO - Successfully discovered 60 config classes using step catalog\n",
      "2025-11-08 05:08:13,475 - INFO - Loading configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,476 - INFO - Loading configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,477 - INFO - Successfully loaded configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,478 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json with 4 specific configs\n",
      "2025-11-08 05:08:13,478 - INFO - Creating additional config instance for BedrockBatchProcessing_training (BedrockBatchProcessingConfig)\n",
      "2025-11-08 05:08:13,479 - INFO - Creating additional config instance for BedrockPromptTemplateGeneration (BedrockPromptTemplateGenerationConfig)\n",
      "2025-11-08 05:08:13,481 - INFO - Generated system prompt config: docker/scripts/prompt_configs/system_prompt.json\n",
      "2025-11-08 05:08:13,482 - INFO - Generated output format config: docker/scripts/prompt_configs/output_format.json\n",
      "2025-11-08 05:08:13,482 - INFO - Generated instruction config: docker/scripts/prompt_configs/instruction.json\n",
      "2025-11-08 05:08:13,483 - INFO - Skipping category_definitions.json generation (no category definitions available)\n",
      "2025-11-08 05:08:13,483 - INFO - Generated prompt configuration bundle in: docker/scripts/prompt_configs\n",
      "2025-11-08 05:08:13,483 - INFO - Bundle contains 3 JSON configuration files: system_prompt.json, output_format.json, instruction.json\n",
      "2025-11-08 05:08:13,484 - INFO - Auto-generated prompt configuration bundle at: docker/scripts/prompt_configs\n",
      "2025-11-08 05:08:13,484 - INFO - Creating additional config instance for DummyDataLoading_training (DummyDataLoadingConfig)\n",
      "2025-11-08 05:08:13,485 - INFO - Creating additional config instance for TabularPreprocessing_training (TabularPreprocessingConfig)\n",
      "2025-11-08 05:08:13,486 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,487 - INFO - Loaded raw configuration data from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,487 - INFO - Initialized registry manager with workspace context: None\n",
      "2025-11-08 05:08:13,487 - INFO - Created specification registry for context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,488 - INFO - Created new workspace-aware registry for context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,489 - INFO - Created registry manager for context: lukexie-BuyerAbuseRnR-pytorch-NA\n",
      "2025-11-08 05:08:13,489 - INFO - Created dependency resolver for context: lukexie-BuyerAbuseRnR-pytorch-NA\n",
      "2025-11-08 05:08:13,489 - INFO - Skipping configuration validation (requested)\n",
      "2025-11-08 05:08:13,490 - INFO - Pipeline name 'lukexie-BuyerAbuseRnR-pytorch-NA-0.0.1-pipeline' sanitized to 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' to conform to SageMaker constraints\n",
      "2025-11-08 05:08:13,490 - INFO - Initialized template for: lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline\n",
      "2025-11-08 05:08:13,490 - INFO - Successfully created template\n",
      "2025-11-08 05:08:13,491 - INFO - Pipeline name 'lukexie-BuyerAbuseRnR-pytorch-NA-0.0.1-pipeline' sanitized to 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' to conform to SageMaker constraints\n",
      "2025-11-08 05:08:13,491 - INFO - Generating pipeline: lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline\n",
      "2025-11-08 05:08:13,491 - INFO - Resolving 4 DAG nodes to configurations\n",
      "2025-11-08 05:08:13,492 - INFO - Using metadata from loaded configuration\n",
      "2025-11-08 05:08:13,492 - INFO - Found exact key match for node 'DummyDataLoading_training'\n",
      "2025-11-08 05:08:13,492 - INFO - Found exact key match for node 'TabularPreprocessing_training'\n",
      "2025-11-08 05:08:13,493 - INFO - Found exact key match for node 'BedrockPromptTemplateGeneration'\n",
      "2025-11-08 05:08:13,493 - INFO - Found exact key match for node 'BedrockBatchProcessing_training'\n",
      "2025-11-08 05:08:13,493 - INFO - Successfully resolved all 4 nodes\n",
      "2025-11-08 05:08:13,500 - INFO - Index built successfully in 0.007s with 96 steps\n",
      "2025-11-08 05:08:13,522 - WARNING - Error loading class RegistrationStepBuilder from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus/steps/builders/builder_registration_step.py (relative module: ..steps.builders.builder_registration_step): No module named 'secure_ai_sandbox_workflow_python_sdk'\n",
      "2025-11-08 05:08:13,551 - WARNING - Error loading class CradleDataLoadingStepBuilder from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus/steps/builders/builder_cradle_data_loading_step.py (relative module: ..steps.builders.builder_cradle_data_loading_step): No module named 'secure_ai_sandbox_workflow_python_sdk'\n",
      "2025-11-08 05:08:13,562 - INFO - Builder discovery complete: 29 builders found\n",
      "2025-11-08 05:08:13,563 - INFO - Using 33 registered step builders from StepCatalog\n",
      "2025-11-08 05:08:13,564 - INFO - Successfully loaded builder class using base name 'DummyDataLoading' for 'DummyDataLoading_training': DummyDataLoadingStepBuilder\n",
      "2025-11-08 05:08:13,564 - INFO - Successfully loaded builder class using base name 'TabularPreprocessing' for 'TabularPreprocessing_training': TabularPreprocessingStepBuilder\n",
      "2025-11-08 05:08:13,565 - INFO - Successfully loaded builder class using base name 'BedrockBatchProcessing' for 'BedrockBatchProcessing_training': BedrockBatchProcessingStepBuilder\n",
      "2025-11-08 05:08:13,565 - INFO - Using provided StepCatalog instance\n",
      "2025-11-08 05:08:13,565 - INFO - Using stored custom pipeline parameters\n",
      "2025-11-08 05:08:13,566 - INFO - Successfully loaded builder class using base name 'DummyDataLoading' for 'DummyDataLoading_training': DummyDataLoadingStepBuilder\n",
      "2025-11-08 05:08:13,566 - INFO - Successfully loaded builder class using base name 'TabularPreprocessing' for 'TabularPreprocessing_training': TabularPreprocessingStepBuilder\n",
      "2025-11-08 05:08:13,566 - INFO - Successfully loaded builder class using base name 'BedrockBatchProcessing' for 'BedrockBatchProcessing_training': BedrockBatchProcessingStepBuilder\n",
      "2025-11-08 05:08:13,567 - INFO - Input validation successful\n",
      "2025-11-08 05:08:13,567 - INFO - Initializing step builders\n",
      "2025-11-08 05:08:13,567 - INFO - Successfully loaded builder class using base name 'DummyDataLoading' for 'DummyDataLoading_training': DummyDataLoadingStepBuilder\n",
      "2025-11-08 05:08:13,568 - INFO - Using training-specific DUMMY_DATA_LOADING_TRAINING_SPEC\n",
      "2025-11-08 05:08:13,568 - INFO - Initializing DummyDataLoadingStepBuilder with region: NA\n",
      "2025-11-08 05:08:13,568 - INFO - Validating DummyDataLoadingConfig...\n",
      "2025-11-08 05:08:13,569 - INFO - DummyDataLoadingConfig validation succeeded.\n",
      "2025-11-08 05:08:13,569 - INFO - Set execution prefix for DummyDataLoading_training\n",
      "2025-11-08 05:08:13,569 - INFO - Initialized builder for step DummyDataLoading_training using StepCatalog\n",
      "2025-11-08 05:08:13,570 - INFO - Successfully loaded builder class using base name 'TabularPreprocessing' for 'TabularPreprocessing_training': TabularPreprocessingStepBuilder\n",
      "2025-11-08 05:08:13,570 - INFO - Using specification for training\n",
      "2025-11-08 05:08:13,570 - INFO - Initializing TabularPreprocessingStepBuilder with region: NA\n",
      "2025-11-08 05:08:13,572 - INFO - Set execution prefix for TabularPreprocessing_training\n",
      "2025-11-08 05:08:13,573 - INFO - Initialized builder for step TabularPreprocessing_training using StepCatalog\n",
      "2025-11-08 05:08:13,573 - INFO - Initializing BedrockPromptTemplateGenerationStepBuilder with region: NA\n",
      "2025-11-08 05:08:13,573 - INFO - Validating BedrockPromptTemplateGenerationConfig...\n",
      "2025-11-08 05:08:13,574 - INFO - BedrockPromptTemplateGenerationConfig validation succeeded.\n",
      "2025-11-08 05:08:13,574 - INFO - Set execution prefix for BedrockPromptTemplateGeneration\n",
      "2025-11-08 05:08:13,575 - INFO - Initialized builder for step BedrockPromptTemplateGeneration using StepCatalog\n",
      "2025-11-08 05:08:13,575 - INFO - Successfully loaded builder class using base name 'BedrockBatchProcessing' for 'BedrockBatchProcessing_training': BedrockBatchProcessingStepBuilder\n",
      "2025-11-08 05:08:13,576 - INFO - Using bedrock batch processing specification for job type: training\n",
      "2025-11-08 05:08:13,576 - INFO - Initializing BedrockBatchProcessingStepBuilder with region: NA\n",
      "2025-11-08 05:08:13,577 - INFO - Validating BedrockBatchProcessingConfig...\n",
      "2025-11-08 05:08:13,577 - INFO - BedrockBatchProcessingConfig validation succeeded.\n",
      "2025-11-08 05:08:13,577 - INFO - Set execution prefix for BedrockBatchProcessing_training\n",
      "2025-11-08 05:08:13,578 - INFO - Initialized builder for step BedrockBatchProcessing_training using StepCatalog\n",
      "2025-11-08 05:08:13,578 - INFO - Initialized 4 step builders in 0.01 seconds\n",
      "2025-11-08 05:08:13,578 - INFO - Generating pipeline: lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline\n",
      "2025-11-08 05:08:13,579 - INFO - Initializing step connections using specifications\n",
      "2025-11-08 05:08:13,579 - INFO - Matched TabularPreprocessing_training.DATA to DummyDataLoading_training.DATA (score: 1.00)\n",
      "2025-11-08 05:08:13,580 - INFO - Matched TabularPreprocessing_training.SIGNATURE to DummyDataLoading_training.SIGNATURE (score: 1.00)\n",
      "2025-11-08 05:08:13,580 - INFO - Matched BedrockBatchProcessing_training.input_data to TabularPreprocessing_training.processed_data (score: 0.89)\n",
      "2025-11-08 05:08:13,581 - INFO - Matched BedrockBatchProcessing_training.prompt_templates to TabularPreprocessing_training.processed_data (score: 0.63)\n",
      "2025-11-08 05:08:13,582 - INFO - Matched BedrockBatchProcessing_training.validation_schema to TabularPreprocessing_training.processed_data (score: 0.63)\n",
      "2025-11-08 05:08:13,585 - INFO - Matched BedrockBatchProcessing_training.prompt_templates to BedrockPromptTemplateGeneration.prompt_templates (score: 1.00)\n",
      "2025-11-08 05:08:13,586 - INFO - Matched BedrockBatchProcessing_training.validation_schema to BedrockPromptTemplateGeneration.validation_schema (score: 1.00)\n",
      "2025-11-08 05:08:13,587 - INFO - Build order: ['DummyDataLoading_training', 'BedrockPromptTemplateGeneration', 'TabularPreprocessing_training', 'BedrockBatchProcessing_training']\n",
      "2025-11-08 05:08:13,587 - INFO - Using execution_prefix for base output path\n",
      "2025-11-08 05:08:13,588 - INFO - Creating Dummy Data Loading ProcessingStep...\n",
      "2025-11-08 05:08:13,588 - INFO - Added configuration environment variables: {...}\n",
      "2025-11-08 05:08:13,589 - INFO - Final dummy data loading environment variables: {...}\n",
      "2025-11-08 05:08:13,591 - INFO - Defaulting to only available Python version: py3\n",
      "2025-11-08 05:08:13,614 - INFO - Using user-provided data source from configuration: s3://buyer-seller-messaging-reversal/production-pipeline/raw-input/2025-11-03 -> /opt/ml/processing/input/data\n",
      "2025-11-08 05:08:13,614 - INFO - No command-line arguments needed for dummy data loading script\n",
      "2025-11-08 05:08:13,615 - INFO - Using script path: docker/scripts/dummy_data_loading.py\n",
      "2025-11-08 05:08:13,615 - INFO - Created ProcessingStep with name: DummyDataLoading-Training\n",
      "2025-11-08 05:08:13,616 - INFO - Built step DummyDataLoading_training\n",
      "2025-11-08 05:08:13,616 - INFO - Using execution_prefix for base output path\n",
      "2025-11-08 05:08:13,616 - INFO - Creating Bedrock Prompt Template Generation ProcessingStep...\n",
      "2025-11-08 05:08:13,617 - INFO - Bedrock prompt template generation environment variables: {...}\n",
      "2025-11-08 05:08:13,618 - INFO - Defaulting to only available Python version: py3\n",
      "2025-11-08 05:08:13,637 - INFO - Added prompt_configs input: docker/scripts/prompt_configs -> /opt/ml/processing/input/prompt_configs\n",
      "2025-11-08 05:08:13,637 - INFO - Added output 'prompt_templates': /opt/ml/processing/output/templates -> [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,637 - INFO - Added output 'template_metadata': /opt/ml/processing/output/metadata -> [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,638 - INFO - Added output 'validation_schema': /opt/ml/processing/output/schema -> [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,638 - INFO - Job arguments for bedrock prompt template generation script: [list with 4 items]\n",
      "2025-11-08 05:08:13,639 - INFO - Using script path: docker/scripts/bedrock_prompt_template_generation.py\n",
      "2025-11-08 05:08:13,639 - INFO - Created ProcessingStep with name: BedrockPromptTemplateGeneration\n",
      "2025-11-08 05:08:13,640 - INFO - Built step BedrockPromptTemplateGeneration\n",
      "2025-11-08 05:08:13,641 - INFO - Using execution_prefix for base output path\n",
      "2025-11-08 05:08:13,641 - INFO - Registered specification for step 'TabularPreprocessingStepStep' of type 'TabularPreprocessing_Training' in context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,641 - INFO - Registered specification for step 'DummyDataLoading-Training' of type 'DummyDataLoading_Training' in context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,642 - INFO - Best match for DATA: DummyDataLoading-Training.DATA (confidence: 1.000)\n",
      "2025-11-08 05:08:13,642 - INFO - Resolved TabularPreprocessingStepStep.DATA -> DummyDataLoading-Training.DATA\n",
      "2025-11-08 05:08:13,643 - INFO - Best match for SIGNATURE: DummyDataLoading-Training.SIGNATURE (confidence: 1.000)\n",
      "2025-11-08 05:08:13,643 - INFO - Resolved TabularPreprocessingStepStep.SIGNATURE -> DummyDataLoading-Training.SIGNATURE\n",
      "2025-11-08 05:08:13,645 - INFO - Defaulting to only available Python version: py3\n",
      "2025-11-08 05:08:13,851 - INFO - Setting job_type argument to: training\n",
      "2025-11-08 05:08:13,852 - INFO - Using script path: docker/scripts/tabular_preprocessing.py\n",
      "2025-11-08 05:08:13,853 - INFO - Built step TabularPreprocessing_training\n",
      "2025-11-08 05:08:13,853 - INFO - Using execution_prefix for base output path\n",
      "2025-11-08 05:08:13,854 - INFO - Creating Bedrock Batch Processing ProcessingStep...\n",
      "2025-11-08 05:08:13,854 - INFO - Registered specification for step 'BedrockBatchProcessingStepStep' of type 'BedrockBatchProcessing' in context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,855 - INFO - Registered specification for step 'TabularPreprocessing-Training' of type 'TabularPreprocessing_Training' in context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,855 - INFO - Registered specification for step 'BedrockPromptTemplateGeneration' of type 'BedrockPromptTemplateGeneration' in context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,857 - INFO - Best match for input_data: TabularPreprocessing-Training.processed_data (confidence: 0.890)\n",
      "2025-11-08 05:08:13,857 - INFO - Resolved BedrockBatchProcessingStepStep.input_data -> TabularPreprocessing-Training.processed_data\n",
      "2025-11-08 05:08:13,859 - INFO - Best match for prompt_templates: BedrockPromptTemplateGeneration.prompt_templates (confidence: 1.000)\n",
      "2025-11-08 05:08:13,860 - INFO - Resolved BedrockBatchProcessingStepStep.prompt_templates -> BedrockPromptTemplateGeneration.prompt_templates\n",
      "2025-11-08 05:08:13,862 - INFO - Best match for validation_schema: BedrockPromptTemplateGeneration.validation_schema (confidence: 1.000)\n",
      "2025-11-08 05:08:13,862 - INFO - Resolved BedrockBatchProcessingStepStep.validation_schema -> BedrockPromptTemplateGeneration.validation_schema\n",
      "2025-11-08 05:08:13,863 - INFO - Using execution_prefix for base output path\n",
      "2025-11-08 05:08:13,863 - INFO - Bedrock batch processing environment variables configured\n",
      "2025-11-08 05:08:13,864 - INFO - Batch input S3 path: [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,864 - INFO - Batch output S3 path: [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,864 - INFO - Batch processing mode: auto\n",
      "2025-11-08 05:08:13,865 - INFO - Batch threshold: 1000 records\n",
      "2025-11-08 05:08:13,866 - INFO - Defaulting to only available Python version: py3\n",
      "2025-11-08 05:08:13,885 - INFO - Added input 'input_data': [Pipeline Variable: Properties] -> /opt/ml/processing/input/data\n",
      "2025-11-08 05:08:13,885 - INFO - Added input 'prompt_templates': [Pipeline Variable: Properties] -> /opt/ml/processing/input/templates\n",
      "2025-11-08 05:08:13,886 - INFO - Added input 'validation_schema': [Pipeline Variable: Properties] -> /opt/ml/processing/input/schema\n",
      "2025-11-08 05:08:13,886 - INFO - Added output 'processed_data': /opt/ml/processing/output/data -> [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,887 - INFO - Added output 'analysis_summary': /opt/ml/processing/output/summary -> [Pipeline Variable: Join]\n",
      "2025-11-08 05:08:13,887 - INFO - Setting job_type argument to: training\n",
      "2025-11-08 05:08:13,887 - INFO - Batch processing configuration: {...}\n",
      "2025-11-08 05:08:13,888 - INFO - Cost optimization: {...}\n",
      "2025-11-08 05:08:13,888 - INFO - Expected processing performance: {...}\n",
      "2025-11-08 05:08:13,888 - INFO - Job arguments for bedrock batch processing script: [list with 6 items]\n",
      "2025-11-08 05:08:13,889 - INFO - Using script path: docker/scripts/bedrock_batch_processing.py\n",
      "2025-11-08 05:08:13,889 - INFO - Created ProcessingStep with name: BedrockBatchProcessing-Training\n",
      "2025-11-08 05:08:13,889 - INFO - Primary model: anthropic.claude-sonnet-4-5-20250929-v1:0\n",
      "2025-11-08 05:08:13,890 - INFO - Fallback model: anthropic.claude-sonnet-4-20250514-v1:0\n",
      "2025-11-08 05:08:13,890 - INFO - Batch processing mode: auto\n",
      "2025-11-08 05:08:13,890 - INFO - Batch threshold: 1000 records\n",
      "2025-11-08 05:08:13,892 - INFO - Batch role ARN: arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default\n",
      "2025-11-08 05:08:13,892 - INFO - Expected cost savings: Up to 50% for datasets >= 1000 records\n",
      "2025-11-08 05:08:13,893 - INFO - Processing mode: Auto selection (batch for >= 1000 records)\n",
      "2025-11-08 05:08:13,893 - INFO - Production ready: True\n",
      "2025-11-08 05:08:13,893 - INFO - Built step BedrockBatchProcessing_training\n",
      "2025-11-08 05:08:13,898 - INFO - Generated pipeline lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline with 4 steps in 0.32 seconds\n",
      "2025-11-08 05:08:13,899 - INFO - Stored 4 step instances\n",
      "2025-11-08 05:08:13,899 - INFO - Pipeline name 'lukexie-BuyerAbuseRnR-pytorch-NA-0.0.1-pipeline' sanitized to 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' to conform to SageMaker constraints\n",
      "2025-11-08 05:08:13,900 - INFO - Successfully compiled DAG to pipeline: lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline\n",
      "2025-11-08 05:08:13,900 - INFO - Previewing resolution for 4 DAG nodes\n",
      "2025-11-08 05:08:13,900 - INFO - Creating template for DAG with 4 nodes\n",
      "2025-11-08 05:08:13,901 - INFO - Loading configs from: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,901 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,901 - INFO - ðŸ”§ BuilderAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,902 - INFO - âœ… BuilderAutoDiscovery basic initialization complete\n",
      "2025-11-08 05:08:13,903 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,903 - INFO - ðŸŽ‰ BuilderAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,904 - INFO - ðŸ” ScriptAutoDiscovery.__init__ starting - package_root: /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/cursus\n",
      "2025-11-08 05:08:13,904 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - workspace_dirs: []\n",
      "2025-11-08 05:08:13,904 - INFO - ðŸ” ScriptAutoDiscovery.__init__ - priority_workspace_dir: None\n",
      "2025-11-08 05:08:13,905 - INFO - âœ… Registry info loaded: 34 steps\n",
      "2025-11-08 05:08:13,905 - INFO - ðŸŽ‰ ScriptAutoDiscovery initialization completed successfully\n",
      "2025-11-08 05:08:13,905 - INFO - Successfully discovered 60 config classes using step catalog\n",
      "2025-11-08 05:08:13,906 - INFO - Loading configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,907 - INFO - Loading configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,908 - INFO - Successfully loaded configuration from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,909 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json with 4 specific configs\n",
      "2025-11-08 05:08:13,909 - INFO - Creating additional config instance for BedrockBatchProcessing_training (BedrockBatchProcessingConfig)\n",
      "2025-11-08 05:08:13,910 - INFO - Creating additional config instance for BedrockPromptTemplateGeneration (BedrockPromptTemplateGenerationConfig)\n",
      "2025-11-08 05:08:13,912 - INFO - Generated system prompt config: docker/scripts/prompt_configs/system_prompt.json\n",
      "2025-11-08 05:08:13,913 - INFO - Generated output format config: docker/scripts/prompt_configs/output_format.json\n",
      "2025-11-08 05:08:13,913 - INFO - Generated instruction config: docker/scripts/prompt_configs/instruction.json\n",
      "2025-11-08 05:08:13,914 - INFO - Skipping category_definitions.json generation (no category definitions available)\n",
      "2025-11-08 05:08:13,914 - INFO - Generated prompt configuration bundle in: docker/scripts/prompt_configs\n",
      "2025-11-08 05:08:13,914 - INFO - Bundle contains 3 JSON configuration files: system_prompt.json, output_format.json, instruction.json\n",
      "2025-11-08 05:08:13,915 - INFO - Auto-generated prompt configuration bundle at: docker/scripts/prompt_configs\n",
      "2025-11-08 05:08:13,915 - INFO - Creating additional config instance for DummyDataLoading_training (DummyDataLoadingConfig)\n",
      "2025-11-08 05:08:13,916 - INFO - Creating additional config instance for TabularPreprocessing_training (TabularPreprocessingConfig)\n",
      "2025-11-08 05:08:13,917 - INFO - Successfully loaded configs from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,918 - INFO - Loaded raw configuration data from /home/ec2-user/SageMaker/AmazonSageMaker-lukexie-sagemaker-bsm-repo/pipelines/rnr_pytorch_bedrock/pipeline_config/config.json\n",
      "2025-11-08 05:08:13,918 - INFO - Initialized registry manager with workspace context: None\n",
      "2025-11-08 05:08:13,918 - INFO - Created specification registry for context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,919 - INFO - Created new workspace-aware registry for context 'lukexie-BuyerAbuseRnR-pytorch-NA'\n",
      "2025-11-08 05:08:13,920 - INFO - Created registry manager for context: lukexie-BuyerAbuseRnR-pytorch-NA\n",
      "2025-11-08 05:08:13,920 - INFO - Created dependency resolver for context: lukexie-BuyerAbuseRnR-pytorch-NA\n",
      "2025-11-08 05:08:13,920 - INFO - Validating dynamic pipeline configuration\n",
      "2025-11-08 05:08:13,921 - INFO - Resolving 4 DAG nodes to configurations\n",
      "2025-11-08 05:08:13,921 - INFO - Using metadata from loaded configuration\n",
      "2025-11-08 05:08:13,921 - INFO - Found exact key match for node 'DummyDataLoading_training'\n",
      "2025-11-08 05:08:13,922 - INFO - Found exact key match for node 'TabularPreprocessing_training'\n",
      "2025-11-08 05:08:13,923 - INFO - Found exact key match for node 'BedrockPromptTemplateGeneration'\n",
      "2025-11-08 05:08:13,923 - INFO - Found exact key match for node 'BedrockBatchProcessing_training'\n",
      "2025-11-08 05:08:13,923 - INFO - Successfully resolved all 4 nodes\n",
      "2025-11-08 05:08:13,924 - INFO - Using 33 registered step builders from StepCatalog\n",
      "2025-11-08 05:08:13,925 - INFO - Successfully loaded builder class using base name 'DummyDataLoading' for 'DummyDataLoading_training': DummyDataLoadingStepBuilder\n",
      "2025-11-08 05:08:13,925 - INFO - Successfully loaded builder class using base name 'TabularPreprocessing' for 'TabularPreprocessing_training': TabularPreprocessingStepBuilder\n",
      "2025-11-08 05:08:13,925 - INFO - Successfully loaded builder class using base name 'BedrockBatchProcessing' for 'BedrockBatchProcessing_training': BedrockBatchProcessingStepBuilder\n",
      "2025-11-08 05:08:13,926 - INFO - Configuration validation passed successfully\n",
      "2025-11-08 05:08:13,926 - INFO - Pipeline name 'lukexie-BuyerAbuseRnR-pytorch-NA-0.0.1-pipeline' sanitized to 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' to conform to SageMaker constraints\n",
      "2025-11-08 05:08:13,927 - INFO - Initialized template for: lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline\n",
      "2025-11-08 05:08:13,927 - INFO - Successfully created template\n",
      "2025-11-08 05:08:13,928 - INFO - Using metadata.config_types mapping with 4 entries\n",
      "2025-11-08 05:08:13,928 - INFO - Found exact key match for node 'DummyDataLoading_training'\n",
      "2025-11-08 05:08:13,929 - INFO - Found exact key match for node 'TabularPreprocessing_training'\n",
      "2025-11-08 05:08:13,929 - INFO - Found exact key match for node 'BedrockPromptTemplateGeneration'\n",
      "2025-11-08 05:08:13,929 - INFO - Found exact key match for node 'BedrockBatchProcessing_training'\n",
      "2025-11-08 05:08:13,930 - ERROR - Failed to generate resolution preview: 0\n",
      "2025-11-08 05:08:13,930 - INFO - Compilation completed with report: Pipeline 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' created successfully with 4 steps (avg confidence: 0.00)\n",
      "2025-11-08 05:08:13,930 - INFO - Conversion complete: Pipeline 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' created successfully with 4 steps (avg confidence: 0.00)\n",
      "2025-11-08 05:08:13,931 - INFO - Pipeline 'lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline' created successfully\n",
      "2025-11-08 05:08:13,931 - INFO - Pipeline ARN: Not available until upserted\n",
      "2025-11-08 05:08:13,931 - INFO - To upsert the pipeline, call pipeline.upsert()\n"
     ]
    }
   ],
   "source": [
    "# Convert DAG to pipeline and get report\n",
    "try:\n",
    "    logger.info(f\"Converting DAG to pipeline\")\n",
    "    template_pipeline, report = dag_compiler.compile_with_report(\n",
    "        dag=dag\n",
    "    )\n",
    "        \n",
    "    # Log report summary\n",
    "    logger.info(f\"Conversion complete: {report.summary()}\")\n",
    "    for node, details in report.resolution_details.items():\n",
    "        logger.info(f\"  {node} â†’ {details['config_type']} ({details['builder_type']})\")\n",
    "        \n",
    "    # Log pipeline creation details\n",
    "    logger.info(f\"Pipeline '{template_pipeline.name}' created successfully\")\n",
    "    logger.info(f\"Pipeline ARN: {template_pipeline.arn if hasattr(template_pipeline, 'arn') else 'Not available until upserted'}\")\n",
    "    logger.info(\"To upsert the pipeline, call pipeline.upsert()\")       \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to convert DAG to pipeline: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee855a-7a10-43e2-aa69-a26ec7bc3fc7",
   "metadata": {},
   "source": [
    "### Pipeline Template\n",
    "\n",
    "After the pipeline is generated, we can retrieve the pipeline template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b24ae2b-ad4f-40db-9389-d405d80e6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_template_builder = dag_compiler.get_last_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1152c1e2-f1d4-458b-83d9-6437fc7c2e75",
   "metadata": {},
   "source": [
    "## Start Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0448dafe-595c-4c14-80d5-260cbe6abac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::178936618742:role/AmazonSageMaker-ExecutionRole-Default'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_arn = pipeline_session.get_caller_identity_arn()\n",
    "role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3c473d23-c3e0-475a-8e72-74fb89b3be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_description=PIPELINE_DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e406ca9-6452-4730-9997-e09c21f2baa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BuyerAbuseRnR pytorch Model NA'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1ab84-5565-4950-848c-cd5f70fe7b99",
   "metadata": {},
   "source": [
    "### Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5436920-9867-4d17-b670-88b4361da282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 05:08:14,243 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:14,606 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:14,652 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:14,713 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:15,041 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:15,085 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:15,129 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-11-08 05:08:15,230 - WARNING - Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:178936618742:pipeline/lukexie-BuyerAbuseRnR-pytorch-NA-0-0-1-pipeline',\n",
       " 'PipelineVersionId': 39,\n",
       " 'ResponseMetadata': {'RequestId': 'edb8c0ac-7263-4a82-9688-814c4413198d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'edb8c0ac-7263-4a82-9688-814c4413198d',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '138',\n",
       "   'date': 'Sat, 08 Nov 2025 05:08:15 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_pipeline.upsert(\n",
    "                role_arn=role_arn, description=pipeline_description\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc41fc",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41ba2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_parameters={\n",
    "    \"EXECUTION_S3_PREFIX\": f\"s3://{bucket_name}/pipeline/{PIPELINE_NAME}/{execution_id}\",\n",
    "    \"KMS_ENCRYPTION_KEY_PARAM\": kms_key_id,\n",
    "    \"VPC_SUBNET\": vpc_subnet_id,\n",
    "    \"SECURITY_GROUP_ID\": security_group_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30a156f0-b19e-4791-9160-38dc3c9209a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution = template_pipeline.start(\n",
    "                parameters=pipeline_execution_parameters\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50734c8a-50a7-4ed7-a431-67945560e51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91899306-eaae-4d7e-a2fc-702162e43a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
