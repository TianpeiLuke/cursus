{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Packaged Model from S3\n",
    "\n",
    "This notebook downloads and inspects the packaged model.tar.gz to verify:\n",
    "1. The circular import fix is present\n",
    "2. The package structure is correct\n",
    "3. All required files are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Model from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 location of your packaged model\n",
    "s3_uri = \"s3://sandboxdependency-abuse-secureaisandboxteamshare-1l77v9am252um/lukexie-Names3Risk-pytorch-NA-1-0-0-pipeline-2026-01-17-07-38-05/package/packaged_model/model.tar.gz\"\n",
    "\n",
    "# Parse S3 URI\n",
    "s3_parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "bucket_name = s3_parts[0]\n",
    "object_key = s3_parts[1]\n",
    "\n",
    "print(f\"Bucket: {bucket_name}\")\n",
    "print(f\"Key: {object_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "s3_client = boto3.client(\"s3\")\n",
    "local_tar_path = \"/tmp/model.tar.gz\"\n",
    "\n",
    "print(\"Downloading model.tar.gz from S3...\")\n",
    "s3_client.download_file(bucket_name, object_key, local_tar_path)\n",
    "print(f\"‚úì Downloaded to {local_tar_path}\")\n",
    "\n",
    "# Check file size\n",
    "file_size_mb = os.path.getsize(local_tar_path) / (1024 * 1024)\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract and Inspect Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract to temporary directory\n",
    "extract_dir = \"/tmp/extracted_model\"\n",
    "\n",
    "# Clean up if exists\n",
    "if os.path.exists(extract_dir):\n",
    "    shutil.rmtree(extract_dir)\n",
    "os.makedirs(extract_dir)\n",
    "\n",
    "print(\"Extracting model.tar.gz...\")\n",
    "with tarfile.open(local_tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(extract_dir)\n",
    "print(f\"‚úì Extracted to {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List directory structure\n",
    "def print_tree(directory, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Print directory tree structure\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "\n",
    "    items = sorted(Path(directory).iterdir(), key=lambda x: (not x.is_dir(), x.name))\n",
    "\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "        print(f\"{prefix}{current_prefix}{item.name}\")\n",
    "\n",
    "        if item.is_dir():\n",
    "            extension_prefix = \"    \" if is_last else \"‚îÇ   \"\n",
    "            print_tree(item, prefix + extension_prefix, max_depth, current_depth + 1)\n",
    "\n",
    "\n",
    "print(\"\\nüì¶ Package Structure:\")\n",
    "print(extract_dir)\n",
    "print_tree(extract_dir, max_depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Circular Import Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pytorch_inference_handler.py file\n",
    "handler_path = Path(extract_dir) / \"code\" / \"pytorch_inference_handler.py\"\n",
    "\n",
    "if handler_path.exists():\n",
    "    with open(handler_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    print(\"‚úì Found pytorch_inference_handler.py\")\n",
    "    print(f\"File size: {len(content)} bytes\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "else:\n",
    "    print(\"‚úó pytorch_inference_handler.py not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the circular import fix\n",
    "print(\"üîç Checking for Circular Import Fix...\\n\")\n",
    "\n",
    "# Check 1: Module-level AutoTokenizer import should be REMOVED\n",
    "lines = content.split(\"\\n\")\n",
    "module_level_imports = []\n",
    "for i, line in enumerate(lines[:400], 1):  # Check first 400 lines\n",
    "    if \"from transformers import AutoTokenizer\" in line and not line.strip().startswith(\n",
    "        \"#\"\n",
    "    ):\n",
    "        # Check if it's at module level (not inside a function)\n",
    "        # Look backwards to see if we're inside a function\n",
    "        in_function = False\n",
    "        for j in range(max(0, i - 50), i):\n",
    "            if \"def \" in lines[j]:\n",
    "                in_function = True\n",
    "                break\n",
    "\n",
    "        if not in_function:\n",
    "            module_level_imports.append((i, line))\n",
    "\n",
    "if module_level_imports:\n",
    "    print(\"‚ùå ISSUE FOUND: Module-level AutoTokenizer import still exists!\")\n",
    "    for line_num, line in module_level_imports:\n",
    "        print(f\"   Line {line_num}: {line.strip()}\")\n",
    "else:\n",
    "    print(\"‚úÖ PASS: No module-level AutoTokenizer import found\")\n",
    "\n",
    "# Check 2: Function-level AutoTokenizer imports should be present\n",
    "function_level_count = 0\n",
    "for i, line in enumerate(lines, 1):\n",
    "    if \"from transformers import AutoTokenizer\" in line:\n",
    "        # Check if inside model_fn\n",
    "        in_model_fn = False\n",
    "        for j in range(max(0, i - 100), i):\n",
    "            if \"def model_fn\" in lines[j]:\n",
    "                in_model_fn = True\n",
    "                break\n",
    "\n",
    "        if in_model_fn:\n",
    "            function_level_count += 1\n",
    "            print(f\"‚úÖ Found function-level import at line {i}\")\n",
    "            # Show context\n",
    "            start = max(0, i - 3)\n",
    "            end = min(len(lines), i + 2)\n",
    "            print(\"\\nContext:\")\n",
    "            for idx in range(start, end):\n",
    "                marker = \">>>\" if idx == i - 1 else \"   \"\n",
    "                print(f\"{marker} {idx + 1:4d}: {lines[idx]}\")\n",
    "            print()\n",
    "\n",
    "if function_level_count == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No function-level AutoTokenizer imports found\")\n",
    "    print(\"   BERT tokenizer loading may fail!\")\n",
    "elif function_level_count >= 2:\n",
    "    print(\n",
    "        f\"\\n‚úÖ PASS: Found {function_level_count} function-level imports (expected 2)\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Found only {function_level_count} function-level import (expected 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Verify custom tokenizer import pattern\n",
    "print(\"\\nüîç Checking Custom Tokenizer Import Pattern...\\n\")\n",
    "\n",
    "custom_tokenizer_pattern = \"from tokenizers import Tokenizer\"\n",
    "found_custom_import = False\n",
    "\n",
    "for i, line in enumerate(lines, 1):\n",
    "    if custom_tokenizer_pattern in line and not line.strip().startswith(\"#\"):\n",
    "        # Check if inside model_fn\n",
    "        in_model_fn = False\n",
    "        for j in range(max(0, i - 100), i):\n",
    "            if \"def model_fn\" in lines[j]:\n",
    "                in_model_fn = True\n",
    "                break\n",
    "\n",
    "        if in_model_fn:\n",
    "            found_custom_import = True\n",
    "            print(f\"‚úÖ Found custom tokenizer import at line {i}\")\n",
    "            print(f\"   Pattern: function-level import (CORRECT)\")\n",
    "            # Show context\n",
    "            start = max(0, i - 3)\n",
    "            end = min(len(lines), i + 2)\n",
    "            print(\"\\nContext:\")\n",
    "            for idx in range(start, end):\n",
    "                marker = \">>>\" if idx == i - 1 else \"   \"\n",
    "                print(f\"{marker} {idx + 1:4d}: {lines[idx]}\")\n",
    "            break\n",
    "\n",
    "if found_custom_import:\n",
    "    print(\"\\n‚úÖ PASS: Custom tokenizer uses function-level import\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: Custom tokenizer import pattern not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check for Tokenizers Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if both custom_tokenizers and tokenizers directories exist\n",
    "code_dir = Path(extract_dir) / \"code\"\n",
    "\n",
    "custom_tokenizers_dir = code_dir / \"custom_tokenizers\"\n",
    "tokenizers_dir = code_dir / \"tokenizers\"\n",
    "\n",
    "print(\"üìÅ Checking tokenizer directories...\\n\")\n",
    "\n",
    "if custom_tokenizers_dir.exists():\n",
    "    print(f\"‚úÖ custom_tokenizers/ exists\")\n",
    "    files = list(custom_tokenizers_dir.glob(\"*\"))\n",
    "    for f in files:\n",
    "        print(f\"   - {f.name}\")\n",
    "else:\n",
    "    print(\"‚úó custom_tokenizers/ NOT found\")\n",
    "\n",
    "print()\n",
    "\n",
    "if tokenizers_dir.exists():\n",
    "    print(f\"‚ö†Ô∏è  tokenizers/ exists (may cause issues!)\")\n",
    "    files = list(tokenizers_dir.glob(\"*\"))\n",
    "    for f in files:\n",
    "        print(f\"   - {f.name}\")\n",
    "    print(\"\\n‚ö†Ô∏è  This directory shadows the HuggingFace tokenizers package!\")\n",
    "    print(\"   However, the fix should handle this correctly.\")\n",
    "else:\n",
    "    print(\"‚úÖ tokenizers/ NOT found (good - no shadowing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compile checks\n",
    "checks = [\n",
    "    (\"Module-level AutoTokenizer import removed\", len(module_level_imports) == 0),\n",
    "    (\"Function-level AutoTokenizer imports present\", function_level_count >= 2),\n",
    "    (\"Custom tokenizer uses function-level import\", found_custom_import),\n",
    "    (\"Handler file exists\", handler_path.exists()),\n",
    "]\n",
    "\n",
    "all_passed = all(result for _, result in checks)\n",
    "\n",
    "for check_name, result in checks:\n",
    "    status = \"‚úÖ PASS\" if result else \"‚ùå FAIL\"\n",
    "    print(f\"{status}: {check_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "if all_passed:\n",
    "    print(\"‚úÖ ALL CHECKS PASSED - Fix is properly deployed!\")\n",
    "    print(\"\\nThe circular import fix has been successfully applied.\")\n",
    "    print(\"transformer2risk/lstm2risk models will NOT import transformers.\")\n",
    "    print(\"BERT models will import AutoTokenizer only when needed.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  SOME CHECKS FAILED - Review the issues above\")\n",
    "    print(\"\\nThe fix may not be properly deployed.\")\n",
    "    print(\"Consider re-running the packaging step.\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Specific Code Sections (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model_fn tokenizer loading section\n",
    "print(\"üìÑ model_fn() Tokenizer Loading Section:\\n\")\n",
    "\n",
    "in_model_fn = False\n",
    "in_tokenizer_section = False\n",
    "line_buffer = []\n",
    "\n",
    "for i, line in enumerate(lines, 1):\n",
    "    if \"def model_fn\" in line:\n",
    "        in_model_fn = True\n",
    "\n",
    "    if in_model_fn and \"Loading tokenizer\" in line:\n",
    "        in_tokenizer_section = True\n",
    "        # Start from a few lines before\n",
    "        start_idx = max(0, i - 5)\n",
    "        for j in range(start_idx, i):\n",
    "            print(f\"{j:4d}: {lines[j]}\")\n",
    "\n",
    "    if in_tokenizer_section:\n",
    "        print(f\"{i:4d}: {line}\")\n",
    "\n",
    "        # Stop after Reconstruct pipelines section\n",
    "        if \"Reconstruct pipelines\" in line:\n",
    "            # Print a few more lines\n",
    "            for j in range(i, min(i + 3, len(lines))):\n",
    "                print(f\"{j + 1:4d}: {lines[j]}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up extracted files\n",
    "# shutil.rmtree(extract_dir)\n",
    "# os.remove(local_tar_path)\n",
    "# print(\"‚úì Cleaned up temporary files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
