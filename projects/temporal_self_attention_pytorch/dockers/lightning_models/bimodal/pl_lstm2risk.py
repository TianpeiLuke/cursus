#!/usr/bin/env python3
"""
LSTM2Risk Lightning Module for Bimodal Fraud Detection

PyTorch Lightning module implementing LSTM2Risk architecture for fraud detection
using text (customer names, emails) and tabular features (transaction data).

Architecture matches legacy lstm2risk.py exactly:
- Text encoder: LSTMEncoder with bidirectional LSTM + attention pooling
- Tabular encoder: BatchNorm + 2-layer MLP with LayerNorm
- Fusion classifier: 3x ResidualBlocks + 2-layer MLP

Supports:
- Binary and multiclass classification
- Distributed training (FSDP, DDP)
- ONNX export for inference
- Class weight balancing
- Gradient checkpointing (memory optimization)

Usage:
```python
from names3risk_pytorch.dockers.lightning_models.bimodal import LSTM2RiskLightning
from names3risk_pytorch.dockers.hyperparams import LSTM2RiskHyperparameters

hyperparams = LSTM2RiskHyperparameters(
    n_embed=4000,
    embedding_size=16,
    hidden_size=128,
    n_lstm_layers=4,
    dropout_rate=0.2,
    ...
)

model = LSTM2RiskLightning(hyperparams)
trainer = pl.Trainer(max_epochs=10)
trainer.fit(model, train_dataloader, val_dataloader)
```

References:
- Legacy: projects/names3risk_legacy/lstm2risk.py
- Architecture: slipbox/4_analysis/2026-01-05_names3risk_pytorch_component_correspondence_analysis.md
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union

import lightning.pytorch as pl
import onnx
import torch
import torch.nn as nn
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.optim import AdamW
from torch.optim.lr_scheduler import OneCycleLR

from ...hyperparams.hyperparameters_lstm2risk import LSTM2RiskHyperparameters
from ...pytorch.blocks import LSTMEncoder
from ...pytorch.feedforward import ResidualBlock

# Assuming these utilities exist from other Lightning modules
try:
    from ..utils.dist_utils import all_gather, get_rank
    from ..utils.pl_model_plots import compute_metrics
except ImportError:
    # Fallback for testing without full environment
    def all_gather(data):
        return [data]

    def get_rank():
        return 0

    def compute_metrics(preds, labels, metric_choices, task, num_classes, prefix):
        return {}


# =================== Logging Setup =================================
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

handler = logging.StreamHandler()
handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.propagate = False


class LSTM2RiskLightning(pl.LightningModule):
    """
    PyTorch Lightning module for LSTM2Risk bimodal fraud detection.

    Architecture (matches legacy lstm2risk.py exactly):
    1. Text encoder: Bidirectional LSTM with attention pooling
       - Token embedding (vocab_size, embedding_dim)
       - Bidirectional LSTM (embedding_dim → hidden_dim, num_layers=4)
       - Attention pooling (2*hidden_dim → 1 attention weight)
       - Layer normalization (2*hidden_dim)
       - Output: (B, 2*hidden_dim)

    2. Tabular encoder: MLP with batch normalization
       - BatchNorm (input_tab_dim)
       - Linear (input_tab_dim → 2*hidden_dim)
       - ReLU + Dropout
       - Linear (2*hidden_dim → 2*hidden_dim)
       - LayerNorm + ReLU + Dropout
       - Output: (B, 2*hidden_dim)

    3. Fusion classifier: Residual blocks + MLP
       - 3x ResidualBlock (4*hidden_dim, expansion=4x, pre-norm)
       - Each followed by ReLU + Dropout
       - Linear (4*hidden_dim → hidden_dim)
       - ReLU + Dropout
       - Linear (hidden_dim → num_classes)
       - Output: (B, num_classes) logits

    Args:
        hyperparams: LSTM2RiskHyperparameters configuration object

    Batch Input Format:
        - batch["text"]: (B, L) - token IDs (padded, sorted by length descending)
        - batch["text_length"]: (B,) - original sequence lengths
        - batch["tabular"]: (B, F) - tabular features
        - batch["label"]: (B,) - class labels (LongTensor)

    Training Features:
        - Distributed training support (FSDP, DDP)
        - Mixed precision training (AMP)
        - Gradient checkpointing (optional, for memory)
        - Class weight balancing
        - Linear warmup + cosine decay scheduler
        - Metrics: accuracy, F1, precision, recall, AUROC, etc.

    ONNX Export:
        - Supports ONNX export for production inference
        - Handles LSTM limitations (pack_padded_sequence not supported)
        - Dynamic batch and sequence dimensions
        - Returns probabilities (not logits)
    """

    def __init__(self, hyperparams: LSTM2RiskHyperparameters):
        super().__init__()
        self.hyperparams = hyperparams
        self.model_class = "lstm2risk"

        # === Task configuration ===
        self.label_name = getattr(hyperparams, "label_name", "label")
        self.is_binary = hyperparams.is_binary
        self.task = "binary" if self.is_binary else "multiclass"
        self.num_classes = hyperparams.num_classes
        self.metric_choices = getattr(
            hyperparams, "metric_choices", ["accuracy", "f1_score"]
        )

        # === Training configuration ===
        self.model_path = getattr(hyperparams, "model_path", "")
        self.lr = hyperparams.lr
        self.weight_decay = getattr(hyperparams, "weight_decay", 0.0)
        self.adam_epsilon = getattr(hyperparams, "adam_epsilon", 1e-8)
        self.warmup_steps = getattr(hyperparams, "warmup_steps", 0)
        self.run_scheduler = getattr(hyperparams, "run_scheduler", True)

        # === Storage for predictions ===
        self.id_lst, self.pred_lst, self.label_lst = [], [], []
        self.test_output_folder = None
        self.test_has_label = False

        # ============================================================
        # 1. Text Encoder: LSTMEncoder (bidirectional LSTM + attention pooling)
        # ============================================================
        self.text_encoder = LSTMEncoder(
            vocab_size=hyperparams.n_embed,
            embedding_dim=hyperparams.embedding_size,
            hidden_dim=hyperparams.hidden_size,
            num_layers=hyperparams.n_lstm_layers,
            dropout=hyperparams.dropout_rate,
            bidirectional=True,
        )
        text_output_dim = 2 * hyperparams.hidden_size  # Bidirectional output

        # ============================================================
        # 2. Tabular Encoder: BatchNorm + 2-layer MLP
        # ============================================================
        tab_hidden_dim = 2 * hyperparams.hidden_size
        self.tab_encoder = nn.Sequential(
            nn.BatchNorm1d(hyperparams.input_tab_dim),
            nn.Linear(hyperparams.input_tab_dim, tab_hidden_dim),
            nn.ReLU(),
            nn.Dropout(hyperparams.dropout_rate),
            nn.Linear(tab_hidden_dim, tab_hidden_dim),
            nn.LayerNorm(tab_hidden_dim),
            nn.ReLU(),
            nn.Dropout(hyperparams.dropout_rate),
        )

        # ============================================================
        # 3. Fusion Classifier: ResidualBlocks + MLP
        # ============================================================
        fusion_dim = text_output_dim + tab_hidden_dim  # 4 * hidden_size
        self.classifier = nn.Sequential(
            # First residual block
            ResidualBlock(
                dim=fusion_dim,
                expansion_factor=4,  # 4x expansion like legacy
                dropout=0.0,  # Dropout applied after block
                activation="relu",
                norm_first=True,  # Pre-norm like legacy
            ),
            nn.ReLU(),
            nn.Dropout(hyperparams.dropout_rate),
            # Second residual block
            ResidualBlock(
                dim=fusion_dim,
                expansion_factor=4,
                dropout=0.0,
                activation="relu",
                norm_first=True,
            ),
            nn.ReLU(),
            nn.Dropout(hyperparams.dropout_rate),
            # Third residual block
            ResidualBlock(
                dim=fusion_dim,
                expansion_factor=4,
                dropout=0.0,
                activation="relu",
                norm_first=True,
            ),
            nn.ReLU(),
            nn.Dropout(hyperparams.dropout_rate),
            # Final MLP
            nn.Linear(fusion_dim, hyperparams.hidden_size),
            nn.ReLU(),
            nn.Dropout(hyperparams.dropout_rate),
            nn.Linear(hyperparams.hidden_size, self.num_classes),
        )

        # ============================================================
        # 4. Loss Function (CrossEntropyLoss with class weights)
        # ============================================================
        if hyperparams.class_weights is not None:
            class_weights_tensor = torch.tensor(
                hyperparams.class_weights, dtype=torch.float
            )
        else:
            class_weights_tensor = torch.ones(self.num_classes, dtype=torch.float)

        self.register_buffer("class_weights_tensor", class_weights_tensor)
        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights_tensor)

        # Save hyperparameters for Lightning checkpointing
        self.save_hyperparameters(hyperparams.model_dump())

    def forward(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Forward pass through LSTM2Risk model.

        Args:
            batch: Dictionary containing:
                - "text": (B, L) token IDs
                - "text_length": (B,) sequence lengths (optional)
                - "tabular": (B, F) tabular features

        Returns:
            logits: (B, num_classes) classification logits
        """
        # Extract inputs
        text_tokens = batch["text"]  # (B, L)
        text_lengths = batch.get("text_length")  # (B,) or None
        tab_data = batch["tabular"].float()  # (B, F)

        # Text encoding: LSTM + attention pooling
        text_hidden = self.text_encoder(text_tokens, text_lengths)  # (B, 2*H)

        # Tabular encoding: BatchNorm + MLP
        tab_hidden = self.tab_encoder(tab_data)  # (B, 2*H)

        # Fusion: concatenate and classify
        combined = torch.cat([text_hidden, tab_hidden], dim=1)  # (B, 4*H)
        logits = self.classifier(combined)  # (B, num_classes)

        return logits

    def configure_optimizers(self):
        """
        Configure optimizer and learning rate scheduler.

        Uses AdamW with weight decay + OneCycleLR scheduler (matches legacy).
        Applies weight decay to all parameters except biases and LayerNorm.

        OneCycleLR provides:
        - Linear warmup phase (10% of training by default)
        - Cosine annealing decay phase
        - Automatic per-step scheduling
        - Better convergence than linear warmup alone
        """
        # Separate parameters with/without weight decay
        no_decay = ["bias", "LayerNorm.weight", "LayerNorm.bias"]
        optimizer_grouped_parameters = [
            {
                "params": [
                    p
                    for n, p in self.named_parameters()
                    if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": self.weight_decay,
            },
            {
                "params": [
                    p
                    for n, p in self.named_parameters()
                    if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]

        optimizer = AdamW(
            optimizer_grouped_parameters, lr=self.lr, eps=self.adam_epsilon
        )

        if self.run_scheduler:
            # OneCycleLR scheduler (matches legacy lstm2risk.py)
            # - max_lr: Peak learning rate (self.lr)
            # - total_steps: Total training steps (automatic via Lightning)
            # - pct_start: Fraction of training for warmup (0.1 = 10%)
            # - anneal_strategy: 'cos' for cosine annealing (default)
            scheduler = OneCycleLR(
                optimizer,
                max_lr=self.lr,
                total_steps=self.trainer.estimated_stepping_batches,
                pct_start=0.1,  # 10% warmup (matches legacy)
                anneal_strategy="cos",
                cycle_momentum=True,
                base_momentum=0.85,
                max_momentum=0.95,
                div_factor=25.0,  # initial_lr = max_lr / div_factor
                final_div_factor=10000.0,  # min_lr = initial_lr / final_div_factor
            )

            return {
                "optimizer": optimizer,
                "lr_scheduler": {
                    "scheduler": scheduler,
                    "interval": "step",  # Step per batch (matches legacy)
                },
            }
        else:
            # No scheduler (constant learning rate)
            return optimizer

    def run_epoch(self, batch: Dict[str, torch.Tensor], stage: str):
        """
        Shared logic for train/val/test/predict steps.

        Args:
            batch: Input batch dictionary
            stage: One of "train", "val", "test", "pred"

        Returns:
            loss: Scalar loss (None for pred stage)
            preds: Predictions (probabilities or class probabilities)
            labels: Ground truth labels (None for pred stage)
        """
        # Get labels (if available)
        labels = batch.get(self.label_name) if stage != "pred" else None

        if labels is not None:
            if not isinstance(labels, torch.Tensor):
                labels = torch.tensor(labels, device=self.device)
            labels = labels.long()  # CrossEntropyLoss expects LongTensor

        # Forward pass
        logits = self(batch)  # (B, num_classes)

        # Compute loss
        loss = self.loss_fn(logits, labels) if labels is not None else None

        # Get predictions (probabilities)
        preds = torch.softmax(logits, dim=1)
        if self.is_binary:
            preds = preds[:, 1]  # Binary: return P(positive class)

        return loss, preds, labels

    def training_step(self, batch, batch_idx):
        """Training step."""
        loss, _, _ = self.run_epoch(batch, "train")
        self.log("train_loss", loss, sync_dist=True, prog_bar=True)
        return {"loss": loss}

    def on_validation_epoch_start(self):
        """Reset accumulators at start of validation."""
        self.pred_lst.clear()
        self.label_lst.clear()

    def validation_step(self, batch, batch_idx):
        """Validation step - accumulate predictions and labels."""
        loss, preds, labels = self.run_epoch(batch, "val")
        self.log("val_loss", loss, sync_dist=True, prog_bar=True)

        # Accumulate for epoch-level metrics
        self.pred_lst.extend(preds.detach().cpu().tolist())
        self.label_lst.extend(labels.detach().cpu().tolist())

    def on_validation_epoch_end(self):
        """Compute and log validation metrics across all GPUs."""
        # Sync predictions/labels across GPUs
        device = self.device
        preds = torch.tensor(sum(all_gather(self.pred_lst), []))
        labels = torch.tensor(sum(all_gather(self.label_lst), []))

        # Compute metrics
        metrics = compute_metrics(
            preds.to(device),
            labels.to(device),
            self.metric_choices,
            self.task,
            self.num_classes,
            "val",
        )
        self.log_dict(metrics, prog_bar=True)

    def on_test_epoch_start(self):
        """Setup for test epoch - create output directory."""
        self.id_lst.clear()
        self.pred_lst.clear()
        self.label_lst.clear()

        timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
        self.test_output_folder = (
            Path(self.model_path) / f"{self.model_class}-{timestamp}"
        )
        self.test_output_folder.mkdir(parents=True, exist_ok=True)

    def test_step(self, batch, batch_idx):
        """Test step - save predictions per rank."""
        mode = "test" if self.label_name in batch else "pred"
        self.test_has_label = mode == "test"

        loss, preds, labels = self.run_epoch(batch, mode)

        # Accumulate predictions
        self.pred_lst.extend(preds.detach().cpu().tolist())
        if labels is not None:
            self.label_lst.extend(labels.detach().cpu().tolist())

        if loss is not None:
            self.log("test_loss", loss, sync_dist=True, prog_bar=True)

    def on_test_epoch_end(self):
        """Save test results to TSV file (per GPU rank)."""
        import pandas as pd

        # Prepare results
        results = {}
        if self.is_binary:
            results["prob"] = self.pred_lst  # Binary: single probability
        else:
            # Multiclass: serialize probability vectors
            results["prob"] = [json.dumps(p) for p in self.pred_lst]

        if self.test_has_label:
            results[self.label_name] = self.label_lst

        # Save per-rank file
        df = pd.DataFrame(results)
        test_file = self.test_output_folder / f"test_result_rank{self.global_rank}.tsv"
        df.reset_index(drop=True).to_csv(test_file, sep="\t", index=False)
        logger.info(f"[Rank {self.global_rank}] Saved test results to {test_file}")

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        """Prediction step - return predictions only."""
        mode = "test" if self.label_name in batch else "pred"
        _, preds, labels = self.run_epoch(batch, mode)
        return (preds, labels) if mode == "test" else preds

    # ============================================================
    # ONNX Export
    # ============================================================

    def export_to_onnx(
        self,
        save_path: Union[str, Path],
        sample_batch: Dict[str, torch.Tensor],
    ):
        """
        Export model to ONNX format for production inference.

        IMPORTANT: ONNX export uses LSTM WITHOUT pack_padded_sequence due to
        ONNX limitations. The forward pass is mathematically equivalent but
        may have slightly different numerical results due to packing behavior.

        Args:
            save_path: Path to save ONNX model
            sample_batch: Sample batch for tracing, must contain:
                - "text": (B, L) token IDs
                - "text_length": (B,) sequence lengths
                - "tabular": (B, F) tabular features

        Example:
            >>> model.export_to_onnx(
            ...     "lstm2risk.onnx",
            ...     {"text": torch.randint(0, 1000, (2, 50)),
            ...      "text_length": torch.tensor([50, 45]),
            ...      "tabular": torch.randn(2, 100)}
            ... )
        """

        class LSTM2RiskONNXWrapper(nn.Module):
            """
            ONNX-compatible wrapper for LSTM2Risk.

            Key differences from training:
            - LSTM forward WITHOUT pack_padded_sequence (ONNX limitation)
            - Returns probabilities (not logits)
            - Handles variable-length sequences via attention masking
            """

            def __init__(self, model: LSTM2RiskLightning):
                super().__init__()
                self.model = model

            def forward(
                self,
                text_tokens: torch.Tensor,  # (B, L)
                text_lengths: torch.Tensor,  # (B,)
                tabular: torch.Tensor,  # (B, F)
            ):
                """
                ONNX-compatible forward pass.

                Args:
                    text_tokens: Token IDs (B, L)
                    text_lengths: Sequence lengths (B,)
                    tabular: Tabular features (B, F)

                Returns:
                    probs: Class probabilities (B, num_classes)
                """
                # Text encoding (WITHOUT pack_padded_sequence for ONNX)
                embeddings = self.model.text_encoder.token_embedding(text_tokens)
                lstm_output, _ = self.model.text_encoder.lstm(embeddings)

                # Attention pooling with length masking
                text_hidden = self.model.text_encoder.pooling(lstm_output, text_lengths)
                text_hidden = self.model.text_encoder.norm(text_hidden)

                # Tabular encoding
                tab_hidden = self.model.tab_encoder(tabular.float())

                # Fusion and classification
                combined = torch.cat([text_hidden, tab_hidden], dim=1)
                logits = self.model.classifier(combined)

                # Return probabilities (not logits)
                return torch.softmax(logits, dim=1)

        # Prepare model for export
        self.eval()
        model_to_export = self.module if isinstance(self, FSDP) else self
        model_to_export = model_to_export.to("cpu")
        wrapper = LSTM2RiskONNXWrapper(model_to_export).to("cpu").eval()

        # Prepare inputs
        text_tokens = sample_batch["text"].to("cpu")
        text_lengths = sample_batch["text_length"].to("cpu")
        tabular = sample_batch["tabular"].to("cpu").float()

        # Verify batch consistency
        batch_size = text_tokens.shape[0]
        assert text_lengths.shape[0] == batch_size, "Inconsistent batch size"
        assert tabular.shape[0] == batch_size, "Inconsistent batch size"

        # Define dynamic axes for variable batch/sequence dimensions
        dynamic_axes = {
            "text_tokens": {0: "batch", 1: "sequence"},
            "text_lengths": {0: "batch"},
            "tabular": {0: "batch"},
            "probs": {0: "batch"},
        }

        try:
            # Export to ONNX
            torch.onnx.export(
                wrapper,
                (text_tokens, text_lengths, tabular),
                f=save_path,
                input_names=["text_tokens", "text_lengths", "tabular"],
                output_names=["probs"],
                dynamic_axes=dynamic_axes,
                opset_version=14,
                do_constant_folding=True,
            )

            # Verify ONNX model
            onnx_model = onnx.load(str(save_path))
            onnx.checker.check_model(onnx_model)
            logger.info(f"✓ ONNX model exported and verified: {save_path}")
        except Exception as e:
            logger.error(f"✗ ONNX export failed: {e}")
            raise
