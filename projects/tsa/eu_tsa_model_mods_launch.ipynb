{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef48e112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --ignore-installed amzn-mods-workflow-helper amzn-mods-python-sdk\n",
    "# !pip install --ignore-installed amzn-secure-ai-sandbox-workflow-python-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d261d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from secure_ai_sandbox_python_lib.session import Session as SaisSession\n",
    "\n",
    "sais_session = SaisSession(\".\")\n",
    "\n",
    "from mods_workflow_helper.sagemaker_pipeline_helper import SecurityConfig\n",
    "\n",
    "security_config = SecurityConfig(\n",
    "    kms_key=sais_session.get_team_owned_bucket_kms_key(),\n",
    "    security_group=sais_session.sandbox_vpc_security_group(),\n",
    "    vpc_subnets=sais_session.sandbox_vpc_subnets(),\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "session = PipelineSession(default_bucket=sais_session.team_owned_s3_bucket_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d4bc90",
   "metadata": {},
   "source": [
    "## Need to change model class name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d13e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eu_tsa_sq_model.py is the MODS template\n",
    "from eu_tsa_sq_model import EUTSASuspectQueueModel\n",
    "\n",
    "model = EUTSASuspectQueueModel(sagemaker_session=session)\n",
    "pipeline = model.generate_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ae69b",
   "metadata": {},
   "source": [
    "### Prepare execution document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b8ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "# split time for training/calibration/testing data\n",
    "train_start_date = (date.today() - timedelta(days=180)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ") + \"T00:00:00\"\n",
    "train_end_date = (date.today() - timedelta(days=90)).strftime(\"%Y-%m-%d\") + \"T00:00:00\"\n",
    "\n",
    "validation_start_date = (date.today() - timedelta(days=89)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ") + \"T00:00:00\"\n",
    "validation_end_date = (date.today() - timedelta(days=79)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ") + \"T00:00:00\"\n",
    "\n",
    "calibration_start_date = (date.today() - timedelta(days=28)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ") + \"T00:00:00\"\n",
    "calibration_end_date = (date.today() - timedelta(days=14)).strftime(\n",
    "    \"%Y-%m-%d\"\n",
    ") + \"T00:00:00\"\n",
    "\n",
    "print(\"train_start_date : \", train_start_date)\n",
    "print(\"train_end_date : \", train_end_date)\n",
    "print(\"validation_start_date : \", validation_start_date)\n",
    "print(\"validation_end_date : \", validation_end_date)\n",
    "print(\"calibration_start_date : \", calibration_start_date)\n",
    "print(\"calibration_end_date : \", calibration_end_date)\n",
    "\n",
    "date_format = \"%Y-%m-%dT%H:%M:%S\"\n",
    "train_delta_days = (\n",
    "    datetime.strptime(train_end_date, date_format)\n",
    "    - datetime.strptime(train_start_date, date_format)\n",
    ").days\n",
    "train_split_job = (train_delta_days + 1) > 7\n",
    "validation_delta_days = (\n",
    "    datetime.strptime(validation_end_date, date_format)\n",
    "    - datetime.strptime(validation_start_date, date_format)\n",
    ").days\n",
    "validation_split_job = (validation_delta_days + 1) > 7\n",
    "calibration_delta_days = (\n",
    "    datetime.strptime(calibration_end_date, date_format)\n",
    "    - datetime.strptime(calibration_start_date, date_format)\n",
    ").days\n",
    "calibration_split_job = (calibration_delta_days + 1) > 7\n",
    "\n",
    "print(train_split_job, validation_split_job, calibration_split_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15710ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from com.amazon.secureaisandboxproxyservice.models.createcradledataloadjobrequest import (\n",
    "    CreateCradleDataLoadJobRequest,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.datasourcesspecification import (\n",
    "    DataSourcesSpecification,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.mdsdatasourceproperties import (\n",
    "    MdsDataSourceProperties,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.andesdatasourceproperties import (\n",
    "    AndesDataSourceProperties,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.transformspecification import (\n",
    "    TransformSpecification,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.outputspecification import (\n",
    "    OutputSpecification,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.cradlejobspecification import (\n",
    "    CradleJobSpecification,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.edxdatasourceproperties import (\n",
    "    EdxDataSourceProperties,\n",
    ")\n",
    "\n",
    "from com.amazon.secureaisandboxproxyservice.models.jobsplitoptions import (\n",
    "    JobSplitOptions,\n",
    ")\n",
    "from com.amazon.secureaisandboxproxyservice.models.field import Field\n",
    "from com.amazon.secureaisandboxproxyservice.models.datasource import DataSource\n",
    "from secure_ai_sandbox_python_lib.utils import coral_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe726b",
   "metadata": {},
   "source": [
    "#### Load necessary files to create Cradle requests for data pulling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc81aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./\"\n",
    "import sys\n",
    "\n",
    "sys.path.append(model_dir + \"/scripts/\")\n",
    "from params import (\n",
    "    seq_cat_vars,\n",
    "    seq_num_vars,\n",
    "    dense_num_vars,\n",
    "    input_data_seq_cat_otf_vars,\n",
    "    input_data_seq_num_otf_vars,\n",
    "    input_data_seq_cat_vars,\n",
    "    input_data_seq_num_vars,\n",
    "    input_data_dense_num_vars,\n",
    "    numerical_cat_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fbcc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_var_list = (\n",
    "    seq_cat_vars\n",
    "    + seq_num_vars\n",
    "    + dense_num_vars\n",
    "    + input_data_seq_cat_otf_vars\n",
    "    + input_data_seq_num_otf_vars\n",
    ")\n",
    "var_list = list(\n",
    "    set(\n",
    "        [\n",
    "            \"objectId\",\n",
    "            \"orderDate\",\n",
    "            \"transactionDate\",\n",
    "            \"marketplaceCountryCode\",\n",
    "            \"marketplaceId\",\n",
    "            \"isQueued\",\n",
    "            \"paymeth\",\n",
    "            \"ictry_cd\",\n",
    "            \"bctry_cd\",\n",
    "            \"sctry_cd\",\n",
    "            \"cctry_cd\",\n",
    "            \"isSidelined\",\n",
    "            \"emailorg\",\n",
    "            \"creditCardIds\",\n",
    "        ]\n",
    "        + model_var_list\n",
    "    )\n",
    ")\n",
    "print(len(model_var_list), len(set(var_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eed81d",
   "metadata": {},
   "source": [
    "#### Create Cradle requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756985e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Placeholder, will not be actually used\n",
    "output_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596fcab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_training = CreateCradleDataLoadJobRequest(\n",
    "    data_sources=DataSourcesSpecification(\n",
    "        start_date=train_start_date,  # data start date\n",
    "        end_date=train_end_date,  # data end date\n",
    "        data_sources=[  # data sources a list of data source properties\n",
    "            DataSource(\n",
    "                data_source_name=\"RAW_MDS\",  # data source name, it should be uniq across the list of data source. this name should be used as table name when you write the SQL\n",
    "                data_source_type=\"MDS\",  # data source type, it can be 'MDS/ANDES/EDX', you need setup the properties according to this type\n",
    "                mds_data_source_properties=MdsDataSourceProperties(  #\n",
    "                    service_name=\"FORTRESS_RETAIL\",\n",
    "                    org_id=\"2\",\n",
    "                    region=\"EU\",\n",
    "                    # output_schema=[Field(field_name=f, field_type='STRING') for f in pullVars],\n",
    "                    output_schema=[\n",
    "                        Field(field_name=f, field_type=\"STRING\") for f in var_list\n",
    "                    ],\n",
    "                    use_hourly_edx_data_set=False,  # MDS/EDX have another data set which merges the raw manifest. you can change this to True to use hourly data provider which can reduce the hot data set's throttling issue. hourly EDX data provider doesn't contain all the data, you need verify and make sure the hourly data set is available. example link: https://edx.corp.amazon.com/providers/cmls-raw-hourly-data/subjects/fortress-retail/datasets/na-1\n",
    "                ),\n",
    "            ),\n",
    "            DataSource(\n",
    "                data_source_name=\"TAGS\",\n",
    "                data_source_type=\"ANDES\",  # this is an example of Andes data source\n",
    "                andes_data_source_properties=AndesDataSourceProperties(\n",
    "                    provider=\"26b27bde-3847-49c6-a07c-0289c17d9c33\",\n",
    "                    table_name=\"fraud-tags-eu\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    transform_specification=TransformSpecification(  # transformSQL should refer the above data source name to query the data\n",
    "        transform_sql=\"\"\"\n",
    "        select \n",
    "            RAW_MDS.*, \n",
    "            TAGS.is_frd AS IS_FRD\n",
    "        from RAW_MDS \n",
    "        left join TAGS \n",
    "            on RAW_MDS.objectId=TAGS.order_id  \n",
    "            AND TAGS.order_day_timestamp >= TO_TIMESTAMP('${startDate}', 'yyyy-MM-dd')  \n",
    "            AND TAGS.order_day_timestamp <= TO_TIMESTAMP('${endDate}', 'yyyy-MM-dd')\n",
    "        where (TAGS.is_frd=1 OR (TAGS.is_frd=0 and rand()<0.025)) and (\n",
    "            ((marketplaceCountryCode in ('GB','DE','FR','IT','ES') and isQueued=1) or \n",
    "            (marketplaceCountryCode in ('TR','NL','SE','SA','AE','EG','PL','BE') and isSidelined=1)) \n",
    "        ) \n",
    "        \"\"\",\n",
    "        job_split_options=JobSplitOptions(\n",
    "            split_job=train_split_job,  # edit for test False, # You can enable job split option by changing this function to True, but you need provide merge_sql. INPUT will the all the data after split executes, you can write extra logic in SQL, e.g. using group by for statistics or dedup.\n",
    "            days_per_split=7,\n",
    "            merge_sql=\"\"\"\n",
    "                WITH data AS (\n",
    "                    SELECT INPUT.*,\n",
    "                        TO_TIMESTAMP(INPUT.transactionDate, 'EEE MMM dd HH:mm:ss zzz yyyy') AS transactionDateV2\n",
    "                    FROM INPUT\n",
    "                    ),\n",
    "                dedup AS (\n",
    "                    SELECT *\n",
    "                    FROM (\n",
    "                        SELECT *, ROW_NUMBER() OVER (PARTITION BY objectId ORDER BY transactionDateV2 DESC) AS __rownum__\n",
    "                        FROM data\n",
    "                        )\n",
    "                    WHERE __rownum__ = 1\n",
    "                    )\n",
    "                select * from dedup\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ),\n",
    "    output_specification=OutputSpecification(\n",
    "        # output_schema=list(pullVars)+['IS_FRD'],  # output_schema should be provided as the final output fields.\n",
    "        output_schema=var_list + [\"IS_FRD\"],\n",
    "        output_path=output_path,  #\n",
    "        output_format=\"UNESCAPED_TSV\",  #     # output format can be CSV, UNESCAPED_TSV, JSON, ION, PARQUET. CSV is the default format if you don't specify it\n",
    "        output_save_mode=\"ERRORIFEXISTS\",  # output save mode can setup to support different case, it can be OVERWRITE, ERRORIFEXISTS, APPEND, IGNORE. In default it's ERRORIFEXISTS. \",\n",
    "        output_file_count=0,  # output file count can be set to reduce or increase final the number of files. Too many output files will cause S3 throttling failure; Too few output will encounter performance issues. current setting is 30 per day, you can provide the overrides for your overrides.\n",
    "        keep_dot_in_output_schema=True,  # When set to true, the output file header will contain normal the '.'. Otherwise when set to False, the output file header will replace every '.' with '__DOT__'.\n",
    "        # edit to test\n",
    "        include_header_in_s3_output=True,  # When set to true, the s3 output file will include header. Note that only S3 supports output with header.\n",
    "    ),\n",
    "    cradle_job_specification=CradleJobSpecification(\n",
    "        cluster_type=\"LARGE\",\n",
    "        cradle_account=\"BRP-ML-Payment-Generate-Data\",\n",
    "        extra_spark_job_arguments=\"\",  # you can customize the spark job driver memory if you need by vending parameters here\n",
    "        job_retry_count=4,  # job retry count in case of failure, in default Cradle will retry once if it fails. you can customize retry times.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# cradle_loading_request_dict = coral_utils.convert_coral_to_dict(request)\n",
    "cradle_training_request_dict = coral_utils.convert_coral_to_dict(request_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_validation = CreateCradleDataLoadJobRequest(\n",
    "    data_sources=DataSourcesSpecification(\n",
    "        start_date=validation_start_date,  # data start date\n",
    "        end_date=validation_end_date,  # data end date\n",
    "        data_sources=[  # data sources a list of data source properties\n",
    "            DataSource(\n",
    "                data_source_name=\"RAW_MDS\",  # data source name, it should be uniq across the list of data source. this name should be used as table name when you write the SQL\n",
    "                data_source_type=\"MDS\",  # data source type, it can be 'MDS/ANDES/EDX', you need setup the properties according to this type\n",
    "                mds_data_source_properties=MdsDataSourceProperties(  #\n",
    "                    service_name=\"FORTRESS_RETAIL\",\n",
    "                    org_id=\"2\",\n",
    "                    region=\"EU\",\n",
    "                    # output_schema=[Field(field_name=f, field_type='STRING') for f in pullVars],\n",
    "                    output_schema=[\n",
    "                        Field(field_name=f, field_type=\"STRING\") for f in var_list\n",
    "                    ],\n",
    "                    use_hourly_edx_data_set=False,  # MDS/EDX have another data set which merges the raw manifest. you can change this to True to use hourly data provider which can reduce the hot data set's throttling issue. hourly EDX data provider doesn't contain all the data, you need verify and make sure the hourly data set is available. example link: https://edx.corp.amazon.com/providers/cmls-raw-hourly-data/subjects/fortress-retail/datasets/na-1\n",
    "                ),\n",
    "            ),\n",
    "            DataSource(\n",
    "                data_source_name=\"TAGS\",\n",
    "                data_source_type=\"ANDES\",  # this is an example of Andes data source\n",
    "                andes_data_source_properties=AndesDataSourceProperties(\n",
    "                    provider=\"26b27bde-3847-49c6-a07c-0289c17d9c33\",\n",
    "                    table_name=\"fraud-tags-eu\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    transform_specification=TransformSpecification(  # transformSQL should refer the above data source name to query the data\n",
    "        transform_sql=\"\"\"\n",
    "        select \n",
    "            RAW_MDS.*, \n",
    "            TAGS.is_frd AS IS_FRD\n",
    "        from RAW_MDS \n",
    "        left join TAGS \n",
    "            on RAW_MDS.objectId=TAGS.order_id  \n",
    "            AND TAGS.order_day_timestamp >= TO_TIMESTAMP('${startDate}', 'yyyy-MM-dd')  \n",
    "            AND TAGS.order_day_timestamp <= TO_TIMESTAMP('${endDate}', 'yyyy-MM-dd')\n",
    "        where TAGS.is_frd!=-1 and rand()<0.15 and (\n",
    "            ((marketplaceCountryCode in ('GB','DE','FR','IT','ES') and isQueued=1) or \n",
    "            (marketplaceCountryCode in ('TR','NL','SE','SA','AE','EG','PL','BE') and isSidelined=1)) \n",
    "        ) \n",
    "        \"\"\",\n",
    "        job_split_options=JobSplitOptions(\n",
    "            split_job=validation_split_job,  # You can enable job split option by changing this function to True, but you need provide merge_sql. INPUT will the all the data after split executes, you can write extra logic in SQL, e.g. using group by for statistics or dedup.\n",
    "            days_per_split=7,\n",
    "            merge_sql=\"\"\"\n",
    "                WITH data AS (\n",
    "                    SELECT INPUT.*,\n",
    "                        TO_TIMESTAMP(INPUT.transactionDate, 'EEE MMM dd HH:mm:ss zzz yyyy') AS transactionDateV2\n",
    "                    FROM INPUT\n",
    "                    ),\n",
    "                dedup AS (\n",
    "                    SELECT *\n",
    "                    FROM (\n",
    "                        SELECT *, ROW_NUMBER() OVER (PARTITION BY objectId ORDER BY transactionDateV2 DESC) AS __rownum__\n",
    "                        FROM data\n",
    "                        )\n",
    "                    WHERE __rownum__ = 1\n",
    "                    )\n",
    "                select * from dedup\n",
    "                where rand() < (select 8000000 / count(*) from dedup)\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ),\n",
    "    output_specification=OutputSpecification(\n",
    "        # output_schema=list(pullVars)+['IS_FRD'],  # output_schema should be provided as the final output fields.\n",
    "        output_schema=var_list + [\"IS_FRD\"],\n",
    "        output_path=output_path,  #\n",
    "        output_format=\"UNESCAPED_TSV\",  #     # output format can be CSV, UNESCAPED_TSV, JSON, ION, PARQUET. CSV is the default format if you don't specify it\n",
    "        output_save_mode=\"ERRORIFEXISTS\",  # output save mode can setup to support different case, it can be OVERWRITE, ERRORIFEXISTS, APPEND, IGNORE. In default it's ERRORIFEXISTS. \",\n",
    "        output_file_count=0,  # output file count can be set to reduce or increase final the number of files. Too many output files will cause S3 throttling failure; Too few output will encounter performance issues. current setting is 30 per day, you can provide the overrides for your overrides.\n",
    "        keep_dot_in_output_schema=True,  # When set to true, the output file header will contain normal the '.'. Otherwise when set to False, the output file header will replace every '.' with '__DOT__'.\n",
    "        # edit to test\n",
    "        include_header_in_s3_output=True,  # When set to true, the s3 output file will include header. Note that only S3 supports output with header.\n",
    "    ),\n",
    "    cradle_job_specification=CradleJobSpecification(\n",
    "        cluster_type=\"LARGE\",\n",
    "        cradle_account=\"BRP-ML-Payment-Generate-Data\",\n",
    "        extra_spark_job_arguments=\"\",  # you can customize the spark job driver memory if you need by vending parameters here\n",
    "        job_retry_count=4,  # job retry count in case of failure, in default Cradle will retry once if it fails. you can customize retry times.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# cradle_loading_request_dict = coral_utils.convert_coral_to_dict(request)\n",
    "cradle_validation_request_dict = coral_utils.convert_coral_to_dict(request_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34003622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request_calibration = CreateCradleDataLoadJobRequest(\n",
    "    data_sources=DataSourcesSpecification(\n",
    "        start_date=calibration_start_date,  # data start date\n",
    "        end_date=calibration_end_date,  # data end date\n",
    "        data_sources=[  # data sources a list of data source properties\n",
    "            DataSource(\n",
    "                data_source_name=\"RAW_MDS\",  # data source name, it should be uniq across the list of data source. this name should be used as table name when you write the SQL\n",
    "                data_source_type=\"MDS\",  # data source type, it can be 'MDS/ANDES/EDX', you need setup the properties according to this type\n",
    "                mds_data_source_properties=MdsDataSourceProperties(  #\n",
    "                    service_name=\"FORTRESS_RETAIL\",\n",
    "                    org_id=\"2\",\n",
    "                    region=\"EU\",\n",
    "                    # output_schema=[Field(field_name=f, field_type='STRING') for f in pullVars],\n",
    "                    output_schema=[\n",
    "                        Field(field_name=f, field_type=\"STRING\") for f in var_list\n",
    "                    ],\n",
    "                    use_hourly_edx_data_set=False,  # MDS/EDX have another data set which merges the raw manifest. you can change this to True to use hourly data provider which can reduce the hot data set's throttling issue. hourly EDX data provider doesn't contain all the data, you need verify and make sure the hourly data set is available. example link: https://edx.corp.amazon.com/providers/cmls-raw-hourly-data/subjects/fortress-retail/datasets/na-1\n",
    "                ),\n",
    "            ),\n",
    "            DataSource(\n",
    "                data_source_name=\"TAGS\",\n",
    "                data_source_type=\"ANDES\",  # this is an example of Andes data source\n",
    "                andes_data_source_properties=AndesDataSourceProperties(\n",
    "                    provider=\"26b27bde-3847-49c6-a07c-0289c17d9c33\",\n",
    "                    table_name=\"fraud-tags-eu\",\n",
    "                ),\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    transform_specification=TransformSpecification(  # transformSQL should refer the above data source name to query the data\n",
    "        transform_sql=\"\"\"\n",
    "        select \n",
    "            RAW_MDS.*, \n",
    "            TAGS.is_frd AS IS_FRD\n",
    "        from RAW_MDS \n",
    "        left join TAGS \n",
    "            on RAW_MDS.objectId=TAGS.order_id  \n",
    "            AND TAGS.order_day_timestamp >= TO_TIMESTAMP('${startDate}', 'yyyy-MM-dd')  \n",
    "            AND TAGS.order_day_timestamp <= TO_TIMESTAMP('${endDate}', 'yyyy-MM-dd')\n",
    "        where rand() < 0.06 and (\n",
    "            ((marketplaceCountryCode in ('GB','DE','FR','IT','ES') and isQueued=1) or \n",
    "            (marketplaceCountryCode in ('TR','NL','SE','SA','AE','EG','PL','BE') and isSidelined=1)) \n",
    "        ) \n",
    "        \"\"\",\n",
    "        job_split_options=JobSplitOptions(\n",
    "            split_job=calibration_split_job,  # You can enable job split option by changing this function to True, but you need provide merge_sql. INPUT will the all the data after split executes, you can write extra logic in SQL, e.g. using group by for statistics or dedup.\n",
    "            days_per_split=7,\n",
    "            merge_sql=\"\"\"\n",
    "                WITH data AS (\n",
    "                    SELECT INPUT.*,\n",
    "                        TO_TIMESTAMP(INPUT.transactionDate, 'EEE MMM dd HH:mm:ss zzz yyyy') AS transactionDateV2\n",
    "                    FROM INPUT\n",
    "                    ),\n",
    "                dedup AS (\n",
    "                    SELECT *\n",
    "                    FROM (\n",
    "                        SELECT *, ROW_NUMBER() OVER (PARTITION BY objectId ORDER BY transactionDateV2 DESC) AS __rownum__\n",
    "                        FROM data\n",
    "                        )\n",
    "                    WHERE __rownum__ = 1\n",
    "                    )\n",
    "                select * from dedup\n",
    "                where rand() < (select 8000000 / count(*) from dedup)\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ),\n",
    "    output_specification=OutputSpecification(\n",
    "        # output_schema=list(pullVars)+['IS_FRD'],  # output_schema should be provided as the final output fields.\n",
    "        output_schema=var_list + [\"IS_FRD\"],\n",
    "        output_path=output_path,  #\n",
    "        output_format=\"UNESCAPED_TSV\",  #     # output format can be CSV, UNESCAPED_TSV, JSON, ION, PARQUET. CSV is the default format if you don't specify it\n",
    "        output_save_mode=\"ERRORIFEXISTS\",  # output save mode can setup to support different case, it can be OVERWRITE, ERRORIFEXISTS, APPEND, IGNORE. In default it's ERRORIFEXISTS. \",\n",
    "        output_file_count=0,  # output file count can be set to reduce or increase final the number of files. Too many output files will cause S3 throttling failure; Too few output will encounter performance issues. current setting is 30 per day, you can provide the overrides for your overrides.\n",
    "        keep_dot_in_output_schema=True,  # When set to true, the output file header will contain normal the '.'. Otherwise when set to False, the output file header will replace every '.' with '__DOT__'.\n",
    "        # edit to test\n",
    "        include_header_in_s3_output=True,  # When set to true, the s3 output file will include header. Note that only S3 supports output with header.\n",
    "    ),\n",
    "    cradle_job_specification=CradleJobSpecification(\n",
    "        cluster_type=\"LARGE\",\n",
    "        cradle_account=\"BRP-ML-Payment-Generate-Data\",\n",
    "        extra_spark_job_arguments=\"\",  # you can customize the spark job driver memory if you need by vending parameters here\n",
    "        job_retry_count=4,  # job retry count in case of failure, in default Cradle will retry once if it fails. you can customize retry times.\n",
    "    ),\n",
    ")\n",
    "\n",
    "# cradle_loading_request_dict = coral_utils.convert_coral_to_dict(request)\n",
    "cradle_calibration_request_dict = coral_utils.convert_coral_to_dict(request_calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b343b",
   "metadata": {},
   "source": [
    "#### Load files to prepare the model registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa06695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_var_dict = {}\n",
    "for var in input_data_seq_cat_otf_vars:\n",
    "    input_var_dict[var] = \"TEXT\"\n",
    "for var in input_data_seq_cat_vars:\n",
    "    input_var_dict[var] = \"TEXT\"\n",
    "for var in input_data_seq_num_otf_vars:\n",
    "    input_var_dict[var] = \"TEXT\"\n",
    "for var in input_data_seq_num_vars:\n",
    "    input_var_dict[var] = \"NUMERIC\"\n",
    "for var in input_data_dense_num_vars:\n",
    "    input_var_dict[var] = \"NUMERIC\"\n",
    "input_var_dict[\"objectId\"] = \"TEXT\"\n",
    "input_var_dict[\"orderDate\"] = \"TEXT\"\n",
    "for var in numerical_cat_vars:\n",
    "    input_var_dict[var] = \"NUMERIC\"\n",
    "del input_var_dict[\"objectId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b9ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_var_dict = {\n",
    "    \"score-percentile\": \"NUMERIC\",\n",
    "    \"legacy-score\": \"NUMERIC\",\n",
    "    \"ProbabilityScore\": \"NUMERIC\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddcbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_payload_s3_bucket = \"sandboxuserdependency-maxueyu-personals3bucket-ysvmoa568sen\"\n",
    "sample_payload_s3_key = (\n",
    "    \"EUTSAModel/pzkwcif1mual/AddInferenceDependencies/payload.tar.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34da19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_registration_config = {\n",
    "#     \"model_domain\": \"integration-test\",\n",
    "#     \"model_objective\": \"TestObjective\",\n",
    "#     \"source_model_inference_content_types\": ['application/json'],\n",
    "#     \"source_model_inference_response_types\": [\"application/json\"],\n",
    "#     \"source_model_inference_input_variable_list\": input_var_dict,\n",
    "#     \"source_model_inference_output_variable_list\": output_var_dict,\n",
    "#     \"model_registration_region\": \"NA\",\n",
    "#     \"source_model_inference_image_arn\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.1.0-cpu-py310\",\n",
    "#     \"source_model_region\": \"us-east-1\",\n",
    "#     \"model_owner\": \"amzn1.abacus.team.5y3aajyhecgqmg6rjxga\",\n",
    "#     \"source_model_environment_variable_map\": {\n",
    "#         \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "#         \"SAGEMAKER_PROGRAM\": \"pytorch_inference_handler.py\",\n",
    "#         \"SAGEMAKER_REGION\": \"us-east-1\"\n",
    "#     },\n",
    "#     \"load_testing_info_map\": {\n",
    "#         \"expected_tps\": 100,\n",
    "#         \"max_latency_in_millisecond\": 100,\n",
    "#         \"sample_payload_s3_bucket\": sample_payload_s3_bucket,\n",
    "#         \"sample_payload_s3_key\": sample_payload_s3_key,\n",
    "#         \"instance_type_list\": [\"ml.m5.xlarge\"],\n",
    "#         # Maximum error rate load test will accept, test will fail if error rate is higher than the number\n",
    "#         \"max_acceptable_error_rate\": 0.2\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b19b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_registration_config = {\n",
    "    \"model_domain\": \"FORTRESS_RETAIL\",\n",
    "    \"model_objective\": \"EUTSASuspectQueueModel\",\n",
    "    \"source_model_inference_content_types\": [\"application/json\"],\n",
    "    \"source_model_inference_response_types\": [\"application/json\"],\n",
    "    \"source_model_inference_input_variable_list\": input_var_dict,\n",
    "    \"source_model_inference_output_variable_list\": output_var_dict,\n",
    "    \"model_registration_region\": \"EU\",\n",
    "    \"source_model_inference_image_arn\": \"763104351884.dkr.ecr.eu-west-1.amazonaws.com/pytorch-inference:2.1.0-cpu-py310\",\n",
    "    \"source_model_region\": \"eu-west-1\",\n",
    "    \"model_owner\": \"amzn1.abacus.team.5y3aajyhecgqmg6rjxga\",\n",
    "    \"source_model_environment_variable_map\": {\n",
    "        \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "        \"SAGEMAKER_PROGRAM\": \"pytorch_inference_handler.py\",\n",
    "        \"SAGEMAKER_REGION\": \"eu-west-1\",\n",
    "    },\n",
    "    \"load_testing_info_map\": {\n",
    "        \"expected_tps\": 100,\n",
    "        \"max_latency_in_millisecond\": 100,\n",
    "        \"sample_payload_s3_bucket\": sample_payload_s3_bucket,\n",
    "        \"sample_payload_s3_key\": sample_payload_s3_key,\n",
    "        \"instance_type_list\": [\"ml.m5.xlarge\"],\n",
    "        # Maximum error rate load test will accept, test will fail if error rate is higher than the number\n",
    "        \"max_acceptable_error_rate\": 0.2,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b033b6",
   "metadata": {},
   "source": [
    "#### Overwrite the default execution document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bd627",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mods_workflow_helper.sagemaker_pipeline_helper import (\n",
    "    SagemakerPipelineHelper,\n",
    "    SecurityConfig,\n",
    ")\n",
    "\n",
    "default_execution_doc = SagemakerPipelineHelper.get_pipeline_default_execution_document(\n",
    "    pipeline\n",
    ")\n",
    "test_execution_doc = default_execution_doc\n",
    "\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"Training_Data_Download\"] = {}\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"Validation_Data_Download\"] = {}\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"Calibration_Data_Download\"] = {}\n",
    "\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"Training_Data_Download\"][\"STEP_CONFIG\"] = (\n",
    "    cradle_training_request_dict\n",
    ")\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"Validation_Data_Download\"][\n",
    "    \"STEP_CONFIG\"\n",
    "] = cradle_validation_request_dict\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"Calibration_Data_Download\"][\n",
    "    \"STEP_CONFIG\"\n",
    "] = cradle_calibration_request_dict\n",
    "\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"MimsModelRegistrationProcessingStep\"] = {}\n",
    "test_execution_doc[\"PIPELINE_STEP_CONFIGS\"][\"MimsModelRegistrationProcessingStep\"][\n",
    "    \"STEP_CONFIG\"\n",
    "] = model_registration_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572a437",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_execution_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697eb1f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mods_workflow_helper.sagemaker_pipeline_helper import SagemakerPipelineHelper\n",
    "\n",
    "SagemakerPipelineHelper.start_pipeline_execution(\n",
    "    pipeline=pipeline,\n",
    "    secure_config=security_config,\n",
    "    sagemaker_session=session,\n",
    "    preparation_space_local_root=\"/tmp\",\n",
    "    pipeline_execution_document=test_execution_doc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406dccd",
   "metadata": {},
   "source": [
    "## Manually Set Scaling Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from secure_ai_sandbox_python_lib.session import Session\n",
    "#\n",
    "# # Initialize sandbox_session\n",
    "# sandbox_session = Session(session_folder='/tmp/temp_folder', retail_region='EU')\n",
    "#\n",
    "# # Create the MIMS resource\n",
    "# mims = sandbox_session.resource('MIMSModelRegistrar')\n",
    "#\n",
    "# # the Scaling Policy of the Endpoint\n",
    "# endpoint_scaling_config_map = {\n",
    "#             \"instance_type\": \"ml.m5.xlarge\",\n",
    "#             \"min_capacity\": 18,\n",
    "#             \"max_capacity\": 36,\n",
    "#             \"scaling_policy_map\": {\n",
    "#                 \"version\": \"1.0\",\n",
    "#                 \"target_value\": 1000.0, #// TPM traffic per minutes 6000TPM = 100TPS\n",
    "#                 \"scale_in_cooldown\": 300, #// seconds wait for delete instance\n",
    "#                 \"scale_out_cooldown\": 60, #// seconds wait for add instance\n",
    "#                 \"disable_scale_in\": False\n",
    "#             }\n",
    "#         }\n",
    "#\n",
    "#\n",
    "# model_domain='FORTRESS_RETAIL'\n",
    "# model_objective='EUTSASuspectQueueModel'\n",
    "# model_id = \"2024-02-14-68203-rapid-cork\"\n",
    "#\n",
    "# response = mims.set_scaling_policy(model_region=\"EU\",\n",
    "#                                     model_domain=model_domain,\n",
    "#                                     model_objective=model_objective,\n",
    "#                                     model_id=model_id,\n",
    "#                                     endpoint_scaling_config_map=endpoint_scaling_config_map)\n",
    "#\n",
    "# response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
