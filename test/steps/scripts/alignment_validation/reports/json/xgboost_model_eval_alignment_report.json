{
  "script_name": "xgboost_model_eval",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "arguments",
        "message": "Script defines config-driven argument provided by builder: --job-type (accessed as args.job_type)",
        "details": {
          "cli_argument": "job-type",
          "python_attribute": "job_type",
          "script": "xgboost_model_eval",
          "source": "builder"
        },
        "recommendation": "Argument --job-type is provided by builder - no action needed"
      },
      {
        "severity": "INFO",
        "category": "testability_compliance",
        "message": "Main function follows testability pattern with all required parameters",
        "details": {
          "script": "xgboost_model_eval",
          "testability_parameters": [
            "job_args",
            "environ_vars",
            "output_paths",
            "input_paths"
          ]
        },
        "recommendation": "No action needed - script follows testability best practices"
      },
      {
        "severity": "WARNING",
        "category": "testability_env_access",
        "message": "Helper functions use direct environment access - consider parameter passing",
        "details": {
          "script": "xgboost_model_eval",
          "helper_accesses": [
            {
              "function": null,
              "variable": "ID_FIELD",
              "line_number": 898
            },
            {
              "function": null,
              "variable": "LABEL_FIELD",
              "line_number": 899
            }
          ]
        },
        "recommendation": "Pass environment variables as parameters to helper functions instead of direct access"
      },
      {
        "severity": "WARNING",
        "category": "testability_entry_point",
        "message": "Main function expects environ_vars parameter but no environment collection found in entry point",
        "details": {
          "script": "xgboost_model_eval"
        },
        "recommendation": "Add environment variable collection in __main__ block to pass to main function"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 822
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 822
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 823
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 823
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 824
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 825
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 827
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 828
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for environ_vars",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "environ_vars",
          "current_pattern": "environ_vars.get",
          "line_number": 832
        },
        "recommendation": "Use environ_vars['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for environ_vars",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "environ_vars",
          "current_pattern": "environ_vars.get",
          "line_number": 833
        },
        "recommendation": "Use environ_vars['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for job_args",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "job_args",
          "current_pattern": "job_args.job_type",
          "line_number": 836
        },
        "recommendation": "Use job_args['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_eval",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 921
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_container_support",
        "message": "No container detection found - consider adding hybrid mode support",
        "details": {
          "script": "xgboost_model_eval"
        },
        "recommendation": "Add container detection to support both local and container execution"
      },
      {
        "severity": "WARNING",
        "category": "testability_helper_functions",
        "message": "Helper function 'None' accesses environment directly",
        "details": {
          "script": "xgboost_model_eval",
          "function": null,
          "env_variables": [
            "ID_FIELD",
            "LABEL_FIELD"
          ],
          "line_numbers": [
            898,
            899
          ]
        },
        "recommendation": "Refactor 'None' to accept environment variables as parameters"
      },
      {
        "severity": "INFO",
        "category": "framework_detected",
        "message": "Processing script uses xgboost framework",
        "details": {
          "script": "xgboost_model_eval",
          "step_type": "Processing",
          "framework": "xgboost"
        },
        "recommendation": "Ensure xgboost dependencies are properly specified"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_model_eval.py",
      "path_references": [
        "path=\"\\n    A processor that performs risk-table-based mapping on a specified categorical variable.\\n    The 'process' method (called via __call__) handles single values.\\n    The 'transform' method handles pandas Series or DataFrames.\\n    \" line_number=26 context='\\nclass RiskTableMappingProcessor:\\n>>>     \"\"\"\\n    A processor that performs risk-table-based mapping on a specified categorical variable.\\n    The \\'process\\' method (called via __call__) handles single values.' is_hardcoded=True construction_method=None",
        "path='column_name must be a non-empty string.' line_number=54 context='\\n        if not isinstance(column_name, str) or not column_name:\\n>>>             raise ValueError(\"column_name must be a non-empty string.\")\\n        self.column_name = column_name\\n        self.label_name = label_name' is_hardcoded=True construction_method=None",
        "path='Risk tables must be a dictionary.' line_number=73 context='    def _validate_risk_tables(self, risk_tables: Dict) -> None:\\n        if not isinstance(risk_tables, dict):\\n>>>             raise ValueError(\"Risk tables must be a dictionary.\")\\n        if \"bins\" not in risk_tables or \"default_bin\" not in risk_tables:\\n            raise ValueError(\"Risk tables must contain \\'bins\\' and \\'default_bin\\' keys.\")' is_hardcoded=True construction_method=None",
        "path=\"Risk tables must contain 'bins' and 'default_bin' keys.\" line_number=75 context='            raise ValueError(\"Risk tables must be a dictionary.\")\\n        if \"bins\" not in risk_tables or \"default_bin\" not in risk_tables:\\n>>>             raise ValueError(\"Risk tables must contain \\'bins\\' and \\'default_bin\\' keys.\")\\n        if not isinstance(risk_tables[\"bins\"], dict):\\n            raise ValueError(\"Risk tables \\'bins\\' must be a dictionary.\")' is_hardcoded=True construction_method=None",
        "path=\"Risk tables 'bins' must be a dictionary.\" line_number=77 context='            raise ValueError(\"Risk tables must contain \\'bins\\' and \\'default_bin\\' keys.\")\\n        if not isinstance(risk_tables[\"bins\"], dict):\\n>>>             raise ValueError(\"Risk tables \\'bins\\' must be a dictionary.\")\\n        if not isinstance(\\n            risk_tables[\"default_bin\"], (int, float, np.floating, np.integer)' is_hardcoded=True construction_method=None",
        "path='.' line_number=82 context='        ):\\n            raise ValueError(\\n>>>                 f\"Risk tables \\'default_bin\\' must be a number, got {type(risk_tables[\\'default_bin\\'])}.\"\\n            )\\n' is_hardcoded=True construction_method=None",
        "path='fit() requires a pandas DataFrame.' line_number=92 context='    def fit(self, data: pd.DataFrame) -> \"RiskTableMappingProcessor\":\\n        if not isinstance(data, pd.DataFrame):\\n>>>             raise TypeError(\"fit() requires a pandas DataFrame.\")\\n        if self.label_name not in data.columns:\\n            raise ValueError(' is_hardcoded=True construction_method=None",
        "path=\"' not found in input data.\" line_number=95 context='        if self.label_name not in data.columns:\\n            raise ValueError(\\n>>>                 f\"Label variable \\'{self.label_name}\\' not found in input data.\"\\n            )\\n        if self.column_name not in data.columns:' is_hardcoded=True construction_method=None",
        "path=\"' not found in input data.\" line_number=99 context='        if self.column_name not in data.columns:\\n            raise ValueError(\\n>>>                 f\"Column to bin \\'{self.column_name}\\' not found in input data.\"\\n            )\\n' is_hardcoded=True construction_method=None",
        "path=\"' is empty during fit. Risk tables will be empty, default_bin will be 0.5 or NaN if no labels at all.\" line_number=109 context='            # Handle case with no valid data for fitting\\n            print(\\n>>>                 f\"Warning: Filtered data for column \\'{self.column_name}\\' is empty during fit. \"\\n                \"Risk tables will be empty, default_bin will be 0.5 or NaN if no labels at all.\"\\n            )' is_hardcoded=True construction_method=None",
        "path='RiskTableMappingProcessor must be fitted or initialized with risk tables before processing.' line_number=191 context='        if not self.is_fitted:\\n            raise RuntimeError(\\n>>>                 \"RiskTableMappingProcessor must be fitted or initialized with risk tables before processing.\"\\n            )\\n        str_value = str(input_value)' is_hardcoded=True construction_method=None",
        "path='RiskTableMappingProcessor must be fitted or initialized with risk tables before transforming.' line_number=207 context='        if not self.is_fitted:\\n            raise RuntimeError(\\n>>>                 \"RiskTableMappingProcessor must be fitted or initialized with risk tables before transforming.\"\\n            )\\n' is_hardcoded=True construction_method=None",
        "path=\"' not found in input DataFrame for transform operation.\" line_number=213 context='            if self.column_name not in data.columns:\\n                raise ValueError(\\n>>>                     f\"Column \\'{self.column_name}\\' not found in input DataFrame for transform operation.\"\\n                )\\n            output_data = data.copy()' is_hardcoded=True construction_method=None",
        "path='RiskTableMappingProcessor has not been fitted or initialized with risk tables.' line_number=235 context='        if not self.is_fitted:\\n            raise RuntimeError(\\n>>>                 \"RiskTableMappingProcessor has not been fitted or initialized with risk tables.\"\\n            )\\n        return self.risk_tables' is_hardcoded=True construction_method=None",
        "path='\\n    A processor that performs imputation on numerical variables using predefined or computed values.\\n    Supports mean, median, and mode imputation strategies.\\n    ' line_number=241 context='\\nclass NumericalVariableImputationProcessor:\\n>>>     \"\"\"\\n    A processor that performs imputation on numerical variables using predefined or computed values.\\n    Supports mean, median, and mode imputation strategies.' is_hardcoded=True construction_method=None",
        "path=\"Processor is not fitted. Call 'fit' with appropriate arguments before using this method.\" line_number=318 context='        if not self.is_fitted:\\n            raise RuntimeError(\\n>>>                 \"Processor is not fitted. Call \\'fit\\' with appropriate arguments before using this method.\"\\n            )\\n' is_hardcoded=True construction_method=None",
        "path=\"Processor is not fitted. Call 'fit' with appropriate arguments before using this method.\" line_number=341 context='        if not self.is_fitted:\\n            raise RuntimeError(\\n>>>                 \"Processor is not fitted. Call \\'fit\\' with appropriate arguments before using this method.\"\\n            )\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/model' line_number=384 context='# Container path constants - aligned with script contract\\nCONTAINER_PATHS = {\\n>>>     \"MODEL_DIR\": \"/opt/ml/processing/input/model\",\\n    \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n    \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/eval_data' line_number=385 context='CONTAINER_PATHS = {\\n    \"MODEL_DIR\": \"/opt/ml/processing/input/model\",\\n>>>     \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n    \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",\\n    \"OUTPUT_METRICS_DIR\": \"/opt/ml/processing/output/metrics\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/eval' line_number=386 context='    \"MODEL_DIR\": \"/opt/ml/processing/input/model\",\\n    \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n>>>     \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",\\n    \"OUTPUT_METRICS_DIR\": \"/opt/ml/processing/output/metrics\",\\n}' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/metrics' line_number=387 context='    \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n    \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",\\n>>>     \"OUTPUT_METRICS_DIR\": \"/opt/ml/processing/output/metrics\",\\n}\\n' is_hardcoded=True construction_method=None",
        "path='\\n    Load the trained XGBoost model and all preprocessing artifacts from the specified directory.\\n    Returns model, risk_tables, impute_dict, feature_columns, and hyperparameters.\\n    ' line_number=392 context='\\ndef load_model_artifacts(model_dir: str) -> Tuple[xgb.Booster, Dict[str, Any], Dict[str, Any], List[str], Dict[str, Any]]:\\n>>>     \"\"\"\\n    Load the trained XGBoost model and all preprocessing artifacts from the specified directory.\\n    Returns model, risk_tables, impute_dict, feature_columns, and hyperparameters.' is_hardcoded=True construction_method=None",
        "path='xgboost_model.bst' line_number=398 context='    logger.info(f\"Loading model artifacts from {model_dir}\")\\n    model = xgb.Booster()\\n>>>     model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:' is_hardcoded=False construction_method='os.path.join'",
        "path='xgboost_model.bst' line_number=398 context='    logger.info(f\"Loading model artifacts from {model_dir}\")\\n    model = xgb.Booster()\\n>>>     model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:' is_hardcoded=True construction_method=None",
        "path='Loaded xgboost_model.bst' line_number=399 context='    model = xgb.Booster()\\n    model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n>>>     logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)' is_hardcoded=True construction_method=None",
        "path='risk_table_map.pkl' line_number=400 context='    model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n>>>     with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")' is_hardcoded=False construction_method='os.path.join'",
        "path='risk_table_map.pkl' line_number=400 context='    model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n>>>     with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")' is_hardcoded=True construction_method=None",
        "path='Loaded risk_table_map.pkl' line_number=402 context='    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)\\n>>>     logger.info(\"Loaded risk_table_map.pkl\")\\n    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)' is_hardcoded=True construction_method=None",
        "path='impute_dict.pkl' line_number=403 context='        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")\\n>>>     with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")' is_hardcoded=False construction_method='os.path.join'",
        "path='impute_dict.pkl' line_number=403 context='        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")\\n>>>     with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")' is_hardcoded=True construction_method=None",
        "path='Loaded impute_dict.pkl' line_number=405 context='    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)\\n>>>     logger.info(\"Loaded impute_dict.pkl\")\\n    with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [' is_hardcoded=True construction_method=None",
        "path='feature_columns.txt' line_number=406 context='        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")\\n>>>     with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [\\n            line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")' is_hardcoded=False construction_method='os.path.join'",
        "path='feature_columns.txt' line_number=406 context='        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")\\n>>>     with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [\\n            line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")' is_hardcoded=True construction_method=None",
        "path='Loaded feature_columns.txt: ' line_number=410 context='            line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")\\n        ]\\n>>>     logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n    with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=411 context='        ]\\n    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n>>>     with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)\\n    logger.info(\"Loaded hyperparameters.json\")' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=411 context='        ]\\n    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n>>>     with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)\\n    logger.info(\"Loaded hyperparameters.json\")' is_hardcoded=True construction_method=None",
        "path='Loaded hyperparameters.json' line_number=413 context='    with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)\\n>>>     logger.info(\"Loaded hyperparameters.json\")\\n    return model, risk_tables, impute_dict, feature_columns, hyperparams\\n' is_hardcoded=True construction_method=None",
        "path='\\n    Apply risk table mapping and numerical imputation to the evaluation DataFrame.\\n    Ensures all features are numeric and columns are ordered as required by the model.\\n    Preserves any non-feature columns like id and label.\\n    ' line_number=418 context='\\ndef preprocess_eval_data(df: pd.DataFrame, feature_columns: List[str], risk_tables: Dict[str, Any], impute_dict: Dict[str, Any]) -> pd.DataFrame:\\n>>>     \"\"\"\\n    Apply risk table mapping and numerical imputation to the evaluation DataFrame.\\n    Ensures all features are numeric and columns are ordered as required by the model.' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=481 context='        # Format numeric values to 4 decimal places\\n        if isinstance(value, (int, float)):\\n>>>             formatted_value = f\"{value:.4f}\"\\n        else:\\n            formatted_value = str(value)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=495 context='    if is_binary:\\n        logger.info(\\n>>>             f\"METRIC_KEY: AUC-ROC               = {metrics.get(\\'auc_roc\\', \\'N/A\\'):.4f}\"\\n        )\\n        logger.info(' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=498 context='        )\\n        logger.info(\\n>>>             f\"METRIC_KEY: Average Precision     = {metrics.get(\\'average_precision\\', \\'N/A\\'):.4f}\"\\n        )\\n        logger.info(' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=501 context='        )\\n        logger.info(\\n>>>             f\"METRIC_KEY: F1 Score              = {metrics.get(\\'f1_score\\', \\'N/A\\'):.4f}\"\\n        )\\n    else:' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=505 context='    else:\\n        logger.info(\\n>>>             f\"METRIC_KEY: Macro AUC-ROC         = {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\"\\n        )\\n        logger.info(' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=508 context='        )\\n        logger.info(\\n>>>             f\"METRIC_KEY: Micro AUC-ROC         = {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\"\\n        )\\n        ap_macro = metrics.get(\"average_precision_macro\", \"N/A\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=512 context='        ap_macro = metrics.get(\"average_precision_macro\", \"N/A\")\\n        if isinstance(ap_macro, (int, float)):\\n>>>             logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro:.4f}\")\\n        else:\\n            logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro}\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=516 context='            logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro}\")\\n        logger.info(\\n>>>             f\"METRIC_KEY: Macro F1              = {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\"\\n        )\\n        logger.info(' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=519 context='        )\\n        logger.info(\\n>>>             f\"METRIC_KEY: Micro F1              = {metrics.get(\\'f1_score_micro\\', \\'N/A\\'):.4f}\"\\n        )\\n' is_hardcoded=True construction_method=None",
        "path='\\n    Compute binary classification metrics: AUC-ROC, average precision, and F1 score.\\n    ' line_number=527 context='\\ndef compute_metrics_binary(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\\n>>>     \"\"\"\\n    Compute binary classification metrics: AUC-ROC, average precision, and F1 score.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='precision_at_threshold_0.5' line_number=540 context='    # Add more detailed metrics\\n    precision, recall, _ = precision_recall_curve(y_true, y_score)\\n>>>     metrics[\"precision_at_threshold_0.5\"] = precision[0]\\n    metrics[\"recall_at_threshold_0.5\"] = recall[0]\\n' is_hardcoded=True construction_method=None",
        "path='recall_at_threshold_0.5' line_number=541 context='    precision, recall, _ = precision_recall_curve(y_true, y_score)\\n    metrics[\"precision_at_threshold_0.5\"] = precision[0]\\n>>>     metrics[\"recall_at_threshold_0.5\"] = recall[0]\\n\\n    # Thresholds at different operating points' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=550 context='    # Log basic summary and detailed formatted metrics\\n    logger.info(\\n>>>         f\"Binary metrics computed: AUC={metrics[\\'auc_roc\\']:.4f}, AP={metrics[\\'average_precision\\']:.4f}, F1={metrics[\\'f1_score\\']:.4f}\"\\n    )\\n    log_metrics_summary(metrics, is_binary=True)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=550 context='    # Log basic summary and detailed formatted metrics\\n    logger.info(\\n>>>         f\"Binary metrics computed: AUC={metrics[\\'auc_roc\\']:.4f}, AP={metrics[\\'average_precision\\']:.4f}, F1={metrics[\\'f1_score\\']:.4f}\"\\n    )\\n    log_metrics_summary(metrics, is_binary=True)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=550 context='    # Log basic summary and detailed formatted metrics\\n    logger.info(\\n>>>         f\"Binary metrics computed: AUC={metrics[\\'auc_roc\\']:.4f}, AP={metrics[\\'average_precision\\']:.4f}, F1={metrics[\\'f1_score\\']:.4f}\"\\n    )\\n    log_metrics_summary(metrics, is_binary=True)' is_hardcoded=True construction_method=None",
        "path='\\n    Compute multiclass metrics: one-vs-rest AUC-ROC, average precision, F1 for each class,\\n    and micro/macro averages for all metrics.\\n    ' line_number=558 context='\\ndef compute_metrics_multiclass(y_true: np.ndarray, y_prob: np.ndarray, n_classes: int) -> Dict[str, Union[int, float]]:\\n>>>     \"\"\"\\n    Compute multiclass metrics: one-vs-rest AUC-ROC, average precision, F1 for each class,\\n    and micro/macro averages for all metrics.' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=601 context='    # Log basic summary and detailed formatted metrics\\n    logger.info(\\n>>>         f\"Multiclass metrics computed: Macro AUC={metrics[\\'auc_roc_macro\\']:.4f}, Micro AUC={metrics[\\'auc_roc_micro\\']:.4f}\"\\n    )\\n    log_metrics_summary(metrics, is_binary=False)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=601 context='    # Log basic summary and detailed formatted metrics\\n    logger.info(\\n>>>         f\"Multiclass metrics computed: Macro AUC={metrics[\\'auc_roc_macro\\']:.4f}, Micro AUC={metrics[\\'auc_roc_micro\\']:.4f}\"\\n    )\\n    log_metrics_summary(metrics, is_binary=False)' is_hardcoded=True construction_method=None",
        "path='\\n    Load the first .csv or .parquet file found in the evaluation data directory.\\n    Returns a pandas DataFrame.\\n    ' line_number=609 context='\\ndef load_eval_data(eval_data_dir: str) -> pd.DataFrame:\\n>>>     \"\"\"\\n    Load the first .csv or .parquet file found in the evaluation data directory.\\n    Returns a pandas DataFrame.' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=618 context='            f\\n            for f in Path(eval_data_dir).glob(\"**/*\")\\n>>>             if f.suffix in [\".csv\", \".parquet\"]\\n        ]\\n    )' is_hardcoded=True construction_method=None",
        "path='No eval data file found in eval_data input.' line_number=622 context='    )\\n    if not eval_files:\\n>>>         logger.error(\"No eval data file found in eval_data input.\")\\n        raise RuntimeError(\"No eval data file found in eval_data input.\")\\n    eval_file = eval_files[0]' is_hardcoded=True construction_method=None",
        "path='No eval data file found in eval_data input.' line_number=623 context='    if not eval_files:\\n        logger.error(\"No eval data file found in eval_data input.\")\\n>>>         raise RuntimeError(\"No eval data file found in eval_data input.\")\\n    eval_file = eval_files[0]\\n    logger.info(f\"Using eval data file: {eval_file}\")' is_hardcoded=True construction_method=None",
        "path='\\n    Determine the ID and label columns in the DataFrame.\\n    Falls back to the first and second columns if not found.\\n    ' line_number=635 context='\\ndef get_id_label_columns(df: pd.DataFrame, id_field: str, label_field: str) -> Tuple[str, str]:\\n>>>     \"\"\"\\n    Determine the ID and label columns in the DataFrame.\\n    Falls back to the first and second columns if not found.' is_hardcoded=True construction_method=None",
        "path='\\n    Save predictions to a CSV file, including id, true label, and class probabilities.\\n    ' line_number=646 context='\\ndef save_predictions(ids: np.ndarray, y_true: np.ndarray, y_prob: np.ndarray, id_col: str, label_col: str, output_eval_dir: str) -> None:\\n>>>     \"\"\"\\n    Save predictions to a CSV file, including id, true label, and class probabilities.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='eval_predictions.csv' line_number=654 context='    for i, col in enumerate(prob_cols):\\n        out_df[col] = y_prob[:, i]\\n>>>     out_path = os.path.join(output_eval_dir, \"eval_predictions.csv\")\\n    out_df.to_csv(out_path, index=False)\\n    logger.info(f\"Saved predictions to {out_path}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='eval_predictions.csv' line_number=654 context='    for i, col in enumerate(prob_cols):\\n        out_df[col] = y_prob[:, i]\\n>>>     out_path = os.path.join(output_eval_dir, \"eval_predictions.csv\")\\n    out_df.to_csv(out_path, index=False)\\n    logger.info(f\"Saved predictions to {out_path}\")' is_hardcoded=True construction_method=None",
        "path='\\n    Save computed metrics as a JSON file.\\n    ' line_number=660 context='\\ndef save_metrics(metrics: Dict[str, Union[int, float, str]], output_metrics_dir: str) -> None:\\n>>>     \"\"\"\\n    Save computed metrics as a JSON file.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='metrics.json' line_number=663 context='    Save computed metrics as a JSON file.\\n    \"\"\"\\n>>>     out_path = os.path.join(output_metrics_dir, \"metrics.json\")\\n    with open(out_path, \"w\") as f:\\n        json.dump(metrics, f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='metrics.json' line_number=663 context='    Save computed metrics as a JSON file.\\n    \"\"\"\\n>>>     out_path = os.path.join(output_metrics_dir, \"metrics.json\")\\n    with open(out_path, \"w\") as f:\\n        json.dump(metrics, f, indent=2)' is_hardcoded=True construction_method=None",
        "path='metrics_summary.txt' line_number=669 context='\\n    # Also create a plain text summary for easy viewing\\n>>>     summary_path = os.path.join(output_metrics_dir, \"metrics_summary.txt\")\\n    with open(summary_path, \"w\") as f:\\n        f.write(\"METRICS SUMMARY\\\\n\")' is_hardcoded=False construction_method='os.path.join'",
        "path='metrics_summary.txt' line_number=669 context='\\n    # Also create a plain text summary for easy viewing\\n>>>     summary_path = os.path.join(output_metrics_dir, \"metrics_summary.txt\")\\n    with open(summary_path, \"w\") as f:\\n        f.write(\"METRICS SUMMARY\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=676 context='        # Write key metrics at the top\\n        if \"auc_roc\" in metrics:  # Binary classification\\n>>>             f.write(f\"AUC-ROC:           {metrics[\\'auc_roc\\']:.4f}\\\\n\")\\n            if \"average_precision\" in metrics:\\n                f.write(f\"Average Precision: {metrics[\\'average_precision\\']:.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=678 context='            f.write(f\"AUC-ROC:           {metrics[\\'auc_roc\\']:.4f}\\\\n\")\\n            if \"average_precision\" in metrics:\\n>>>                 f.write(f\"Average Precision: {metrics[\\'average_precision\\']:.4f}\\\\n\")\\n            if \"f1_score\" in metrics:\\n                f.write(f\"F1 Score:          {metrics[\\'f1_score\\']:.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=680 context='                f.write(f\"Average Precision: {metrics[\\'average_precision\\']:.4f}\\\\n\")\\n            if \"f1_score\" in metrics:\\n>>>                 f.write(f\"F1 Score:          {metrics[\\'f1_score\\']:.4f}\\\\n\")\\n        else:  # Multiclass classification\\n            f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=682 context='                f.write(f\"F1 Score:          {metrics[\\'f1_score\\']:.4f}\\\\n\")\\n        else:  # Multiclass classification\\n>>>             f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"AUC-ROC (Micro):   {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"F1 Score (Macro):  {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=683 context='        else:  # Multiclass classification\\n            f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n>>>             f.write(f\"AUC-ROC (Micro):   {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"F1 Score (Macro):  {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=684 context='            f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"AUC-ROC (Micro):   {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\\\\n\")\\n>>>             f.write(f\"F1 Score (Macro):  {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n\\n        f.write(\"=\" * 50 + \"\\\\n\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.6f' line_number=693 context='        for name, value in sorted(metrics.items()):\\n            if isinstance(value, (int, float)):\\n>>>                 f.write(f\"{name}: {value:.6f}\\\\n\")\\n            else:\\n                f.write(f\"{name}: {value}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='\\n    Plot ROC curve and save as JPG.\\n    ' line_number=701 context='\\ndef plot_and_save_roc_curve(y_true: np.ndarray, y_score: np.ndarray, output_dir: str, prefix: str = \"\") -> None:\\n>>>     \"\"\"\\n    Plot ROC curve and save as JPG.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=707 context='    auc = roc_auc_score(y_true, y_score)\\n    plt.figure()\\n>>>     plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc:.2f})\")\\n    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\\n    plt.xlabel(\"False Positive Rate\")' is_hardcoded=True construction_method=None",
        "path='roc_curve.jpg' line_number=713 context='    plt.title(\"ROC Curve\")\\n    plt.legend(loc=\"lower right\")\\n>>>     out_path = os.path.join(output_dir, f\"{prefix}roc_curve.jpg\")\\n    plt.savefig(out_path, format=\"jpg\")\\n    plt.close()' is_hardcoded=True construction_method=None",
        "path='\\n    Plot Precision-Recall curve and save as JPG.\\n    ' line_number=720 context='\\ndef plot_and_save_pr_curve(y_true: np.ndarray, y_score: np.ndarray, output_dir: str, prefix: str = \"\") -> None:\\n>>>     \"\"\"\\n    Plot Precision-Recall curve and save as JPG.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=726 context='    ap = average_precision_score(y_true, y_score)\\n    plt.figure()\\n>>>     plt.plot(recall, precision, label=f\"PR curve (AP = {ap:.2f})\")\\n    plt.xlabel(\"Recall\")\\n    plt.ylabel(\"Precision\")' is_hardcoded=True construction_method=None",
        "path='pr_curve.jpg' line_number=731 context='    plt.title(\"Precision-Recall Curve\")\\n    plt.legend(loc=\"lower left\")\\n>>>     out_path = os.path.join(output_dir, f\"{prefix}pr_curve.jpg\")\\n    plt.savefig(out_path, format=\"jpg\")\\n    plt.close()' is_hardcoded=True construction_method=None",
        "path='\\n    Run model prediction and evaluation, then save predictions and metrics.\\n    Also generate and save ROC and PR curves as JPG.\\n    ' line_number=747 context='    output_metrics_dir: str,\\n) -> None:\\n>>>     \"\"\"\\n    Run model prediction and evaluation, then save predictions and metrics.\\n    Also generate and save ROC and PR curves as JPG.' is_hardcoded=True construction_method=None",
        "path='Detected binary classification task based on model hyperparameters.' line_number=769 context='    if is_binary_model:\\n        logger.info(\\n>>>             \"Detected binary classification task based on model hyperparameters.\"\\n        )\\n        # Ensure y_true is also binary (0 or 1) for consistent metric calculation' is_hardcoded=True construction_method=None",
        "path=' classes.' line_number=779 context='        n_classes = y_prob.shape[1]\\n        logger.info(\\n>>>             f\"Detected multiclass classification task with {n_classes} classes.\"\\n        )\\n        metrics = compute_metrics_multiclass(y_true, y_prob, n_classes)' is_hardcoded=True construction_method=None",
        "path='Create a health check file to signal script completion.' line_number=798 context='\\ndef create_health_check_file(output_path: str) -> str:\\n>>>     \"\"\"Create a health check file to signal script completion.\"\"\"\\n    health_path = output_path\\n    with open(health_path, \"w\") as f:' is_hardcoded=True construction_method=None",
        "path='_SUCCESS' line_number=907 context='\\n        # Signal success\\n>>>         success_path = os.path.join(output_paths[\"metrics_output\"], \"_SUCCESS\")\\n        Path(success_path).touch()\\n        logger.info(f\"Created success marker: {success_path}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='_HEALTH' line_number=912 context='\\n        # Create health check file\\n>>>         health_path = os.path.join(output_paths[\"metrics_output\"], \"_HEALTH\")\\n        create_health_check_file(health_path)\\n        logger.info(f\"Created health check file: {health_path}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='_FAILURE' line_number=920 context='        # Log error and create failure marker\\n        logger.exception(f\"Script failed with error: {e}\")\\n>>>         failure_path = os.path.join(\\n            output_paths.get(\"metrics_output\", \"/tmp\"), \"_FAILURE\"\\n        )' is_hardcoded=False construction_method='os.path.join'"
      ],
      "env_var_accesses": [
        "variable_name='ID_FIELD' line_number=898 context='    # Collect environment variables - ID_FIELD and LABEL_FIELD are required per contract\\n    environ_vars = {\\n>>>         \"ID_FIELD\": os.environ.get(\"ID_FIELD\", \"id\"),  # Fallback for testing\\n        \"LABEL_FIELD\": os.environ.get(\"LABEL_FIELD\", \"label\"),  # Fallback for testing\\n    }' access_method='os.environ.get' has_default=True default_value='id'",
        "variable_name='LABEL_FIELD' line_number=899 context='    environ_vars = {\\n        \"ID_FIELD\": os.environ.get(\"ID_FIELD\", \"id\"),  # Fallback for testing\\n>>>         \"LABEL_FIELD\": os.environ.get(\"LABEL_FIELD\", \"label\"),  # Fallback for testing\\n    }\\n' access_method='os.environ.get' has_default=True default_value='label'"
      ],
      "imports": [
        "module_name='os' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=3 is_from_import=False imported_items=[]",
        "module_name='argparse' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='pandas' import_alias='pd' line_number=5 is_from_import=False imported_items=[]",
        "module_name='numpy' import_alias='np' line_number=6 is_from_import=False imported_items=[]",
        "module_name='pickle' import_alias='pkl' line_number=7 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=8 is_from_import=True imported_items=['Path']",
        "module_name='sklearn.metrics' import_alias=None line_number=9 is_from_import=True imported_items=['roc_auc_score', 'average_precision_score', 'precision_recall_curve', 'roc_curve', 'f1_score']",
        "module_name='xgboost' import_alias='xgb' line_number=16 is_from_import=False imported_items=[]",
        "module_name='matplotlib.pyplot' import_alias='plt' line_number=17 is_from_import=False imported_items=[]",
        "module_name='time' import_alias=None line_number=18 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=19 is_from_import=False imported_items=[]",
        "module_name='datetime' import_alias=None line_number=20 is_from_import=True imported_items=['datetime']",
        "module_name='typing' import_alias=None line_number=21 is_from_import=True imported_items=['Dict', 'Any', 'Optional', 'List', 'Tuple', 'Union']",
        "module_name='logging' import_alias=None line_number=374 is_from_import=False imported_items=[]"
      ],
      "argument_definitions": [
        "argument_name='job_type' line_number=882 is_required=True has_default=False default_value=None argument_type='str' choices=None"
      ],
      "file_operations": [
        "file_path='<file_object>' operation_type='read' line_number=401 context='    logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n>>>         risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")\\n    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:' mode=None method='pickle.load'",
        "file_path='<file_object>' operation_type='read' line_number=404 context='    logger.info(\"Loaded risk_table_map.pkl\")\\n    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n>>>         impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")\\n    with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:' mode=None method='pickle.load'",
        "file_path='<file_object>' operation_type='read' line_number=412 context='    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n    with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n>>>         hyperparams = json.load(f)\\n    logger.info(\"Loaded hyperparameters.json\")\\n    return model, risk_tables, impute_dict, feature_columns, hyperparams' mode=None method='json.load'",
        "file_path='<file_object>' operation_type='write' line_number=665 context='    out_path = os.path.join(output_metrics_dir, \"metrics.json\")\\n    with open(out_path, \"w\") as f:\\n>>>         json.dump(metrics, f, indent=2)\\n    logger.info(f\"Saved metrics to {out_path}\")\\n' mode=None method='json.dump'"
      ],
      "step_type": "Processing",
      "framework": "xgboost",
      "step_type_patterns": {}
    },
    "contract": {
      "entry_point": "xgboost_model_eval.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "processed_data": {
          "path": "/opt/ml/processing/input/eval_data"
        }
      },
      "outputs": {
        "eval_output": {
          "path": "/opt/ml/processing/output/eval"
        },
        "metrics_output": {
          "path": "/opt/ml/processing/output/metrics"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "ID_FIELD",
          "LABEL_FIELD"
        ],
        "optional": {}
      },
      "description": "\n    XGBoost model evaluation script that:\n    1. Loads trained XGBoost model and preprocessing artifacts\n    2. Loads and preprocesses evaluation data using risk tables and imputation\n    3. Generates predictions and computes performance metrics\n    4. Creates ROC and Precision-Recall curve visualizations\n    5. Saves predictions, metrics, and plots\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts directory containing:\n      - xgboost_model.bst: Trained XGBoost model\n      - risk_table_map.pkl: Risk table mappings for categorical features\n      - impute_dict.pkl: Imputation dictionary for numerical features\n      - feature_columns.txt: Feature column names and order\n      - hyperparameters.json: Model hyperparameters and metadata\n    - /opt/ml/processing/input/eval_data: Evaluation data (CSV or Parquet files)\n    \n    Output Structure:\n    - /opt/ml/processing/output/eval/eval_predictions.csv: Model predictions with probabilities\n    - /opt/ml/processing/output/metrics/metrics.json: Performance metrics\n    - /opt/ml/processing/output/metrics/roc_curve.jpg: ROC curve visualization\n    - /opt/ml/processing/output/metrics/pr_curve.jpg: Precision-Recall curve visualization\n    \n    Environment Variables:\n    - ID_FIELD: Name of the ID column in evaluation data\n    - LABEL_FIELD: Name of the label column in evaluation data\n    \n    Arguments:\n    - job_type: Type of evaluation job to perform (e.g., \"evaluation\", \"validation\")\n    \n    Supports both binary and multiclass classification with appropriate metrics for each.\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0",
        "xgboost": ">=1.6.0",
        "matplotlib": ">=3.5.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "step_type_resolution",
        "message": "Step type resolved via registry: XGBoostModelEval -> XGBoostModelEval -> Processing",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "original_spec_type": "XGBoostModelEval",
          "canonical_name": "XGBoostModelEval",
          "resolved_sagemaker_type": "Processing",
          "registry_available": true
        },
        "recommendation": "Using Processing step property paths for validation"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output eval_output: properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "logical_name": "eval_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
          "step_type": "processing",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output metrics_output: properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "logical_name": "metrics_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "step_type": "processing",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation_summary",
        "message": "Property path validation completed for xgboost_model_eval_contract",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "step_type": "processing",
          "node_type": "internal",
          "total_outputs": 2,
          "outputs_with_property_paths": 2,
          "validation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference",
          "documentation_version": "v2.92.2"
        },
        "recommendation": "Validated 2/2 outputs with property paths against SageMaker documentation"
      }
    ],
    "contract": {
      "entry_point": "xgboost_model_eval.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "processed_data": {
          "path": "/opt/ml/processing/input/eval_data"
        }
      },
      "outputs": {
        "eval_output": {
          "path": "/opt/ml/processing/output/eval"
        },
        "metrics_output": {
          "path": "/opt/ml/processing/output/metrics"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "ID_FIELD",
          "LABEL_FIELD"
        ],
        "optional": {}
      },
      "description": "\n    XGBoost model evaluation script that:\n    1. Loads trained XGBoost model and preprocessing artifacts\n    2. Loads and preprocesses evaluation data using risk tables and imputation\n    3. Generates predictions and computes performance metrics\n    4. Creates ROC and Precision-Recall curve visualizations\n    5. Saves predictions, metrics, and plots\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts directory containing:\n      - xgboost_model.bst: Trained XGBoost model\n      - risk_table_map.pkl: Risk table mappings for categorical features\n      - impute_dict.pkl: Imputation dictionary for numerical features\n      - feature_columns.txt: Feature column names and order\n      - hyperparameters.json: Model hyperparameters and metadata\n    - /opt/ml/processing/input/eval_data: Evaluation data (CSV or Parquet files)\n    \n    Output Structure:\n    - /opt/ml/processing/output/eval/eval_predictions.csv: Model predictions with probabilities\n    - /opt/ml/processing/output/metrics/metrics.json: Performance metrics\n    - /opt/ml/processing/output/metrics/roc_curve.jpg: ROC curve visualization\n    - /opt/ml/processing/output/metrics/pr_curve.jpg: Precision-Recall curve visualization\n    \n    Environment Variables:\n    - ID_FIELD: Name of the ID column in evaluation data\n    - LABEL_FIELD: Name of the label column in evaluation data\n    \n    Arguments:\n    - job_type: Type of evaluation job to perform (e.g., \"evaluation\", \"validation\")\n    \n    Supports both binary and multiclass classification with appropriate metrics for each.\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0",
        "xgboost": ">=1.6.0",
        "matplotlib": ">=3.5.0"
      }
    },
    "specifications": {
      "xgboost_model_eval_spec": {
        "step_type": "XGBoostModelEval",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "DummyTraining",
              "PyTorchModel",
              "XGBoostTraining",
              "PyTorchTraining",
              "XGBoostModel"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
          },
          {
            "logical_name": "processed_data",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "CurrencyConversion",
              "RiskTableMapping",
              "CradleDataLoading"
            ],
            "data_type": "S3Uri",
            "description": "Evaluation dataset for model assessment"
          }
        ],
        "outputs": [
          {
            "logical_name": "eval_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation results including predictions"
          },
          {
            "logical_name": "metrics_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "XGBoostModelEval",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "DummyTraining",
              "PyTorchModel",
              "XGBoostTraining",
              "PyTorchTraining",
              "XGBoostModel"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
          },
          {
            "logical_name": "processed_data",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "CurrencyConversion",
              "RiskTableMapping",
              "CradleDataLoading"
            ],
            "data_type": "S3Uri",
            "description": "Evaluation dataset for model assessment"
          }
        ],
        "outputs": [
          {
            "logical_name": "eval_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation results including predictions"
          },
          {
            "logical_name": "metrics_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
          }
        ]
      },
      "variants": {
        "generic": {
          "step_type": "XGBoostModelEval",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "model_input",
              "dependency_type": "model_artifacts",
              "required": true,
              "compatible_sources": [
                "DummyTraining",
                "PyTorchModel",
                "XGBoostTraining",
                "PyTorchTraining",
                "XGBoostModel"
              ],
              "data_type": "S3Uri",
              "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
            },
            {
              "logical_name": "processed_data",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "TabularPreprocessing",
                "CurrencyConversion",
                "RiskTableMapping",
                "CradleDataLoading"
              ],
              "data_type": "S3Uri",
              "description": "Evaluation dataset for model assessment"
            }
          ],
          "outputs": [
            {
              "logical_name": "eval_output",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Model evaluation results including predictions"
            },
            {
              "logical_name": "metrics_output",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
            }
          ]
        }
      },
      "unified_dependencies": {
        "model_input": {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "DummyTraining",
            "PyTorchModel",
            "XGBoostTraining",
            "PyTorchTraining",
            "XGBoostModel"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
        },
        "processed_data": {
          "logical_name": "processed_data",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "TabularPreprocessing",
            "CurrencyConversion",
            "RiskTableMapping",
            "CradleDataLoading"
          ],
          "data_type": "S3Uri",
          "description": "Evaluation dataset for model assessment"
        }
      },
      "unified_outputs": {
        "eval_output": {
          "logical_name": "eval_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation results including predictions"
        },
        "metrics_output": {
          "logical_name": "metrics_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
        }
      },
      "dependency_sources": {
        "model_input": [
          "generic"
        ],
        "processed_data": [
          "generic"
        ]
      },
      "output_sources": {
        "eval_output": [
          "generic"
        ],
        "metrics_output": [
          "generic"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": true,
    "issues": [],
    "specification": {
      "step_type": "XGBoostModelEval",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "DummyTraining",
            "PyTorchModel",
            "XGBoostTraining",
            "PyTorchTraining",
            "XGBoostModel"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
        },
        {
          "logical_name": "processed_data",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "TabularPreprocessing",
            "CurrencyConversion",
            "RiskTableMapping",
            "CradleDataLoading"
          ],
          "data_type": "S3Uri",
          "description": "Evaluation dataset for model assessment"
        }
      ],
      "outputs": [
        {
          "logical_name": "eval_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation results including predictions"
        },
        {
          "logical_name": "metrics_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
        }
      ]
    }
  },
  "level4": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "configuration_fields",
        "message": "Required configuration field not accessed in builder: project_root_folder",
        "details": {
          "field_name": "project_root_folder",
          "builder": "xgboost_model_eval"
        },
        "recommendation": "Access required field project_root_folder in builder or make it optional"
      },
      {
        "severity": "INFO",
        "category": "required_field_validation",
        "message": "Builder has required fields but no explicit validation logic detected",
        "details": {
          "required_fields": [
            "region",
            "author",
            "hyperparameters",
            "pipeline_version",
            "service_name",
            "project_root_folder",
            "role",
            "bucket"
          ],
          "builder": "xgboost_model_eval"
        },
        "recommendation": "Consider adding explicit validation logic for required configuration fields"
      }
    ],
    "builder_analysis": {
      "config_accesses": [
        {
          "field_name": "use_large_processing_instance",
          "line_number": 129,
          "context": "line_129"
        },
        {
          "field_name": "processing_instance_type_large",
          "line_number": 128,
          "context": "line_128"
        },
        {
          "field_name": "processing_instance_type_small",
          "line_number": 130,
          "context": "line_130"
        },
        {
          "field_name": "xgboost_framework_version",
          "line_number": 134,
          "context": "line_134"
        },
        {
          "field_name": "processing_instance_count",
          "line_number": 137,
          "context": "line_137"
        },
        {
          "field_name": "processing_volume_size",
          "line_number": 138,
          "context": "line_138"
        },
        {
          "field_name": "hyperparameters",
          "line_number": 158,
          "context": "line_158"
        },
        {
          "field_name": "hyperparameters",
          "line_number": 159,
          "context": "line_159"
        },
        {
          "field_name": "hyperparameters",
          "line_number": 160,
          "context": "line_160"
        },
        {
          "field_name": "hyperparameters",
          "line_number": 161,
          "context": "line_161"
        },
        {
          "field_name": "job_type",
          "line_number": 292,
          "context": "line_292"
        }
      ],
      "validation_calls": [],
      "default_assignments": [],
      "class_definitions": [
        {
          "class_name": "XGBoostModelEvalStepBuilder",
          "line_number": 26,
          "base_classes": [
            "StepBuilderBase"
          ],
          "decorators": []
        }
      ],
      "method_definitions": [
        {
          "method_name": "__init__",
          "line_number": 34,
          "args": [
            "self",
            "config",
            "sagemaker_session",
            "role",
            "registry_manager",
            "dependency_resolver"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "validate_configuration",
          "line_number": 70,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_create_processor",
          "line_number": 117,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_environment_variables",
          "line_number": 144,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_inputs",
          "line_number": 166,
          "args": [
            "self",
            "inputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_outputs",
          "line_number": 219,
          "args": [
            "self",
            "outputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_job_arguments",
          "line_number": 280,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "create_step",
          "line_number": 300,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        }
      ],
      "import_statements": [
        {
          "type": "from_import",
          "module": "typing",
          "name": "Dict",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Optional",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Any",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "List",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "pathlib",
          "name": "Path",
          "alias": null,
          "line_number": 2
        },
        {
          "type": "import",
          "module": "logging",
          "alias": null,
          "line_number": 3
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "ProcessingStep",
          "alias": null,
          "line_number": 5
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "Step",
          "alias": null,
          "line_number": 5
        },
        {
          "type": "from_import",
          "module": "sagemaker.processing",
          "name": "ProcessingInput",
          "alias": null,
          "line_number": 6
        },
        {
          "type": "from_import",
          "module": "sagemaker.processing",
          "name": "ProcessingOutput",
          "alias": null,
          "line_number": 6
        },
        {
          "type": "from_import",
          "module": "sagemaker.xgboost",
          "name": "XGBoostProcessor",
          "alias": null,
          "line_number": 7
        },
        {
          "type": "from_import",
          "module": "configs.config_xgboost_model_eval_step",
          "name": "XGBoostModelEvalConfig",
          "alias": null,
          "line_number": 9
        },
        {
          "type": "from_import",
          "module": "core.base.builder_base",
          "name": "StepBuilderBase",
          "alias": null,
          "line_number": 10
        },
        {
          "type": "from_import",
          "module": "core.deps.registry_manager",
          "name": "RegistryManager",
          "alias": null,
          "line_number": 11
        },
        {
          "type": "from_import",
          "module": "core.deps.dependency_resolver",
          "name": "UnifiedDependencyResolver",
          "alias": null,
          "line_number": 12
        },
        {
          "type": "from_import",
          "module": "specs.xgboost_model_eval_spec",
          "name": "MODEL_EVAL_SPEC",
          "alias": null,
          "line_number": 16
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.functions",
          "name": "Join",
          "alias": null,
          "line_number": 261
        }
      ],
      "config_class_usage": []
    },
    "config_analysis": {
      "class_name": "XGBoostModelEvalConfig",
      "fields": {
        "author": {
          "type": "<class 'str'>",
          "required": true
        },
        "bucket": {
          "type": "<class 'str'>",
          "required": true
        },
        "role": {
          "type": "<class 'str'>",
          "required": true
        },
        "region": {
          "type": "<class 'str'>",
          "required": true
        },
        "service_name": {
          "type": "<class 'str'>",
          "required": true
        },
        "pipeline_version": {
          "type": "<class 'str'>",
          "required": true
        },
        "model_class": {
          "type": "<class 'str'>",
          "required": false
        },
        "current_date": {
          "type": "<class 'str'>",
          "required": false
        },
        "framework_version": {
          "type": "<class 'str'>",
          "required": false
        },
        "py_version": {
          "type": "<class 'str'>",
          "required": false
        },
        "source_dir": {
          "type": "typing.Optional[str]",
          "required": false
        },
        "project_root_folder": {
          "type": "<class 'str'>",
          "required": true
        },
        "processing_instance_count": {
          "type": "<class 'int'>",
          "required": false
        },
        "processing_volume_size": {
          "type": "<class 'int'>",
          "required": false
        },
        "processing_instance_type_large": {
          "type": "<class 'str'>",
          "required": false
        },
        "processing_instance_type_small": {
          "type": "<class 'str'>",
          "required": false
        },
        "use_large_processing_instance": {
          "type": "<class 'bool'>",
          "required": false
        },
        "processing_source_dir": {
          "type": "typing.Optional[str]",
          "required": false
        },
        "processing_entry_point": {
          "type": "<class 'str'>",
          "required": false
        },
        "processing_script_arguments": {
          "type": "typing.Optional[typing.List[str]]",
          "required": false
        },
        "processing_framework_version": {
          "type": "<class 'str'>",
          "required": false
        },
        "hyperparameters": {
          "type": "<class 'cursus.steps.hyperparams.hyperparameters_xgboost.XGBoostModelHyperparameters'>",
          "required": true
        },
        "job_type": {
          "type": "<class 'str'>",
          "required": false
        },
        "eval_metric_choices": {
          "type": "typing.List[str]",
          "required": false
        },
        "xgboost_framework_version": {
          "type": "<class 'str'>",
          "required": false
        },
        "aws_region": {
          "type": "property",
          "required": false
        },
        "effective_instance_type": {
          "type": "property",
          "required": false
        },
        "effective_source_dir": {
          "type": "property",
          "required": false
        },
        "model_extra": {
          "type": "property",
          "required": false
        },
        "model_fields_set": {
          "type": "property",
          "required": false
        },
        "pipeline_description": {
          "type": "property",
          "required": false
        },
        "pipeline_name": {
          "type": "property",
          "required": false
        },
        "pipeline_s3_loc": {
          "type": "property",
          "required": false
        },
        "resolved_processing_source_dir": {
          "type": "property",
          "required": false
        },
        "resolved_source_dir": {
          "type": "property",
          "required": false
        },
        "script_contract": {
          "type": "property",
          "required": false
        },
        "script_path": {
          "type": "property",
          "required": false
        },
        "step_catalog": {
          "type": "property",
          "required": false
        }
      },
      "required_fields": [
        "author",
        "bucket",
        "role",
        "region",
        "service_name",
        "pipeline_version",
        "project_root_folder",
        "hyperparameters"
      ],
      "optional_fields": [
        "model_class",
        "current_date",
        "framework_version",
        "py_version",
        "source_dir",
        "processing_instance_count",
        "processing_volume_size",
        "processing_instance_type_large",
        "processing_instance_type_small",
        "use_large_processing_instance",
        "processing_source_dir",
        "processing_entry_point",
        "processing_script_arguments",
        "processing_framework_version",
        "job_type",
        "eval_metric_choices",
        "xgboost_framework_version",
        "aws_region",
        "effective_instance_type",
        "effective_source_dir",
        "model_extra",
        "model_fields_set",
        "pipeline_description",
        "pipeline_name",
        "pipeline_s3_loc",
        "resolved_processing_source_dir",
        "resolved_source_dir",
        "script_contract",
        "script_path",
        "step_catalog"
      ],
      "default_values": {
        "author": "PydanticUndefined",
        "bucket": "PydanticUndefined",
        "role": "PydanticUndefined",
        "region": "PydanticUndefined",
        "service_name": "PydanticUndefined",
        "pipeline_version": "PydanticUndefined",
        "model_class": "xgboost",
        "current_date": "PydanticUndefined",
        "framework_version": "2.1.0",
        "py_version": "py310",
        "source_dir": null,
        "project_root_folder": "PydanticUndefined",
        "processing_instance_count": 1,
        "processing_volume_size": 500,
        "processing_instance_type_large": "ml.m5.4xlarge",
        "processing_instance_type_small": "ml.m5.2xlarge",
        "use_large_processing_instance": true,
        "processing_source_dir": null,
        "processing_entry_point": "xgboost_model_eval.py",
        "processing_script_arguments": null,
        "processing_framework_version": "1.2-1",
        "hyperparameters": "PydanticUndefined",
        "job_type": "calibration",
        "eval_metric_choices": "PydanticUndefined",
        "xgboost_framework_version": "1.5-1"
      }
    }
  },
  "overall_status": "PASSING",
  "scoring": {
    "overall_score": 100.0,
    "quality_rating": "Excellent",
    "level_scores": {
      "level1_script_contract": 100.0,
      "level2_contract_spec": 100.0,
      "level3_spec_dependencies": 100.0,
      "level4_builder_config": 100.0
    }
  },
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_model_eval.py",
    "validation_timestamp": "2025-09-29T22:16:23.327087",
    "validator_version": "1.0.0"
  }
}