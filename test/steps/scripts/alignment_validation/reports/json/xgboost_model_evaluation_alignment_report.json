{
  "script_name": "xgboost_model_evaluation",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "arguments",
        "message": "Script defines config-driven argument provided by builder: --job-type (accessed as args.job_type)",
        "details": {
          "cli_argument": "job-type",
          "python_attribute": "job_type",
          "script": "xgboost_model_evaluation",
          "source": "builder"
        },
        "recommendation": "Argument --job-type is provided by builder - no action needed"
      },
      {
        "severity": "INFO",
        "category": "testability_compliance",
        "message": "Main function follows testability pattern with all required parameters",
        "details": {
          "script": "xgboost_model_evaluation",
          "testability_parameters": [
            "input_paths",
            "environ_vars",
            "job_args",
            "output_paths"
          ]
        },
        "recommendation": "No action needed - script follows testability best practices"
      },
      {
        "severity": "WARNING",
        "category": "testability_env_access",
        "message": "Helper functions use direct environment access - consider parameter passing",
        "details": {
          "script": "xgboost_model_evaluation",
          "helper_accesses": [
            {
              "function": null,
              "variable": "ID_FIELD",
              "line_number": 436
            },
            {
              "function": null,
              "variable": "LABEL_FIELD",
              "line_number": 437
            }
          ]
        },
        "recommendation": "Pass environment variables as parameters to helper functions instead of direct access"
      },
      {
        "severity": "WARNING",
        "category": "testability_entry_point",
        "message": "Main function expects environ_vars parameter but no environment collection found in entry point",
        "details": {
          "script": "xgboost_model_evaluation"
        },
        "recommendation": "Add environment variable collection in __main__ block to pass to main function"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 380
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 380
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 381
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for input_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "input_paths",
          "current_pattern": "input_paths.get",
          "line_number": 381
        },
        "recommendation": "Use input_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 382
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 382
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 383
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 383
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for environ_vars",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "environ_vars",
          "current_pattern": "environ_vars.get",
          "line_number": 386
        },
        "recommendation": "Use environ_vars['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for environ_vars",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "environ_vars",
          "current_pattern": "environ_vars.get",
          "line_number": 387
        },
        "recommendation": "Use environ_vars['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for job_args",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "job_args",
          "current_pattern": "job_args.job_type",
          "line_number": 390
        },
        "recommendation": "Use job_args['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_parameter_access",
        "message": "Consider using dictionary-style access for output_paths",
        "details": {
          "script": "xgboost_model_evaluation",
          "parameter": "output_paths",
          "current_pattern": "output_paths.get",
          "line_number": 458
        },
        "recommendation": "Use output_paths['key'] for accessing nested values"
      },
      {
        "severity": "INFO",
        "category": "testability_container_support",
        "message": "No container detection found - consider adding hybrid mode support",
        "details": {
          "script": "xgboost_model_evaluation"
        },
        "recommendation": "Add container detection to support both local and container execution"
      },
      {
        "severity": "WARNING",
        "category": "testability_helper_functions",
        "message": "Helper function 'None' accesses environment directly",
        "details": {
          "script": "xgboost_model_evaluation",
          "function": null,
          "env_variables": [
            "ID_FIELD",
            "LABEL_FIELD"
          ],
          "line_numbers": [
            436,
            437
          ]
        },
        "recommendation": "Refactor 'None' to accept environment variables as parameters"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_model_evaluation.py",
      "path_references": [
        "path='/opt/ml/processing/input/model' line_number=28 context='# Container path constants - aligned with script contract\\nCONTAINER_PATHS = {\\n>>>     \"MODEL_DIR\": \"/opt/ml/processing/input/model\",\\n    \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n    \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/input/eval_data' line_number=29 context='CONTAINER_PATHS = {\\n    \"MODEL_DIR\": \"/opt/ml/processing/input/model\",\\n>>>     \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n    \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",\\n    \"OUTPUT_METRICS_DIR\": \"/opt/ml/processing/output/metrics\"' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/eval' line_number=30 context='    \"MODEL_DIR\": \"/opt/ml/processing/input/model\",\\n    \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n>>>     \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",\\n    \"OUTPUT_METRICS_DIR\": \"/opt/ml/processing/output/metrics\"\\n}' is_hardcoded=True construction_method=None",
        "path='/opt/ml/processing/output/metrics' line_number=31 context='    \"EVAL_DATA_DIR\": \"/opt/ml/processing/input/eval_data\",\\n    \"OUTPUT_EVAL_DIR\": \"/opt/ml/processing/output/eval\",\\n>>>     \"OUTPUT_METRICS_DIR\": \"/opt/ml/processing/output/metrics\"\\n}\\n' is_hardcoded=True construction_method=None",
        "path='\\n    Load the trained XGBoost model and all preprocessing artifacts from the specified directory.\\n    Returns model, risk_tables, impute_dict, feature_columns, and hyperparameters.\\n    ' line_number=35 context='\\ndef load_model_artifacts(model_dir):\\n>>>     \"\"\"\\n    Load the trained XGBoost model and all preprocessing artifacts from the specified directory.\\n    Returns model, risk_tables, impute_dict, feature_columns, and hyperparameters.' is_hardcoded=True construction_method=None",
        "path='xgboost_model.bst' line_number=41 context='    logger.info(f\"Loading model artifacts from {model_dir}\")\\n    model = xgb.Booster()\\n>>>     model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:' is_hardcoded=False construction_method='os.path.join'",
        "path='xgboost_model.bst' line_number=41 context='    logger.info(f\"Loading model artifacts from {model_dir}\")\\n    model = xgb.Booster()\\n>>>     model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:' is_hardcoded=True construction_method=None",
        "path='Loaded xgboost_model.bst' line_number=42 context='    model = xgb.Booster()\\n    model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n>>>     logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)' is_hardcoded=True construction_method=None",
        "path='risk_table_map.pkl' line_number=43 context='    model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n>>>     with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")' is_hardcoded=False construction_method='os.path.join'",
        "path='risk_table_map.pkl' line_number=43 context='    model.load_model(os.path.join(model_dir, \"xgboost_model.bst\"))\\n    logger.info(\"Loaded xgboost_model.bst\")\\n>>>     with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")' is_hardcoded=True construction_method=None",
        "path='Loaded risk_table_map.pkl' line_number=45 context='    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n        risk_tables = pkl.load(f)\\n>>>     logger.info(\"Loaded risk_table_map.pkl\")\\n    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)' is_hardcoded=True construction_method=None",
        "path='impute_dict.pkl' line_number=46 context='        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")\\n>>>     with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")' is_hardcoded=False construction_method='os.path.join'",
        "path='impute_dict.pkl' line_number=46 context='        risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")\\n>>>     with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")' is_hardcoded=True construction_method=None",
        "path='Loaded impute_dict.pkl' line_number=48 context='    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n        impute_dict = pkl.load(f)\\n>>>     logger.info(\"Loaded impute_dict.pkl\")\\n    with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")]' is_hardcoded=True construction_method=None",
        "path='feature_columns.txt' line_number=49 context='        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")\\n>>>     with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")]\\n    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='feature_columns.txt' line_number=49 context='        impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")\\n>>>     with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")]\\n    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")' is_hardcoded=True construction_method=None",
        "path='Loaded feature_columns.txt: ' line_number=51 context='    with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:\\n        feature_columns = [line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")]\\n>>>     logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n    with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=52 context='        feature_columns = [line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")]\\n    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n>>>     with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)\\n    logger.info(\"Loaded hyperparameters.json\")' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=52 context='        feature_columns = [line.strip().split(\",\")[1] for line in f if not line.startswith(\"#\")]\\n    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n>>>     with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)\\n    logger.info(\"Loaded hyperparameters.json\")' is_hardcoded=True construction_method=None",
        "path='Loaded hyperparameters.json' line_number=54 context='    with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n        hyperparams = json.load(f)\\n>>>     logger.info(\"Loaded hyperparameters.json\")\\n    return model, risk_tables, impute_dict, feature_columns, hyperparams\\n' is_hardcoded=True construction_method=None",
        "path='\\n    Apply risk table mapping and numerical imputation to the evaluation DataFrame.\\n    Ensures all features are numeric and columns are ordered as required by the model.\\n    ' line_number=58 context='\\ndef preprocess_eval_data(df, feature_columns, risk_tables, impute_dict):\\n>>>     \"\"\"\\n    Apply risk table mapping and numerical imputation to the evaluation DataFrame.\\n    Ensures all features are numeric and columns are ordered as required by the model.' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=101 context='        # Format numeric values to 4 decimal places\\n        if isinstance(value, (int, float)):\\n>>>             formatted_value = f\"{value:.4f}\"\\n        else:\\n            formatted_value = str(value)' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=114 context='    \\n    if is_binary:\\n>>>         logger.info(f\"METRIC_KEY: AUC-ROC               = {metrics.get(\\'auc_roc\\', \\'N/A\\'):.4f}\")\\n        logger.info(f\"METRIC_KEY: Average Precision     = {metrics.get(\\'average_precision\\', \\'N/A\\'):.4f}\")\\n        logger.info(f\"METRIC_KEY: F1 Score              = {metrics.get(\\'f1_score\\', \\'N/A\\'):.4f}\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=115 context='    if is_binary:\\n        logger.info(f\"METRIC_KEY: AUC-ROC               = {metrics.get(\\'auc_roc\\', \\'N/A\\'):.4f}\")\\n>>>         logger.info(f\"METRIC_KEY: Average Precision     = {metrics.get(\\'average_precision\\', \\'N/A\\'):.4f}\")\\n        logger.info(f\"METRIC_KEY: F1 Score              = {metrics.get(\\'f1_score\\', \\'N/A\\'):.4f}\")\\n    else:' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=116 context='        logger.info(f\"METRIC_KEY: AUC-ROC               = {metrics.get(\\'auc_roc\\', \\'N/A\\'):.4f}\")\\n        logger.info(f\"METRIC_KEY: Average Precision     = {metrics.get(\\'average_precision\\', \\'N/A\\'):.4f}\")\\n>>>         logger.info(f\"METRIC_KEY: F1 Score              = {metrics.get(\\'f1_score\\', \\'N/A\\'):.4f}\")\\n    else:\\n        logger.info(f\"METRIC_KEY: Macro AUC-ROC         = {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=118 context='        logger.info(f\"METRIC_KEY: F1 Score              = {metrics.get(\\'f1_score\\', \\'N/A\\'):.4f}\")\\n    else:\\n>>>         logger.info(f\"METRIC_KEY: Macro AUC-ROC         = {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\")\\n        logger.info(f\"METRIC_KEY: Micro AUC-ROC         = {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\")\\n        ap_macro = metrics.get(\\'average_precision_macro\\', \\'N/A\\')' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=119 context='    else:\\n        logger.info(f\"METRIC_KEY: Macro AUC-ROC         = {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\")\\n>>>         logger.info(f\"METRIC_KEY: Micro AUC-ROC         = {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\")\\n        ap_macro = metrics.get(\\'average_precision_macro\\', \\'N/A\\')\\n        if isinstance(ap_macro, (int, float)):' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=122 context='        ap_macro = metrics.get(\\'average_precision_macro\\', \\'N/A\\')\\n        if isinstance(ap_macro, (int, float)):\\n>>>             logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro:.4f}\")\\n        else:\\n            logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro}\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=125 context='        else:\\n            logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro}\")\\n>>>         logger.info(f\"METRIC_KEY: Macro F1              = {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\")\\n        logger.info(f\"METRIC_KEY: Micro F1              = {metrics.get(\\'f1_score_micro\\', \\'N/A\\'):.4f}\")\\n    ' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=126 context='            logger.info(f\"METRIC_KEY: Macro Average Precision = {ap_macro}\")\\n        logger.info(f\"METRIC_KEY: Macro F1              = {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\")\\n>>>         logger.info(f\"METRIC_KEY: Micro F1              = {metrics.get(\\'f1_score_micro\\', \\'N/A\\'):.4f}\")\\n    \\n    # Add a summary section with pass/fail criteria if defined' is_hardcoded=True construction_method=None",
        "path='\\n    Compute binary classification metrics: AUC-ROC, average precision, and F1 score.\\n    ' line_number=132 context='\\ndef compute_metrics_binary(y_true, y_prob):\\n>>>     \"\"\"\\n    Compute binary classification metrics: AUC-ROC, average precision, and F1 score.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='precision_at_threshold_0.5' line_number=145 context='    # Add more detailed metrics\\n    precision, recall, _ = precision_recall_curve(y_true, y_score)\\n>>>     metrics[\"precision_at_threshold_0.5\"] = precision[0]\\n    metrics[\"recall_at_threshold_0.5\"] = recall[0]\\n    ' is_hardcoded=True construction_method=None",
        "path='recall_at_threshold_0.5' line_number=146 context='    precision, recall, _ = precision_recall_curve(y_true, y_score)\\n    metrics[\"precision_at_threshold_0.5\"] = precision[0]\\n>>>     metrics[\"recall_at_threshold_0.5\"] = recall[0]\\n    \\n    # Thresholds at different operating points' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=154 context='    \\n    # Log basic summary and detailed formatted metrics\\n>>>     logger.info(f\"Binary metrics computed: AUC={metrics[\\'auc_roc\\']:.4f}, AP={metrics[\\'average_precision\\']:.4f}, F1={metrics[\\'f1_score\\']:.4f}\")\\n    log_metrics_summary(metrics, is_binary=True)\\n    ' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=154 context='    \\n    # Log basic summary and detailed formatted metrics\\n>>>     logger.info(f\"Binary metrics computed: AUC={metrics[\\'auc_roc\\']:.4f}, AP={metrics[\\'average_precision\\']:.4f}, F1={metrics[\\'f1_score\\']:.4f}\")\\n    log_metrics_summary(metrics, is_binary=True)\\n    ' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=154 context='    \\n    # Log basic summary and detailed formatted metrics\\n>>>     logger.info(f\"Binary metrics computed: AUC={metrics[\\'auc_roc\\']:.4f}, AP={metrics[\\'average_precision\\']:.4f}, F1={metrics[\\'f1_score\\']:.4f}\")\\n    log_metrics_summary(metrics, is_binary=True)\\n    ' is_hardcoded=True construction_method=None",
        "path='\\n    Compute multiclass metrics: one-vs-rest AUC-ROC, average precision, F1 for each class,\\n    and micro/macro averages for all metrics.\\n    ' line_number=160 context='\\ndef compute_metrics_multiclass(y_true, y_prob, n_classes):\\n>>>     \"\"\"\\n    Compute multiclass metrics: one-vs-rest AUC-ROC, average precision, F1 for each class,\\n    and micro/macro averages for all metrics.' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=192 context='    \\n    # Log basic summary and detailed formatted metrics\\n>>>     logger.info(f\"Multiclass metrics computed: Macro AUC={metrics[\\'auc_roc_macro\\']:.4f}, Micro AUC={metrics[\\'auc_roc_micro\\']:.4f}\")\\n    log_metrics_summary(metrics, is_binary=False)\\n    ' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=192 context='    \\n    # Log basic summary and detailed formatted metrics\\n>>>     logger.info(f\"Multiclass metrics computed: Macro AUC={metrics[\\'auc_roc_macro\\']:.4f}, Micro AUC={metrics[\\'auc_roc_micro\\']:.4f}\")\\n    log_metrics_summary(metrics, is_binary=False)\\n    ' is_hardcoded=True construction_method=None",
        "path='\\n    Load the first .csv or .parquet file found in the evaluation data directory.\\n    Returns a pandas DataFrame.\\n    ' line_number=198 context='\\ndef load_eval_data(eval_data_dir):\\n>>>     \"\"\"\\n    Load the first .csv or .parquet file found in the evaluation data directory.\\n    Returns a pandas DataFrame.' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=203 context='    \"\"\"\\n    logger.info(f\"Loading eval data from {eval_data_dir}\")\\n>>>     eval_files = sorted([f for f in Path(eval_data_dir).glob(\"**/*\") if f.suffix in [\".csv\", \".parquet\"]])\\n    if not eval_files:\\n        logger.error(\"No eval data file found in eval_data input.\")' is_hardcoded=True construction_method=None",
        "path='No eval data file found in eval_data input.' line_number=205 context='    eval_files = sorted([f for f in Path(eval_data_dir).glob(\"**/*\") if f.suffix in [\".csv\", \".parquet\"]])\\n    if not eval_files:\\n>>>         logger.error(\"No eval data file found in eval_data input.\")\\n        raise RuntimeError(\"No eval data file found in eval_data input.\")\\n    eval_file = eval_files[0]' is_hardcoded=True construction_method=None",
        "path='No eval data file found in eval_data input.' line_number=206 context='    if not eval_files:\\n        logger.error(\"No eval data file found in eval_data input.\")\\n>>>         raise RuntimeError(\"No eval data file found in eval_data input.\")\\n    eval_file = eval_files[0]\\n    logger.info(f\"Using eval data file: {eval_file}\")' is_hardcoded=True construction_method=None",
        "path='\\n    Determine the ID and label columns in the DataFrame.\\n    Falls back to the first and second columns if not found.\\n    ' line_number=217 context='\\ndef get_id_label_columns(df, id_field, label_field):\\n>>>     \"\"\"\\n    Determine the ID and label columns in the DataFrame.\\n    Falls back to the first and second columns if not found.' is_hardcoded=True construction_method=None",
        "path='\\n    Save predictions to a CSV file, including id, true label, and class probabilities.\\n    ' line_number=227 context='\\ndef save_predictions(ids, y_true, y_prob, id_col, label_col, output_eval_dir):\\n>>>     \"\"\"\\n    Save predictions to a CSV file, including id, true label, and class probabilities.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='eval_predictions.csv' line_number=235 context='    for i, col in enumerate(prob_cols):\\n        out_df[col] = y_prob[:, i]\\n>>>     out_path = os.path.join(output_eval_dir, \"eval_predictions.csv\")\\n    out_df.to_csv(out_path, index=False)\\n    logger.info(f\"Saved predictions to {out_path}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='eval_predictions.csv' line_number=235 context='    for i, col in enumerate(prob_cols):\\n        out_df[col] = y_prob[:, i]\\n>>>     out_path = os.path.join(output_eval_dir, \"eval_predictions.csv\")\\n    out_df.to_csv(out_path, index=False)\\n    logger.info(f\"Saved predictions to {out_path}\")' is_hardcoded=True construction_method=None",
        "path='\\n    Save computed metrics as a JSON file.\\n    ' line_number=240 context='\\ndef save_metrics(metrics, output_metrics_dir):\\n>>>     \"\"\"\\n    Save computed metrics as a JSON file.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='metrics.json' line_number=243 context='    Save computed metrics as a JSON file.\\n    \"\"\"\\n>>>     out_path = os.path.join(output_metrics_dir, \"metrics.json\")\\n    with open(out_path, \"w\") as f:\\n        json.dump(metrics, f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='metrics.json' line_number=243 context='    Save computed metrics as a JSON file.\\n    \"\"\"\\n>>>     out_path = os.path.join(output_metrics_dir, \"metrics.json\")\\n    with open(out_path, \"w\") as f:\\n        json.dump(metrics, f, indent=2)' is_hardcoded=True construction_method=None",
        "path='metrics_summary.txt' line_number=249 context='    \\n    # Also create a plain text summary for easy viewing\\n>>>     summary_path = os.path.join(output_metrics_dir, \"metrics_summary.txt\")\\n    with open(summary_path, \"w\") as f:\\n        f.write(\"METRICS SUMMARY\\\\n\")' is_hardcoded=False construction_method='os.path.join'",
        "path='metrics_summary.txt' line_number=249 context='    \\n    # Also create a plain text summary for easy viewing\\n>>>     summary_path = os.path.join(output_metrics_dir, \"metrics_summary.txt\")\\n    with open(summary_path, \"w\") as f:\\n        f.write(\"METRICS SUMMARY\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=256 context='        # Write key metrics at the top\\n        if \"auc_roc\" in metrics:  # Binary classification\\n>>>             f.write(f\"AUC-ROC:           {metrics[\\'auc_roc\\']:.4f}\\\\n\")\\n            if \\'average_precision\\' in metrics:\\n                f.write(f\"Average Precision: {metrics[\\'average_precision\\']:.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=258 context='            f.write(f\"AUC-ROC:           {metrics[\\'auc_roc\\']:.4f}\\\\n\")\\n            if \\'average_precision\\' in metrics:\\n>>>                 f.write(f\"Average Precision: {metrics[\\'average_precision\\']:.4f}\\\\n\")\\n            if \\'f1_score\\' in metrics:\\n                f.write(f\"F1 Score:          {metrics[\\'f1_score\\']:.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=260 context='                f.write(f\"Average Precision: {metrics[\\'average_precision\\']:.4f}\\\\n\")\\n            if \\'f1_score\\' in metrics:\\n>>>                 f.write(f\"F1 Score:          {metrics[\\'f1_score\\']:.4f}\\\\n\")\\n        else:  # Multiclass classification\\n            f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=262 context='                f.write(f\"F1 Score:          {metrics[\\'f1_score\\']:.4f}\\\\n\")\\n        else:  # Multiclass classification\\n>>>             f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"AUC-ROC (Micro):   {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"F1 Score (Macro):  {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=263 context='        else:  # Multiclass classification\\n            f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n>>>             f.write(f\"AUC-ROC (Micro):   {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"F1 Score (Macro):  {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n        ' is_hardcoded=True construction_method=None",
        "path='.4f' line_number=264 context='            f.write(f\"AUC-ROC (Macro):   {metrics.get(\\'auc_roc_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n            f.write(f\"AUC-ROC (Micro):   {metrics.get(\\'auc_roc_micro\\', \\'N/A\\'):.4f}\\\\n\")\\n>>>             f.write(f\"F1 Score (Macro):  {metrics.get(\\'f1_score_macro\\', \\'N/A\\'):.4f}\\\\n\")\\n        \\n        f.write(\"=\" * 50 + \"\\\\n\\\\n\")' is_hardcoded=True construction_method=None",
        "path='.6f' line_number=273 context='        for name, value in sorted(metrics.items()):\\n            if isinstance(value, (int, float)):\\n>>>                 f.write(f\"{name}: {value:.6f}\\\\n\")\\n            else:\\n                f.write(f\"{name}: {value}\\\\n\")' is_hardcoded=True construction_method=None",
        "path='\\n    Plot ROC curve and save as JPG.\\n    ' line_number=280 context='\\ndef plot_and_save_roc_curve(y_true, y_score, output_dir, prefix=\"\"):\\n>>>     \"\"\"\\n    Plot ROC curve and save as JPG.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=286 context='    auc = roc_auc_score(y_true, y_score)\\n    plt.figure()\\n>>>     plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc:.2f})\")\\n    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\\n    plt.xlabel(\"False Positive Rate\")' is_hardcoded=True construction_method=None",
        "path='roc_curve.jpg' line_number=292 context='    plt.title(\"ROC Curve\")\\n    plt.legend(loc=\"lower right\")\\n>>>     out_path = os.path.join(output_dir, f\"{prefix}roc_curve.jpg\")\\n    plt.savefig(out_path, format=\"jpg\")\\n    plt.close()' is_hardcoded=True construction_method=None",
        "path='\\n    Plot Precision-Recall curve and save as JPG.\\n    ' line_number=298 context='\\ndef plot_and_save_pr_curve(y_true, y_score, output_dir, prefix=\"\"):\\n>>>     \"\"\"\\n    Plot Precision-Recall curve and save as JPG.\\n    \"\"\"' is_hardcoded=True construction_method=None",
        "path='.2f' line_number=304 context='    ap = average_precision_score(y_true, y_score)\\n    plt.figure()\\n>>>     plt.plot(recall, precision, label=f\"PR curve (AP = {ap:.2f})\")\\n    plt.xlabel(\"Recall\")\\n    plt.ylabel(\"Precision\")' is_hardcoded=True construction_method=None",
        "path='pr_curve.jpg' line_number=309 context='    plt.title(\"Precision-Recall Curve\")\\n    plt.legend(loc=\"lower left\")\\n>>>     out_path = os.path.join(output_dir, f\"{prefix}pr_curve.jpg\")\\n    plt.savefig(out_path, format=\"jpg\")\\n    plt.close()' is_hardcoded=True construction_method=None",
        "path='\\n    Run model prediction and evaluation, then save predictions and metrics.\\n    Also generate and save ROC and PR curves as JPG.\\n    ' line_number=315 context='\\ndef evaluate_model(model, df, feature_columns, id_col, label_col, hyperparams, output_eval_dir, output_metrics_dir):\\n>>>     \"\"\"\\n    Run model prediction and evaluation, then save predictions and metrics.\\n    Also generate and save ROC and PR curves as JPG.' is_hardcoded=True construction_method=None",
        "path='Detected binary classification task based on model hyperparameters.' line_number=336 context='    \\n    if is_binary_model:\\n>>>         logger.info(\"Detected binary classification task based on model hyperparameters.\")\\n        # Ensure y_true is also binary (0 or 1) for consistent metric calculation\\n        y_true = (y_true > 0).astype(int)' is_hardcoded=True construction_method=None",
        "path=' classes.' line_number=344 context='    else:\\n        n_classes = y_prob.shape[1]\\n>>>         logger.info(f\"Detected multiclass classification task with {n_classes} classes.\")\\n        metrics = compute_metrics_multiclass(y_true, y_prob, n_classes)\\n        for i in range(n_classes):' is_hardcoded=True construction_method=None",
        "path='Create a health check file to signal script completion.' line_number=357 context='\\ndef create_health_check_file(output_path: str) -> str:\\n>>>     \"\"\"Create a health check file to signal script completion.\"\"\"\\n    health_path = output_path\\n    with open(health_path, \"w\") as f:' is_hardcoded=True construction_method=None",
        "path='_SUCCESS' line_number=445 context='        \\n        # Signal success\\n>>>         success_path = os.path.join(output_paths[\"metrics_output\"], \"_SUCCESS\")\\n        Path(success_path).touch()\\n        logger.info(f\"Created success marker: {success_path}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='_HEALTH' line_number=450 context='        \\n        # Create health check file\\n>>>         health_path = os.path.join(output_paths[\"metrics_output\"], \"_HEALTH\")\\n        create_health_check_file(health_path)\\n        logger.info(f\"Created health check file: {health_path}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='_FAILURE' line_number=458 context='        # Log error and create failure marker\\n        logger.exception(f\"Script failed with error: {e}\")\\n>>>         failure_path = os.path.join(output_paths.get(\"metrics_output\", \"/tmp\"), \"_FAILURE\")\\n        os.makedirs(os.path.dirname(failure_path), exist_ok=True)\\n        with open(failure_path, \"w\") as f:' is_hardcoded=False construction_method='os.path.join'"
      ],
      "env_var_accesses": [
        "variable_name='ID_FIELD' line_number=436 context='    # Collect environment variables - ID_FIELD and LABEL_FIELD are required per contract\\n    environ_vars = {\\n>>>         \"ID_FIELD\": os.environ.get(\"ID_FIELD\", \"id\"),  # Fallback for testing\\n        \"LABEL_FIELD\": os.environ.get(\"LABEL_FIELD\", \"label\"),  # Fallback for testing\\n    }' access_method='os.environ.get' has_default=True default_value='id'",
        "variable_name='LABEL_FIELD' line_number=437 context='    environ_vars = {\\n        \"ID_FIELD\": os.environ.get(\"ID_FIELD\", \"id\"),  # Fallback for testing\\n>>>         \"LABEL_FIELD\": os.environ.get(\"LABEL_FIELD\", \"label\"),  # Fallback for testing\\n    }\\n    ' access_method='os.environ.get' has_default=True default_value='label'"
      ],
      "imports": [
        "module_name='os' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=3 is_from_import=False imported_items=[]",
        "module_name='argparse' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='pandas' import_alias='pd' line_number=5 is_from_import=False imported_items=[]",
        "module_name='numpy' import_alias='np' line_number=6 is_from_import=False imported_items=[]",
        "module_name='pickle' import_alias='pkl' line_number=7 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=8 is_from_import=True imported_items=['Path']",
        "module_name='sklearn.metrics' import_alias=None line_number=9 is_from_import=True imported_items=['roc_auc_score', 'average_precision_score', 'precision_recall_curve', 'roc_curve', 'f1_score']",
        "module_name='xgboost' import_alias='xgb' line_number=10 is_from_import=False imported_items=[]",
        "module_name='matplotlib.pyplot' import_alias='plt' line_number=11 is_from_import=False imported_items=[]",
        "module_name='time' import_alias=None line_number=12 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=13 is_from_import=False imported_items=[]",
        "module_name='datetime' import_alias=None line_number=14 is_from_import=True imported_items=['datetime']",
        "module_name='typing' import_alias=None line_number=15 is_from_import=True imported_items=['Dict', 'Any', 'Optional']",
        "module_name='processing.risk_table_processor' import_alias=None line_number=17 is_from_import=True imported_items=['RiskTableMappingProcessor']",
        "module_name='processing.numerical_imputation_processor' import_alias=None line_number=18 is_from_import=True imported_items=['NumericalVariableImputationProcessor']",
        "module_name='logging' import_alias=None line_number=20 is_from_import=False imported_items=[]"
      ],
      "argument_definitions": [
        "argument_name='job_type' line_number=420 is_required=True has_default=False default_value=None argument_type='str' choices=None"
      ],
      "file_operations": [
        "file_path='<file_object>' operation_type='read' line_number=44 context='    logger.info(\"Loaded xgboost_model.bst\")\\n    with open(os.path.join(model_dir, \"risk_table_map.pkl\"), \"rb\") as f:\\n>>>         risk_tables = pkl.load(f)\\n    logger.info(\"Loaded risk_table_map.pkl\")\\n    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:' mode=None method='pickle.load'",
        "file_path='<file_object>' operation_type='read' line_number=47 context='    logger.info(\"Loaded risk_table_map.pkl\")\\n    with open(os.path.join(model_dir, \"impute_dict.pkl\"), \"rb\") as f:\\n>>>         impute_dict = pkl.load(f)\\n    logger.info(\"Loaded impute_dict.pkl\")\\n    with open(os.path.join(model_dir, \"feature_columns.txt\"), \"r\") as f:' mode=None method='pickle.load'",
        "file_path='<file_object>' operation_type='read' line_number=53 context='    logger.info(f\"Loaded feature_columns.txt: {feature_columns}\")\\n    with open(os.path.join(model_dir, \"hyperparameters.json\"), \"r\") as f:\\n>>>         hyperparams = json.load(f)\\n    logger.info(\"Loaded hyperparameters.json\")\\n    return model, risk_tables, impute_dict, feature_columns, hyperparams' mode=None method='json.load'",
        "file_path='<file_object>' operation_type='write' line_number=245 context='    out_path = os.path.join(output_metrics_dir, \"metrics.json\")\\n    with open(out_path, \"w\") as f:\\n>>>         json.dump(metrics, f, indent=2)\\n    logger.info(f\"Saved metrics to {out_path}\")\\n    ' mode=None method='json.dump'"
      ]
    },
    "contract": {
      "entry_point": "xgboost_model_evaluation.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "processed_data": {
          "path": "/opt/ml/processing/input/eval_data"
        }
      },
      "outputs": {
        "eval_output": {
          "path": "/opt/ml/processing/output/eval"
        },
        "metrics_output": {
          "path": "/opt/ml/processing/output/metrics"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "ID_FIELD",
          "LABEL_FIELD"
        ],
        "optional": {}
      },
      "description": "\n    XGBoost model evaluation script that:\n    1. Loads trained XGBoost model and preprocessing artifacts\n    2. Loads and preprocesses evaluation data using risk tables and imputation\n    3. Generates predictions and computes performance metrics\n    4. Creates ROC and Precision-Recall curve visualizations\n    5. Saves predictions, metrics, and plots\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts directory containing:\n      - xgboost_model.bst: Trained XGBoost model\n      - risk_table_map.pkl: Risk table mappings for categorical features\n      - impute_dict.pkl: Imputation dictionary for numerical features\n      - feature_columns.txt: Feature column names and order\n      - hyperparameters.json: Model hyperparameters and metadata\n    - /opt/ml/processing/input/eval_data: Evaluation data (CSV or Parquet files)\n    \n    Output Structure:\n    - /opt/ml/processing/output/eval/eval_predictions.csv: Model predictions with probabilities\n    - /opt/ml/processing/output/metrics/metrics.json: Performance metrics\n    - /opt/ml/processing/output/metrics/roc_curve.jpg: ROC curve visualization\n    - /opt/ml/processing/output/metrics/pr_curve.jpg: Precision-Recall curve visualization\n    \n    Environment Variables:\n    - ID_FIELD: Name of the ID column in evaluation data\n    - LABEL_FIELD: Name of the label column in evaluation data\n    \n    Arguments:\n    - job_type: Type of evaluation job to perform (e.g., \"evaluation\", \"validation\")\n    \n    Supports both binary and multiclass classification with appropriate metrics for each.\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0",
        "xgboost": ">=1.6.0",
        "matplotlib": ">=3.5.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "step_type_resolution",
        "message": "Step type resolved via registry: XGBoostModelEval -> XGBoostModelEval -> Processing",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "original_spec_type": "XGBoostModelEval",
          "canonical_name": "XGBoostModelEval",
          "resolved_sagemaker_type": "Processing",
          "registry_available": true
        },
        "recommendation": "Using Processing step property paths for validation"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output eval_output: properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "logical_name": "eval_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
          "step_type": "processing",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output metrics_output: properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "logical_name": "metrics_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "step_type": "processing",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation_summary",
        "message": "Property path validation completed for xgboost_model_eval_contract",
        "details": {
          "contract": "xgboost_model_eval_contract",
          "step_type": "processing",
          "node_type": "internal",
          "total_outputs": 2,
          "outputs_with_property_paths": 2,
          "validation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference",
          "documentation_version": "v2.92.2"
        },
        "recommendation": "Validated 2/2 outputs with property paths against SageMaker documentation"
      }
    ],
    "contract": {
      "entry_point": "xgboost_model_evaluation.py",
      "inputs": {
        "model_input": {
          "path": "/opt/ml/processing/input/model"
        },
        "processed_data": {
          "path": "/opt/ml/processing/input/eval_data"
        }
      },
      "outputs": {
        "eval_output": {
          "path": "/opt/ml/processing/output/eval"
        },
        "metrics_output": {
          "path": "/opt/ml/processing/output/metrics"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [
          "ID_FIELD",
          "LABEL_FIELD"
        ],
        "optional": {}
      },
      "description": "\n    XGBoost model evaluation script that:\n    1. Loads trained XGBoost model and preprocessing artifacts\n    2. Loads and preprocesses evaluation data using risk tables and imputation\n    3. Generates predictions and computes performance metrics\n    4. Creates ROC and Precision-Recall curve visualizations\n    5. Saves predictions, metrics, and plots\n    \n    Input Structure:\n    - /opt/ml/processing/input/model: Model artifacts directory containing:\n      - xgboost_model.bst: Trained XGBoost model\n      - risk_table_map.pkl: Risk table mappings for categorical features\n      - impute_dict.pkl: Imputation dictionary for numerical features\n      - feature_columns.txt: Feature column names and order\n      - hyperparameters.json: Model hyperparameters and metadata\n    - /opt/ml/processing/input/eval_data: Evaluation data (CSV or Parquet files)\n    \n    Output Structure:\n    - /opt/ml/processing/output/eval/eval_predictions.csv: Model predictions with probabilities\n    - /opt/ml/processing/output/metrics/metrics.json: Performance metrics\n    - /opt/ml/processing/output/metrics/roc_curve.jpg: ROC curve visualization\n    - /opt/ml/processing/output/metrics/pr_curve.jpg: Precision-Recall curve visualization\n    \n    Environment Variables:\n    - ID_FIELD: Name of the ID column in evaluation data\n    - LABEL_FIELD: Name of the label column in evaluation data\n    \n    Arguments:\n    - job_type: Type of evaluation job to perform (e.g., \"evaluation\", \"validation\")\n    \n    Supports both binary and multiclass classification with appropriate metrics for each.\n    ",
      "framework_requirements": {
        "pandas": ">=1.3.0",
        "numpy": ">=1.21.0",
        "scikit-learn": ">=1.0.0",
        "xgboost": ">=1.6.0",
        "matplotlib": ">=3.5.0"
      }
    },
    "specifications": {
      "xgboost_model_eval_spec": {
        "step_type": "XGBoostModelEval",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "PyTorchModel",
              "XGBoostModel",
              "DummyTraining",
              "PyTorchTraining",
              "XGBoostTraining"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
          },
          {
            "logical_name": "processed_data",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "CradleDataLoading",
              "CurrencyConversion",
              "RiskTableMapping",
              "TabularPreprocessing"
            ],
            "data_type": "S3Uri",
            "description": "Evaluation dataset for model assessment"
          }
        ],
        "outputs": [
          {
            "logical_name": "eval_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation results including predictions"
          },
          {
            "logical_name": "metrics_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "XGBoostModelEval",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "model_input",
            "dependency_type": "model_artifacts",
            "required": true,
            "compatible_sources": [
              "PyTorchModel",
              "XGBoostModel",
              "DummyTraining",
              "PyTorchTraining",
              "XGBoostTraining"
            ],
            "data_type": "S3Uri",
            "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
          },
          {
            "logical_name": "processed_data",
            "dependency_type": "processing_output",
            "required": true,
            "compatible_sources": [
              "CradleDataLoading",
              "CurrencyConversion",
              "RiskTableMapping",
              "TabularPreprocessing"
            ],
            "data_type": "S3Uri",
            "description": "Evaluation dataset for model assessment"
          }
        ],
        "outputs": [
          {
            "logical_name": "eval_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation results including predictions"
          },
          {
            "logical_name": "metrics_output",
            "output_type": "processing_output",
            "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
            "data_type": "S3Uri",
            "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
          }
        ]
      },
      "variants": {
        "generic": {
          "step_type": "XGBoostModelEval",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "model_input",
              "dependency_type": "model_artifacts",
              "required": true,
              "compatible_sources": [
                "PyTorchModel",
                "XGBoostModel",
                "DummyTraining",
                "PyTorchTraining",
                "XGBoostTraining"
              ],
              "data_type": "S3Uri",
              "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
            },
            {
              "logical_name": "processed_data",
              "dependency_type": "processing_output",
              "required": true,
              "compatible_sources": [
                "CradleDataLoading",
                "CurrencyConversion",
                "RiskTableMapping",
                "TabularPreprocessing"
              ],
              "data_type": "S3Uri",
              "description": "Evaluation dataset for model assessment"
            }
          ],
          "outputs": [
            {
              "logical_name": "eval_output",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Model evaluation results including predictions"
            },
            {
              "logical_name": "metrics_output",
              "output_type": "processing_output",
              "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
              "data_type": "S3Uri",
              "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
            }
          ]
        }
      },
      "unified_dependencies": {
        "model_input": {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "PyTorchModel",
            "XGBoostModel",
            "DummyTraining",
            "PyTorchTraining",
            "XGBoostTraining"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
        },
        "processed_data": {
          "logical_name": "processed_data",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "CradleDataLoading",
            "CurrencyConversion",
            "RiskTableMapping",
            "TabularPreprocessing"
          ],
          "data_type": "S3Uri",
          "description": "Evaluation dataset for model assessment"
        }
      },
      "unified_outputs": {
        "eval_output": {
          "logical_name": "eval_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation results including predictions"
        },
        "metrics_output": {
          "logical_name": "metrics_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
        }
      },
      "dependency_sources": {
        "model_input": [
          "generic"
        ],
        "processed_data": [
          "generic"
        ]
      },
      "output_sources": {
        "eval_output": [
          "generic"
        ],
        "metrics_output": [
          "generic"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": true,
    "issues": [],
    "specification": {
      "step_type": "XGBoostModelEval",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "model_input",
          "dependency_type": "model_artifacts",
          "required": true,
          "compatible_sources": [
            "PyTorchModel",
            "XGBoostModel",
            "DummyTraining",
            "PyTorchTraining",
            "XGBoostTraining"
          ],
          "data_type": "S3Uri",
          "description": "Trained model artifacts to be evaluated (includes hyperparameters.json)"
        },
        {
          "logical_name": "processed_data",
          "dependency_type": "processing_output",
          "required": true,
          "compatible_sources": [
            "CradleDataLoading",
            "CurrencyConversion",
            "RiskTableMapping",
            "TabularPreprocessing"
          ],
          "data_type": "S3Uri",
          "description": "Evaluation dataset for model assessment"
        }
      ],
      "outputs": [
        {
          "logical_name": "eval_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['eval_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation results including predictions"
        },
        {
          "logical_name": "metrics_output",
          "output_type": "processing_output",
          "property_path": "properties.ProcessingOutputConfig.Outputs['metrics_output'].S3Output.S3Uri",
          "data_type": "S3Uri",
          "description": "Model evaluation metrics (AUC, precision, recall, etc.)"
        }
      ]
    }
  },
  "level4": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "config_import",
        "message": "Builder may not be properly importing configuration class xgboost_model_evaluationConfig",
        "details": {
          "config_class": "xgboost_model_evaluationConfig",
          "builder": "xgboost_model_evaluation"
        },
        "recommendation": "Ensure builder imports and uses xgboost_model_evaluationConfig"
      }
    ],
    "builder_analysis": {
      "config_accesses": [],
      "validation_calls": [],
      "default_assignments": [],
      "class_definitions": [
        {
          "class_name": "XGBoostModelEvalStepBuilder",
          "line_number": 27,
          "base_classes": [
            "StepBuilderBase"
          ],
          "decorators": [
            "Call"
          ]
        }
      ],
      "method_definitions": [
        {
          "method_name": "__init__",
          "line_number": 35,
          "args": [
            "self",
            "config",
            "sagemaker_session",
            "role",
            "notebook_root",
            "registry_manager",
            "dependency_resolver"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "validate_configuration",
          "line_number": 75,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_create_processor",
          "line_number": 111,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_environment_variables",
          "line_number": 134,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_inputs",
          "line_number": 156,
          "args": [
            "self",
            "inputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_outputs",
          "line_number": 209,
          "args": [
            "self",
            "outputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_job_arguments",
          "line_number": 264,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "create_step",
          "line_number": 284,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        }
      ],
      "import_statements": [
        {
          "type": "from_import",
          "module": "typing",
          "name": "Dict",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Optional",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Any",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "List",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "pathlib",
          "name": "Path",
          "alias": null,
          "line_number": 2
        },
        {
          "type": "import",
          "module": "logging",
          "alias": null,
          "line_number": 3
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "ProcessingStep",
          "alias": null,
          "line_number": 5
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "Step",
          "alias": null,
          "line_number": 5
        },
        {
          "type": "from_import",
          "module": "sagemaker.processing",
          "name": "ProcessingInput",
          "alias": null,
          "line_number": 6
        },
        {
          "type": "from_import",
          "module": "sagemaker.processing",
          "name": "ProcessingOutput",
          "alias": null,
          "line_number": 6
        },
        {
          "type": "from_import",
          "module": "sagemaker.xgboost",
          "name": "XGBoostProcessor",
          "alias": null,
          "line_number": 7
        },
        {
          "type": "from_import",
          "module": "configs.config_xgboost_model_eval_step",
          "name": "XGBoostModelEvalConfig",
          "alias": null,
          "line_number": 9
        },
        {
          "type": "from_import",
          "module": "core.base.builder_base",
          "name": "StepBuilderBase",
          "alias": null,
          "line_number": 10
        },
        {
          "type": "from_import",
          "module": "core.deps.registry_manager",
          "name": "RegistryManager",
          "alias": null,
          "line_number": 11
        },
        {
          "type": "from_import",
          "module": "core.deps.dependency_resolver",
          "name": "UnifiedDependencyResolver",
          "alias": null,
          "line_number": 12
        },
        {
          "type": "from_import",
          "module": "registry.builder_registry",
          "name": "register_builder",
          "alias": null,
          "line_number": 13
        },
        {
          "type": "from_import",
          "module": "specs.xgboost_model_eval_spec",
          "name": "MODEL_EVAL_SPEC",
          "alias": null,
          "line_number": 17
        }
      ],
      "config_class_usage": []
    },
    "config_analysis": {
      "class_name": "xgboost_model_evaluationConfig",
      "fields": {},
      "required_fields": [],
      "optional_fields": [],
      "default_values": {},
      "load_error": "Configuration class not found in /Users/tianpeixie/github_workspace/cursus/src/cursus/steps/configs/config_xgboost_model_eval_step.py. Available classes: ['Any', 'Path', 'ProcessingStepConfigBase', 'XGBoostModelEvalConfig', 'XGBoostModelHyperparameters']"
    }
  },
  "overall_status": "PASSING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_model_evaluation.py",
    "contract_mapping": "xgboost_model_eval_contract",
    "validation_timestamp": "2025-08-12T21:58:51.743763",
    "validator_version": "1.0.0"
  }
}