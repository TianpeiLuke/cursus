{
  "script_name": "xgboost_training",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "path_usage",
        "message": "Script correctly uses parent directory to construct file path: /opt/ml/input/data/config \u2192 /opt/ml/input/data/config/hyperparameters.json",
        "details": {
          "script_path": "/opt/ml/input/data/config",
          "contract_path": "/opt/ml/input/data/config/hyperparameters.json",
          "construction_method": "os.path.join",
          "script": "xgboost_training"
        },
        "recommendation": "Path usage pattern is correct - no action needed"
      },
      {
        "severity": "INFO",
        "category": "file_operations",
        "message": "Contract declares input not read by script: /opt/ml/input/data/config/hyperparameters.json",
        "details": {
          "path": "/opt/ml/input/data/config/hyperparameters.json",
          "operation": "read",
          "script": "xgboost_training"
        },
        "recommendation": "Either read /opt/ml/input/data/config/hyperparameters.json in script or remove from contract inputs"
      },
      {
        "severity": "INFO",
        "category": "testability_compliance",
        "message": "Main function follows testability pattern with all required parameters",
        "details": {
          "script": "xgboost_training",
          "testability_parameters": [
            "job_args",
            "environ_vars",
            "input_paths",
            "output_paths"
          ]
        },
        "recommendation": "No action needed - script follows testability best practices"
      },
      {
        "severity": "WARNING",
        "category": "testability_entry_point",
        "message": "Main function expects environ_vars parameter but no environment collection found in entry point",
        "details": {
          "script": "xgboost_training"
        },
        "recommendation": "Add environment variable collection in __main__ block to pass to main function"
      },
      {
        "severity": "WARNING",
        "category": "testability_parameter_usage",
        "message": "Testability parameters defined but not used: job_args, environ_vars",
        "details": {
          "script": "xgboost_training",
          "unused_parameters": [
            "job_args",
            "environ_vars"
          ],
          "used_parameters": [
            "output_paths",
            "input_paths"
          ]
        },
        "recommendation": "Either use the testability parameters or remove them from function signature"
      },
      {
        "severity": "INFO",
        "category": "testability_container_support",
        "message": "No container detection found - consider adding hybrid mode support",
        "details": {
          "script": "xgboost_training"
        },
        "recommendation": "Add container detection to support both local and container execution"
      },
      {
        "severity": "WARNING",
        "category": "step_type_enhancement_error",
        "message": "Failed to apply step type enhancements: 'str' object has no attribute 'get'",
        "details": {
          "script": "xgboost_training",
          "error": "'str' object has no attribute 'get'"
        },
        "recommendation": "Check step type detection and framework patterns"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_training.py",
      "path_references": [
        "path='\\n    Load everything from your pipeline\u2019s XGBoostModelHyperparameters,\\n    plus the two risk-table params this script needs.\\n    ' line_number=77 context='\\nclass XGBoostConfig(XGBoostModelHyperparameters):\\n>>>     \"\"\"\\n    Load everything from your pipeline\u2019s XGBoostModelHyperparameters,\\n    plus the two risk-table params this script needs.' is_hardcoded=True construction_method=None",
        "path='Loads and validates the hyperparameters JSON file.' line_number=92 context='# -------------------------------------------------------------------------\\ndef load_and_validate_config(hparam_path: str) -> dict:\\n>>>     \"\"\"Loads and validates the hyperparameters JSON file.\"\"\"\\n    try:\\n        with open(hparam_path, \"r\") as f:' is_hardcoded=True construction_method=None",
        "path='Finds the first supported data file in a directory.' line_number=114 context='\\ndef find_first_data_file(data_dir: str) -> str:\\n>>>     \"\"\"Finds the first supported data file in a directory.\"\"\"\\n    if not os.path.isdir(data_dir):\\n        return None' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=118 context='        return None\\n    for fname in sorted(os.listdir(data_dir)):\\n>>>         if fname.lower().endswith((\".csv\", \".parquet\", \".json\")):\\n            return os.path.join(data_dir, fname)\\n    return None' is_hardcoded=True construction_method=None",
        "path='.json' line_number=118 context='        return None\\n    for fname in sorted(os.listdir(data_dir)):\\n>>>         if fname.lower().endswith((\".csv\", \".parquet\", \".json\")):\\n            return os.path.join(data_dir, fname)\\n    return None' is_hardcoded=True construction_method=None",
        "path='Loads the training, validation, and test datasets.' line_number=123 context='\\ndef load_datasets(input_path: str) -> tuple:\\n>>>     \"\"\"Loads the training, validation, and test datasets.\"\"\"\\n    train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n    val_file = find_first_data_file(os.path.join(input_path, \"val\"))' is_hardcoded=True construction_method=None",
        "path='train' line_number=124 context='def load_datasets(input_path: str) -> tuple:\\n    \"\"\"Loads the training, validation, and test datasets.\"\"\"\\n>>>     train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n    val_file = find_first_data_file(os.path.join(input_path, \"val\"))\\n    test_file = find_first_data_file(os.path.join(input_path, \"test\"))' is_hardcoded=False construction_method='os.path.join'",
        "path='val' line_number=125 context='    \"\"\"Loads the training, validation, and test datasets.\"\"\"\\n    train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n>>>     val_file = find_first_data_file(os.path.join(input_path, \"val\"))\\n    test_file = find_first_data_file(os.path.join(input_path, \"test\"))\\n' is_hardcoded=False construction_method='os.path.join'",
        "path='test' line_number=126 context='    train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n    val_file = find_first_data_file(os.path.join(input_path, \"val\"))\\n>>>     test_file = find_first_data_file(os.path.join(input_path, \"test\"))\\n\\n    if not train_file or not val_file or not test_file:' is_hardcoded=False construction_method='os.path.join'",
        "path='Training, validation, or test data file not found in the expected subfolders.' line_number=129 context='\\n    if not train_file or not val_file or not test_file:\\n>>>         raise FileNotFoundError(\"Training, validation, or test data file not found in the expected subfolders.\")\\n\\n    train_df = pd.read_parquet(train_file) if train_file.endswith(\\'.parquet\\') else pd.read_csv(train_file)' is_hardcoded=True construction_method=None",
        "path='Applies numerical imputation to the datasets.' line_number=139 context='\\ndef apply_numerical_imputation(config: dict, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple:\\n>>>     \"\"\"Applies numerical imputation to the datasets.\"\"\"\\n    imputer = NumericalVariableImputationProcessor(variables=config[\\'tab_field_list\\'], strategy=\\'mean\\')\\n    imputer.fit(train_df)' is_hardcoded=True construction_method=None",
        "path='Fits risk tables on training data and applies them to all splits.' line_number=150 context='\\ndef fit_and_apply_risk_tables(config: dict, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple:\\n>>>     \"\"\"Fits risk tables on training data and applies them to all splits.\"\"\"\\n    risk_processors = {}\\n    train_df_transformed = train_df.copy()' is_hardcoded=True construction_method=None",
        "path='xgboost_model.bst' line_number=279 context='    \\n    # Save XGBoost model\\n>>>     model_file = os.path.join(model_path, \"xgboost_model.bst\")\\n    model.save_model(model_file)\\n    logger.info(f\"Saved XGBoost model to {model_file}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='xgboost_model.bst' line_number=279 context='    \\n    # Save XGBoost model\\n>>>     model_file = os.path.join(model_path, \"xgboost_model.bst\")\\n    model.save_model(model_file)\\n    logger.info(f\"Saved XGBoost model to {model_file}\")' is_hardcoded=True construction_method=None",
        "path='risk_table_map.pkl' line_number=284 context='\\n    # Save risk tables\\n>>>     risk_map_file = os.path.join(model_path, \"risk_table_map.pkl\")\\n    with open(risk_map_file, \"wb\") as f:\\n        pkl.dump(risk_tables, f)' is_hardcoded=False construction_method='os.path.join'",
        "path='risk_table_map.pkl' line_number=284 context='\\n    # Save risk tables\\n>>>     risk_map_file = os.path.join(model_path, \"risk_table_map.pkl\")\\n    with open(risk_map_file, \"wb\") as f:\\n        pkl.dump(risk_tables, f)' is_hardcoded=True construction_method=None",
        "path='impute_dict.pkl' line_number=290 context='    \\n    # Save imputation dictionary\\n>>>     impute_file = os.path.join(model_path, \"impute_dict.pkl\")\\n    with open(impute_file, \"wb\") as f:\\n        pkl.dump(impute_dict, f)' is_hardcoded=False construction_method='os.path.join'",
        "path='impute_dict.pkl' line_number=290 context='    \\n    # Save imputation dictionary\\n>>>     impute_file = os.path.join(model_path, \"impute_dict.pkl\")\\n    with open(impute_file, \"wb\") as f:\\n        pkl.dump(impute_dict, f)' is_hardcoded=True construction_method=None",
        "path='feature_importance.json' line_number=296 context='\\n    # Save feature importance\\n>>>     fmap_json = os.path.join(model_path, \"feature_importance.json\")\\n    with open(fmap_json, \"w\") as f:\\n        json.dump(model.get_fscore(), f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='feature_importance.json' line_number=296 context='\\n    # Save feature importance\\n>>>     fmap_json = os.path.join(model_path, \"feature_importance.json\")\\n    with open(fmap_json, \"w\") as f:\\n        json.dump(model.get_fscore(), f, indent=2)' is_hardcoded=True construction_method=None",
        "path='feature_columns.txt' line_number=302 context='    \\n    # Save feature columns with ordering information\\n>>>     feature_columns_file = os.path.join(model_path, \"feature_columns.txt\")\\n    with open(feature_columns_file, \"w\") as f:\\n        # Add a header comment to document the importance of ordering' is_hardcoded=False construction_method='os.path.join'",
        "path='feature_columns.txt' line_number=302 context='    \\n    # Save feature columns with ordering information\\n>>>     feature_columns_file = os.path.join(model_path, \"feature_columns.txt\")\\n    with open(feature_columns_file, \"w\") as f:\\n        # Add a header comment to document the importance of ordering' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=313 context='\\n    # Save hyperparameters configuration\\n>>>     hyperparameters_file = os.path.join(model_path, \"hyperparameters.json\")\\n    with open(hyperparameters_file, \"w\") as f:\\n        json.dump(config, f, indent=2, sort_keys=True)' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=313 context='\\n    # Save hyperparameters configuration\\n>>>     hyperparameters_file = os.path.join(model_path, \"hyperparameters.json\")\\n    with open(hyperparameters_file, \"w\") as f:\\n        json.dump(config, f, indent=2, sort_keys=True)' is_hardcoded=True construction_method=None",
        "path='metrics.json' line_number=356 context='        logger.info(f\"F1-Score (micro): {metrics[\\'f1_score_micro\\']}\")\\n        logger.info(f\"F1-Score (macro): {metrics[\\'f1_score_macro\\']}\")\\n>>>     with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\\n        json.dump(metrics, f, indent=2)\\n    # preds' is_hardcoded=False construction_method='os.path.join'",
        "path='metrics.json' line_number=356 context='        logger.info(f\"F1-Score (micro): {metrics[\\'f1_score_micro\\']}\")\\n        logger.info(f\"F1-Score (macro): {metrics[\\'f1_score_macro\\']}\")\\n>>>     with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\\n        json.dump(metrics, f, indent=2)\\n    # preds' is_hardcoded=True construction_method=None",
        "path='predictions.csv' line_number=361 context='    df = pd.DataFrame({id_col: ids, label_col: y_true})\\n    for i in range(y_prob.shape[1]): df[f\"prob_class_{i}\"] = y_prob[:,i]\\n>>>     df.to_csv(os.path.join(out_dir, \"predictions.csv\"), index=False)\\n\\n' is_hardcoded=False construction_method='os.path.join'",
        "path='predictions.csv' line_number=361 context='    df = pd.DataFrame({id_col: ids, label_col: y_true})\\n    for i in range(y_prob.shape[1]): df[f\"prob_class_{i}\"] = y_prob[:,i]\\n>>>     df.to_csv(os.path.join(out_dir, \"predictions.csv\"), index=False)\\n\\n' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=371 context='        auc = roc_auc_score(y_true, score)\\n        plt.figure()\\n>>>         plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\\n        plt.plot([0,1], [0,1], \"--\")\\n        plt.title(f\"{prefix} ROC\")' is_hardcoded=True construction_method=None",
        "path='roc.jpg' line_number=377 context='        plt.ylabel(\"TPR\")\\n        plt.legend()\\n>>>         plt.savefig(os.path.join(out_dir, f\"{prefix}roc.jpg\"))\\n        plt.close()\\n        precision, recall, _ = precision_recall_curve(y_true, score)' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=382 context='        ap = average_precision_score(y_true, score)\\n        plt.figure()\\n>>>         plt.plot(recall, precision, label=f\"AP={ap:.3f}\")\\n        plt.title(f\"{prefix} PR\")\\n        plt.xlabel(\"Recall\")' is_hardcoded=True construction_method=None",
        "path='pr.jpg' line_number=387 context='        plt.ylabel(\"Precision\")\\n        plt.legend()\\n>>>         plt.savefig(os.path.join(out_dir, f\"{prefix}pr.jpg\"))\\n        plt.close()\\n    else:' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=397 context='                auc = roc_auc_score(y_bin, y_prob[:,i])\\n                plt.figure()\\n>>>                 plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\\n                plt.plot([0,1], [0,1], \"--\")\\n                plt.title(f\"{prefix} class {i} ROC\")' is_hardcoded=True construction_method=None",
        "path='_roc.jpg' line_number=403 context='                plt.ylabel(\"TPR\")\\n                plt.legend()\\n>>>                 plt.savefig(os.path.join(out_dir, f\"{prefix}class_{i}_roc.jpg\"))\\n                plt.close()\\n                precision, recall, _ = precision_recall_curve(y_bin, y_prob[:,i])' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=408 context='                ap = average_precision_score(y_bin, y_prob[:,i])\\n                plt.figure()\\n>>>                 plt.plot(recall, precision, label=f\"AP={ap:.3f}\")\\n                plt.title(f\"{prefix} class {i} PR\")\\n                plt.xlabel(\"Recall\")' is_hardcoded=True construction_method=None",
        "path='_pr.jpg' line_number=413 context='                plt.ylabel(\"Precision\")\\n                plt.legend()\\n>>>                 plt.savefig(os.path.join(out_dir, f\"{prefix}class_{i}_pr.jpg\"))\\n                plt.close()\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/output/data' line_number=417 context='\\n\\n>>> def evaluate_split(name, df, feats, model, cfg, prefix=\"/opt/ml/output/data\"):\\n    is_bin = cfg.get(\"is_binary\", True)\\n    label = cfg[\"label_name\"]' is_hardcoded=True construction_method=None",
        "path='.tar.gz' line_number=441 context='    plot_curves(y_true,        y_prob, out_metrics, f\"{name}_\", is_bin)\\n\\n>>>     tar = os.path.join(prefix, f\"{name}.tar.gz\")\\n    with tarfile.open(tar, \"w:gz\") as t:\\n        t.add(out_base,    arcname=name)' is_hardcoded=True construction_method=None",
        "path='\\n    Main function to execute the XGBoost training logic.\\n    \\n    Args:\\n        input_paths: Dictionary of input paths with logical names\\n            - \"input_path\": Directory containing train/val/test data\\n            - \"hyperparameters_s3_uri\": Path to hyperparameters.json file\\n        output_paths: Dictionary of output paths with logical names\\n            - \"model_output\": Directory to save model artifacts\\n            - \"evaluation_output\": Directory to save evaluation outputs\\n        environ_vars: Dictionary of environment variables\\n        job_args: Command line arguments\\n    ' line_number=459 context='    job_args: argparse.Namespace\\n) -> None:\\n>>>     \"\"\"\\n    Main function to execute the XGBoost training logic.\\n    ' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=484 context='            hparam_path = input_paths[\"hyperparameters_s3_uri\"]\\n            # If it\\'s a directory path, append the filename\\n>>>             if not hparam_path.endswith(\"hyperparameters.json\"):\\n                hparam_path = os.path.join(hparam_path, \"hyperparameters.json\")\\n        else:' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=485 context='            # If it\\'s a directory path, append the filename\\n            if not hparam_path.endswith(\"hyperparameters.json\"):\\n>>>                 hparam_path = os.path.join(hparam_path, \"hyperparameters.json\")\\n        else:\\n            # Fallback to default location within input_path' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=485 context='            # If it\\'s a directory path, append the filename\\n            if not hparam_path.endswith(\"hyperparameters.json\"):\\n>>>                 hparam_path = os.path.join(hparam_path, \"hyperparameters.json\")\\n        else:\\n            # Fallback to default location within input_path' is_hardcoded=True construction_method=None",
        "path='config/hyperparameters.json' line_number=488 context='        else:\\n            # Fallback to default location within input_path\\n>>>             hparam_path = os.path.join(data_dir, \"config\", \"hyperparameters.json\")\\n        \\n        logger.info(\"Starting XGBoost training process...\")' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=488 context='        else:\\n            # Fallback to default location within input_path\\n>>>             hparam_path = os.path.join(data_dir, \"config\", \"hyperparameters.json\")\\n        \\n        logger.info(\"Starting XGBoost training process...\")' is_hardcoded=True construction_method=None",
        "path='Starting XGBoost training process...' line_number=490 context='            hparam_path = os.path.join(data_dir, \"config\", \"hyperparameters.json\")\\n        \\n>>>         logger.info(\"Starting XGBoost training process...\")\\n        logger.info(f\"Loading configuration from {hparam_path}\")\\n        config = load_and_validate_config(hparam_path)' is_hardcoded=True construction_method=None",
        "path='Loading datasets...' line_number=495 context='        logger.info(\"Configuration loaded successfully\")\\n        \\n>>>         logger.info(\"Loading datasets...\")\\n        train_df, val_df, test_df = load_datasets(data_dir)\\n        logger.info(\"Datasets loaded successfully\")' is_hardcoded=True construction_method=None",
        "path='Starting numerical imputation...' line_number=500 context='        \\n        # Apply numerical imputation\\n>>>         logger.info(\"Starting numerical imputation...\")\\n        train_df, val_df, test_df, impute_dict = apply_numerical_imputation(config, train_df, val_df, test_df)\\n        logger.info(\"Numerical imputation completed\")' is_hardcoded=True construction_method=None",
        "path='Starting risk table mapping...' line_number=505 context='    \\n        # Apply risk table mapping\\n>>>         logger.info(\"Starting risk table mapping...\")\\n        train_df, val_df, test_df, risk_tables = fit_and_apply_risk_tables(config, train_df, val_df, test_df)\\n        logger.info(\"Risk table mapping completed\")' is_hardcoded=True construction_method=None",
        "path='Preparing DMatrices for XGBoost...' line_number=509 context='        logger.info(\"Risk table mapping completed\")\\n        \\n>>>         logger.info(\"Preparing DMatrices for XGBoost...\")\\n        dtrain, dval, feature_columns = prepare_dmatrices(config, train_df, val_df)\\n        logger.info(\"DMatrices prepared successfully\")' is_hardcoded=True construction_method=None",
        "path='Starting model training...' line_number=514 context='        logger.info(f\"Using {len(feature_columns)} features in order: {feature_columns}\")\\n        \\n>>>         logger.info(\"Starting model training...\")\\n        model = train_model(config, dtrain, dval)\\n        logger.info(\"Model training completed\")' is_hardcoded=True construction_method=None",
        "path='Saving model artifacts...' line_number=518 context='        logger.info(\"Model training completed\")\\n    \\n>>>         logger.info(\"Saving model artifacts...\")\\n        logger.info(f\"Model path: {model_dir}, Output path: {output_dir}\")\\n        logger.info(f\"Output path exists: {os.path.exists(output_dir)}\")' is_hardcoded=True construction_method=None",
        "path=' does not exist, creating...' line_number=538 context='        logger.info(f\"Checking output directory: {output_dir}\")\\n        if not os.path.exists(output_dir):\\n>>>             logger.warning(f\"Output directory {output_dir} does not exist, creating...\")\\n            os.makedirs(output_dir, exist_ok=True)\\n        ' is_hardcoded=True construction_method=None",
        "path='All evaluation steps complete.' line_number=559 context='            logger.error(traceback.format_exc())\\n    \\n>>>         logger.info(\"All evaluation steps complete.\")\\n        logger.info(\"====== MAIN EXECUTION COMPLETED SUCCESSFULLY ======\")\\n        logger.info(\"Training script finished successfully.\")' is_hardcoded=True construction_method=None",
        "path='Training script finished successfully.' line_number=561 context='        logger.info(\"All evaluation steps complete.\")\\n        logger.info(\"====== MAIN EXECUTION COMPLETED SUCCESSFULLY ======\")\\n>>>         logger.info(\"Training script finished successfully.\")\\n    except Exception as e:\\n        logger.error(f\"FATAL ERROR in main execution: {str(e)}\")' is_hardcoded=True construction_method=None",
        "path='Script starting...' line_number=572 context='# -------------------------------------------------------------------------\\nif __name__ == \"__main__\":\\n>>>     logger.info(\"Script starting...\")\\n    \\n    # Container path constants' is_hardcoded=True construction_method=None",
        "path='/opt/ml/input/data' line_number=576 context='    # Container path constants\\n    CONTAINER_PATHS = {\\n>>>         \"INPUT_DATA\": \"/opt/ml/input/data\",\\n        \"MODEL_DIR\": \"/opt/ml/model\",\\n        \"OUTPUT_DATA\": \"/opt/ml/output/data\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/model' line_number=577 context='    CONTAINER_PATHS = {\\n        \"INPUT_DATA\": \"/opt/ml/input/data\",\\n>>>         \"MODEL_DIR\": \"/opt/ml/model\",\\n        \"OUTPUT_DATA\": \"/opt/ml/output/data\",\\n        \"CONFIG_DIR\": \"/opt/ml/input/data/config\"' is_hardcoded=True construction_method=None",
        "path='/opt/ml/output/data' line_number=578 context='        \"INPUT_DATA\": \"/opt/ml/input/data\",\\n        \"MODEL_DIR\": \"/opt/ml/model\",\\n>>>         \"OUTPUT_DATA\": \"/opt/ml/output/data\",\\n        \"CONFIG_DIR\": \"/opt/ml/input/data/config\"\\n    }' is_hardcoded=True construction_method=None",
        "path='/opt/ml/input/data/config' line_number=579 context='        \"MODEL_DIR\": \"/opt/ml/model\",\\n        \"OUTPUT_DATA\": \"/opt/ml/output/data\",\\n>>>         \"CONFIG_DIR\": \"/opt/ml/input/data/config\"\\n    }\\n    ' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [],
      "imports": [
        "module_name='os' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=3 is_from_import=False imported_items=[]",
        "module_name='argparse' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=5 is_from_import=False imported_items=[]",
        "module_name='logging' import_alias=None line_number=6 is_from_import=False imported_items=[]",
        "module_name='traceback' import_alias=None line_number=7 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=8 is_from_import=True imported_items=['Path']",
        "module_name='typing' import_alias=None line_number=9 is_from_import=True imported_items=['Dict', 'List', 'Any', 'Optional', 'Union', 'Tuple']",
        "module_name='pandas' import_alias='pd' line_number=11 is_from_import=False imported_items=[]",
        "module_name='numpy' import_alias='np' line_number=12 is_from_import=False imported_items=[]",
        "module_name='pickle' import_alias='pkl' line_number=13 is_from_import=False imported_items=[]",
        "module_name='xgboost' import_alias='xgb' line_number=14 is_from_import=False imported_items=[]",
        "module_name='tarfile' import_alias=None line_number=16 is_from_import=False imported_items=[]",
        "module_name='matplotlib.pyplot' import_alias='plt' line_number=17 is_from_import=False imported_items=[]",
        "module_name='sklearn.metrics' import_alias=None line_number=18 is_from_import=True imported_items=['roc_auc_score', 'average_precision_score', 'f1_score', 'roc_curve', 'precision_recall_curve']",
        "module_name='processing.risk_table_processor' import_alias=None line_number=30 is_from_import=True imported_items=['RiskTableMappingProcessor']",
        "module_name='processing.numerical_imputation_processor' import_alias=None line_number=31 is_from_import=True imported_items=['NumericalVariableImputationProcessor']",
        "module_name='pydantic' import_alias=None line_number=73 is_from_import=True imported_items=['BaseModel', 'Field', 'model_validator']",
        "module_name='hyperparams.hyperparameters_xgboost' import_alias=None line_number=74 is_from_import=True imported_items=['XGBoostModelHyperparameters']"
      ],
      "argument_definitions": [],
      "file_operations": [
        "file_path='<file_object>' operation_type='read' line_number=95 context='    try:\\n        with open(hparam_path, \"r\") as f:\\n>>>             config = json.load(f)\\n        \\n        required_keys = [\"tab_field_list\", \"cat_field_list\", \"label_name\", \"is_binary\", \"num_classes\"]' mode=None method='json.load'",
        "file_path='<file_object>' operation_type='write' line_number=286 context='    risk_map_file = os.path.join(model_path, \"risk_table_map.pkl\")\\n    with open(risk_map_file, \"wb\") as f:\\n>>>         pkl.dump(risk_tables, f)\\n    logger.info(f\"Saved consolidated risk table map to {risk_map_file}\")\\n    ' mode=None method='pickle.dump'",
        "file_path='<file_object>' operation_type='write' line_number=292 context='    impute_file = os.path.join(model_path, \"impute_dict.pkl\")\\n    with open(impute_file, \"wb\") as f:\\n>>>         pkl.dump(impute_dict, f)\\n    logger.info(f\"Saved imputation dictionary to {impute_file}\")\\n' mode=None method='pickle.dump'",
        "file_path='<file_object>' operation_type='write' line_number=298 context='    fmap_json = os.path.join(model_path, \"feature_importance.json\")\\n    with open(fmap_json, \"w\") as f:\\n>>>         json.dump(model.get_fscore(), f, indent=2)\\n    logger.info(f\"Saved feature importance to {fmap_json}\")\\n    ' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=315 context='    hyperparameters_file = os.path.join(model_path, \"hyperparameters.json\")\\n    with open(hyperparameters_file, \"w\") as f:\\n>>>         json.dump(config, f, indent=2, sort_keys=True)\\n    logger.info(f\"Saved hyperparameters configuration to {hyperparameters_file}\")\\n' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=357 context='        logger.info(f\"F1-Score (macro): {metrics[\\'f1_score_macro\\']}\")\\n    with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\\n>>>         json.dump(metrics, f, indent=2)\\n    # preds\\n    df = pd.DataFrame({id_col: ids, label_col: y_true})' mode=None method='json.dump'"
      ],
      "step_type": "Training",
      "framework": "xgboost",
      "step_type_patterns": {
        "error": "Training pattern detection failed: 'str' object has no attribute 'get'"
      }
    },
    "contract": {
      "entry_point": "xgboost_training.py",
      "inputs": {
        "input_path": {
          "path": "/opt/ml/input/data"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/input/data/config/hyperparameters.json"
        }
      },
      "outputs": {
        "model_output": {
          "path": "/opt/ml/model"
        },
        "evaluation_output": {
          "path": "/opt/ml/output/data"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    XGBoost training script for tabular data classification that:\n    1. Loads training, validation, and test datasets from split directories\n    2. Applies numerical imputation using mean strategy for missing values\n    3. Fits risk tables on categorical features using training data\n    4. Transforms all datasets using fitted preprocessing artifacts\n    5. Trains XGBoost model with configurable hyperparameters\n    6. Supports both binary and multiclass classification\n    7. Handles class weights for imbalanced datasets\n    8. Evaluates model performance with comprehensive metrics\n    9. Saves model artifacts and preprocessing components\n    10. Generates prediction files and performance visualizations\n    \n    Input Structure:\n    - /opt/ml/input/data: Root directory containing train/val/test subdirectories\n      - /opt/ml/input/data/train: Training data files (.csv, .parquet, .json)\n      - /opt/ml/input/data/val: Validation data files\n      - /opt/ml/input/data/test: Test data files\n    - /opt/ml/input/data/config/hyperparameters.json: Model configuration (optional)\n    \n    Output Structure:\n    - /opt/ml/model: Model artifacts directory\n      - /opt/ml/model/xgboost_model.bst: Trained XGBoost model\n      - /opt/ml/model/risk_table_map.pkl: Risk table mappings for categorical features\n      - /opt/ml/model/impute_dict.pkl: Imputation values for numerical features\n      - /opt/ml/model/feature_importance.json: Feature importance scores\n      - /opt/ml/model/feature_columns.txt: Ordered feature column names\n      - /opt/ml/model/hyperparameters.json: Model hyperparameters\n    - /opt/ml/output/data: Evaluation results directory\n      - /opt/ml/output/data/val.tar.gz: Validation predictions and metrics\n      - /opt/ml/output/data/test.tar.gz: Test predictions and metrics\n    \n    Contract aligned with step specification:\n    - Inputs: input_path (required), hyperparameters_s3_uri (optional)\n    - Outputs: model_output (primary), evaluation_output (secondary)\n    \n    Hyperparameters (via JSON config):\n    - Data fields: tab_field_list, cat_field_list, label_name, id_name\n    - Model: is_binary, num_classes, class_weights\n    - XGBoost: eta, gamma, max_depth, subsample, colsample_bytree, lambda_xgb, alpha_xgb\n    - Training: num_round, early_stopping_rounds\n    - Risk tables: smooth_factor, count_threshold\n    \n    Binary Classification:\n    - Uses binary:logistic objective\n    - Supports scale_pos_weight for class imbalance\n    - Generates ROC and PR curves\n    - Computes AUC-ROC, Average Precision, F1-Score\n    \n    Multiclass Classification:\n    - Uses multi:softprob objective\n    - Supports sample weights for class imbalance\n    - Generates per-class and aggregate metrics\n    - Computes micro/macro averaged metrics\n    \n    Risk Table Processing:\n    - Fits risk tables on categorical features using target correlation\n    - Applies smoothing and count thresholds for robust estimation\n    - Transforms categorical values to risk scores\n    \n    Numerical Imputation:\n    - Uses mean imputation strategy for missing numerical values\n    - Fits imputation on training data only\n    - Applies same imputation to validation and test sets\n    ",
      "framework_requirements": {
        "boto3": ">=1.26.0",
        "xgboost": "==1.7.6",
        "scikit-learn": ">=0.23.2,<1.0.0",
        "pandas": ">=1.2.0,<2.0.0",
        "pyarrow": ">=4.0.0,<6.0.0",
        "beautifulsoup4": ">=4.9.3",
        "flask": ">=2.0.0,<3.0.0",
        "pydantic": ">=2.0.0,<3.0.0",
        "typing-extensions": ">=4.2.0",
        "matplotlib": ">=3.0.0",
        "numpy": ">=1.19.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "step_type_resolution",
        "message": "Step type resolved via registry: XGBoostTraining -> XGBoostTraining -> Training",
        "details": {
          "contract": "xgboost_training_contract",
          "original_spec_type": "XGBoostTraining",
          "canonical_name": "XGBoostTraining",
          "resolved_sagemaker_type": "Training",
          "registry_available": true
        },
        "recommendation": "Using Training step property paths for validation"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output model_output: properties.ModelArtifacts.S3ModelArtifacts",
        "details": {
          "contract": "xgboost_training_contract",
          "logical_name": "model_output",
          "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
          "step_type": "training",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output evaluation_output: properties.OutputDataConfig.S3OutputPath",
        "details": {
          "contract": "xgboost_training_contract",
          "logical_name": "evaluation_output",
          "property_path": "properties.OutputDataConfig.S3OutputPath",
          "step_type": "training",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation_summary",
        "message": "Property path validation completed for xgboost_training_contract",
        "details": {
          "contract": "xgboost_training_contract",
          "step_type": "training",
          "node_type": "internal",
          "total_outputs": 2,
          "outputs_with_property_paths": 2,
          "validation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference",
          "documentation_version": "v2.92.2"
        },
        "recommendation": "Validated 2/2 outputs with property paths against SageMaker documentation"
      }
    ],
    "contract": {
      "entry_point": "xgboost_training.py",
      "inputs": {
        "input_path": {
          "path": "/opt/ml/input/data"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/input/data/config/hyperparameters.json"
        }
      },
      "outputs": {
        "model_output": {
          "path": "/opt/ml/model"
        },
        "evaluation_output": {
          "path": "/opt/ml/output/data"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    XGBoost training script for tabular data classification that:\n    1. Loads training, validation, and test datasets from split directories\n    2. Applies numerical imputation using mean strategy for missing values\n    3. Fits risk tables on categorical features using training data\n    4. Transforms all datasets using fitted preprocessing artifacts\n    5. Trains XGBoost model with configurable hyperparameters\n    6. Supports both binary and multiclass classification\n    7. Handles class weights for imbalanced datasets\n    8. Evaluates model performance with comprehensive metrics\n    9. Saves model artifacts and preprocessing components\n    10. Generates prediction files and performance visualizations\n    \n    Input Structure:\n    - /opt/ml/input/data: Root directory containing train/val/test subdirectories\n      - /opt/ml/input/data/train: Training data files (.csv, .parquet, .json)\n      - /opt/ml/input/data/val: Validation data files\n      - /opt/ml/input/data/test: Test data files\n    - /opt/ml/input/data/config/hyperparameters.json: Model configuration (optional)\n    \n    Output Structure:\n    - /opt/ml/model: Model artifacts directory\n      - /opt/ml/model/xgboost_model.bst: Trained XGBoost model\n      - /opt/ml/model/risk_table_map.pkl: Risk table mappings for categorical features\n      - /opt/ml/model/impute_dict.pkl: Imputation values for numerical features\n      - /opt/ml/model/feature_importance.json: Feature importance scores\n      - /opt/ml/model/feature_columns.txt: Ordered feature column names\n      - /opt/ml/model/hyperparameters.json: Model hyperparameters\n    - /opt/ml/output/data: Evaluation results directory\n      - /opt/ml/output/data/val.tar.gz: Validation predictions and metrics\n      - /opt/ml/output/data/test.tar.gz: Test predictions and metrics\n    \n    Contract aligned with step specification:\n    - Inputs: input_path (required), hyperparameters_s3_uri (optional)\n    - Outputs: model_output (primary), evaluation_output (secondary)\n    \n    Hyperparameters (via JSON config):\n    - Data fields: tab_field_list, cat_field_list, label_name, id_name\n    - Model: is_binary, num_classes, class_weights\n    - XGBoost: eta, gamma, max_depth, subsample, colsample_bytree, lambda_xgb, alpha_xgb\n    - Training: num_round, early_stopping_rounds\n    - Risk tables: smooth_factor, count_threshold\n    \n    Binary Classification:\n    - Uses binary:logistic objective\n    - Supports scale_pos_weight for class imbalance\n    - Generates ROC and PR curves\n    - Computes AUC-ROC, Average Precision, F1-Score\n    \n    Multiclass Classification:\n    - Uses multi:softprob objective\n    - Supports sample weights for class imbalance\n    - Generates per-class and aggregate metrics\n    - Computes micro/macro averaged metrics\n    \n    Risk Table Processing:\n    - Fits risk tables on categorical features using target correlation\n    - Applies smoothing and count thresholds for robust estimation\n    - Transforms categorical values to risk scores\n    \n    Numerical Imputation:\n    - Uses mean imputation strategy for missing numerical values\n    - Fits imputation on training data only\n    - Applies same imputation to validation and test sets\n    ",
      "framework_requirements": {
        "boto3": ">=1.26.0",
        "xgboost": "==1.7.6",
        "scikit-learn": ">=0.23.2,<1.0.0",
        "pandas": ">=1.2.0,<2.0.0",
        "pyarrow": ">=4.0.0,<6.0.0",
        "beautifulsoup4": ">=4.9.3",
        "flask": ">=2.0.0,<3.0.0",
        "pydantic": ">=2.0.0,<3.0.0",
        "typing-extensions": ">=4.2.0",
        "matplotlib": ">=3.0.0",
        "numpy": ">=1.19.0"
      }
    },
    "specifications": {
      "xgboost_training_spec": {
        "step_type": "XGBoostTraining",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "input_path",
            "dependency_type": "training_data",
            "required": true,
            "compatible_sources": [
              "RiskTableMapping",
              "ProcessingStep",
              "DataLoad",
              "TabularPreprocessing"
            ],
            "data_type": "S3Uri",
            "description": "Training dataset S3 location with train/val/test subdirectories"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "ProcessingStep",
              "HyperparameterPrep"
            ],
            "data_type": "S3Uri",
            "description": "Hyperparameters configuration file (optional, can be generated internally)"
          }
        ],
        "outputs": [
          {
            "logical_name": "model_output",
            "output_type": "model_artifacts",
            "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
            "data_type": "S3Uri",
            "description": "Trained XGBoost model artifacts"
          },
          {
            "logical_name": "evaluation_output",
            "output_type": "processing_output",
            "property_path": "properties.OutputDataConfig.S3OutputPath",
            "data_type": "S3Uri",
            "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "XGBoostTraining",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "input_path",
            "dependency_type": "training_data",
            "required": true,
            "compatible_sources": [
              "RiskTableMapping",
              "ProcessingStep",
              "DataLoad",
              "TabularPreprocessing"
            ],
            "data_type": "S3Uri",
            "description": "Training dataset S3 location with train/val/test subdirectories"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "ProcessingStep",
              "HyperparameterPrep"
            ],
            "data_type": "S3Uri",
            "description": "Hyperparameters configuration file (optional, can be generated internally)"
          }
        ],
        "outputs": [
          {
            "logical_name": "model_output",
            "output_type": "model_artifacts",
            "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
            "data_type": "S3Uri",
            "description": "Trained XGBoost model artifacts"
          },
          {
            "logical_name": "evaluation_output",
            "output_type": "processing_output",
            "property_path": "properties.OutputDataConfig.S3OutputPath",
            "data_type": "S3Uri",
            "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
          }
        ]
      },
      "variants": {
        "training": {
          "step_type": "XGBoostTraining",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "input_path",
              "dependency_type": "training_data",
              "required": true,
              "compatible_sources": [
                "RiskTableMapping",
                "ProcessingStep",
                "DataLoad",
                "TabularPreprocessing"
              ],
              "data_type": "S3Uri",
              "description": "Training dataset S3 location with train/val/test subdirectories"
            },
            {
              "logical_name": "hyperparameters_s3_uri",
              "dependency_type": "hyperparameters",
              "required": false,
              "compatible_sources": [
                "ProcessingStep",
                "HyperparameterPrep"
              ],
              "data_type": "S3Uri",
              "description": "Hyperparameters configuration file (optional, can be generated internally)"
            }
          ],
          "outputs": [
            {
              "logical_name": "model_output",
              "output_type": "model_artifacts",
              "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
              "data_type": "S3Uri",
              "description": "Trained XGBoost model artifacts"
            },
            {
              "logical_name": "evaluation_output",
              "output_type": "processing_output",
              "property_path": "properties.OutputDataConfig.S3OutputPath",
              "data_type": "S3Uri",
              "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
            }
          ]
        }
      },
      "unified_dependencies": {
        "input_path": {
          "logical_name": "input_path",
          "dependency_type": "training_data",
          "required": true,
          "compatible_sources": [
            "RiskTableMapping",
            "ProcessingStep",
            "DataLoad",
            "TabularPreprocessing"
          ],
          "data_type": "S3Uri",
          "description": "Training dataset S3 location with train/val/test subdirectories"
        },
        "hyperparameters_s3_uri": {
          "logical_name": "hyperparameters_s3_uri",
          "dependency_type": "hyperparameters",
          "required": false,
          "compatible_sources": [
            "ProcessingStep",
            "HyperparameterPrep"
          ],
          "data_type": "S3Uri",
          "description": "Hyperparameters configuration file (optional, can be generated internally)"
        }
      },
      "unified_outputs": {
        "model_output": {
          "logical_name": "model_output",
          "output_type": "model_artifacts",
          "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
          "data_type": "S3Uri",
          "description": "Trained XGBoost model artifacts"
        },
        "evaluation_output": {
          "logical_name": "evaluation_output",
          "output_type": "processing_output",
          "property_path": "properties.OutputDataConfig.S3OutputPath",
          "data_type": "S3Uri",
          "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
        }
      },
      "dependency_sources": {
        "input_path": [
          "training"
        ],
        "hyperparameters_s3_uri": [
          "training"
        ]
      },
      "output_sources": {
        "model_output": [
          "training"
        ],
        "evaluation_output": [
          "training"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": false,
    "issues": [
      {
        "severity": "ERROR",
        "category": "dependency_compatibility",
        "message": "Dependency input_path has low compatibility score: 0.450",
        "details": {
          "logical_name": "input_path",
          "specification": "xgboost_training",
          "best_match": {
            "provider": "TabularPreprocessing",
            "output": "processed_data",
            "score": 0.45
          },
          "required": true,
          "threshold_info": {
            "mode": "relaxed",
            "thresholds": {
              "pass": "\u2265 0.6",
              "warning": "0.4 - 0.59",
              "error": "0.2 - 0.39",
              "critical": "< 0.2"
            },
            "resolution_threshold": 0.5,
            "description": "Relaxed validation allowing reasonable compatibility matches"
          },
          "score_breakdown": {
            "type_compatibility": 0.2,
            "data_type_compatibility": 0.2,
            "semantic_similarity": 0.05,
            "exact_match_bonus": 0.0,
            "source_compatibility": 0.0,
            "keyword_matching": 0.0
          },
          "all_candidates": [
            {
              "provider": "TabularPreprocessing",
              "output": "processed_data",
              "score": 0.45
            },
            {
              "provider": "RiskTableMapping",
              "output": "processed_data",
              "score": 0.45
            },
            {
              "provider": "CradleDataLoading",
              "output": "DATA",
              "score": 0.44642857142857145
            }
          ]
        },
        "recommendation": "Consider renaming 'input_path' or adding aliases to improve semantic matching; Add 'TabularPreprocessing' to compatible_sources for input_path"
      },
      {
        "severity": "WARNING",
        "category": "dependency_compatibility",
        "message": "Dependency hyperparameters_s3_uri has low compatibility score: 0.420",
        "details": {
          "logical_name": "hyperparameters_s3_uri",
          "specification": "xgboost_training",
          "best_match": {
            "provider": "BatchTransform",
            "output": "transform_output",
            "score": 0.4197368421052632
          },
          "required": false,
          "threshold_info": {
            "mode": "relaxed",
            "thresholds": {
              "pass": "\u2265 0.6",
              "warning": "0.4 - 0.59",
              "error": "0.2 - 0.39",
              "critical": "< 0.2"
            },
            "resolution_threshold": 0.5,
            "description": "Relaxed validation allowing reasonable compatibility matches"
          },
          "score_breakdown": {
            "type_compatibility": 0.2,
            "data_type_compatibility": 0.2,
            "semantic_similarity": 0.019736842105263157,
            "exact_match_bonus": 0.0,
            "source_compatibility": 0.0,
            "keyword_matching": 0.0
          },
          "all_candidates": [
            {
              "provider": "BatchTransform",
              "output": "transform_output",
              "score": 0.4197368421052632
            },
            {
              "provider": "PyTorchModel",
              "output": "model_name",
              "score": 0.31875000000000003
            },
            {
              "provider": "XGBoostModel",
              "output": "model_name",
              "score": 0.31875000000000003
            }
          ]
        },
        "recommendation": "Consider renaming 'hyperparameters_s3_uri' or adding aliases to improve semantic matching; Add 'BatchTransform' to compatible_sources for hyperparameters_s3_uri"
      }
    ],
    "specification": {
      "step_type": "XGBoostTraining",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "input_path",
          "dependency_type": "training_data",
          "required": true,
          "compatible_sources": [
            "RiskTableMapping",
            "ProcessingStep",
            "DataLoad",
            "TabularPreprocessing"
          ],
          "data_type": "S3Uri",
          "description": "Training dataset S3 location with train/val/test subdirectories"
        },
        {
          "logical_name": "hyperparameters_s3_uri",
          "dependency_type": "hyperparameters",
          "required": false,
          "compatible_sources": [
            "ProcessingStep",
            "HyperparameterPrep"
          ],
          "data_type": "S3Uri",
          "description": "Hyperparameters configuration file (optional, can be generated internally)"
        }
      ],
      "outputs": [
        {
          "logical_name": "model_output",
          "output_type": "model_artifacts",
          "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
          "data_type": "S3Uri",
          "description": "Trained XGBoost model artifacts"
        },
        {
          "logical_name": "evaluation_output",
          "output_type": "processing_output",
          "property_path": "properties.OutputDataConfig.S3OutputPath",
          "data_type": "S3Uri",
          "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
        }
      ]
    }
  },
  "level4": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "config_import",
        "message": "Builder may not be properly importing configuration class xgboost_trainingConfig",
        "details": {
          "config_class": "xgboost_trainingConfig",
          "builder": "xgboost_training"
        },
        "recommendation": "Ensure builder imports and uses xgboost_trainingConfig"
      }
    ],
    "builder_analysis": {
      "config_accesses": [],
      "validation_calls": [],
      "default_assignments": [],
      "class_definitions": [
        {
          "class_name": "XGBoostTrainingStepBuilder",
          "line_number": 35,
          "base_classes": [
            "StepBuilderBase"
          ],
          "decorators": [
            "Call"
          ]
        }
      ],
      "method_definitions": [
        {
          "method_name": "__init__",
          "line_number": 42,
          "args": [
            "self",
            "config",
            "sagemaker_session",
            "role",
            "notebook_root",
            "registry_manager",
            "dependency_resolver"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "validate_configuration",
          "line_number": 88,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_create_estimator",
          "line_number": 117,
          "args": [
            "self",
            "output_path"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_environment_variables",
          "line_number": 148,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_create_data_channels_from_source",
          "line_number": 167,
          "args": [
            "self",
            "base_path"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_inputs",
          "line_number": 189,
          "args": [
            "self",
            "inputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_outputs",
          "line_number": 306,
          "args": [
            "self",
            "outputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_normalize_s3_uri",
          "line_number": 365,
          "args": [
            "self",
            "uri",
            "description"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_s3_directory_path",
          "line_number": 388,
          "args": [
            "self",
            "uri",
            "filename"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_prepare_hyperparameters_file",
          "line_number": 412,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_validate_s3_uri",
          "line_number": 492,
          "args": [
            "self",
            "uri",
            "description"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "create_step",
          "line_number": 526,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        }
      ],
      "import_statements": [
        {
          "type": "from_import",
          "module": "typing",
          "name": "Dict",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Optional",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Any",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "List",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "pathlib",
          "name": "Path",
          "alias": null,
          "line_number": 2
        },
        {
          "type": "import",
          "module": "logging",
          "alias": null,
          "line_number": 3
        },
        {
          "type": "import",
          "module": "tempfile",
          "alias": null,
          "line_number": 4
        },
        {
          "type": "import",
          "module": "json",
          "alias": null,
          "line_number": 5
        },
        {
          "type": "import",
          "module": "shutil",
          "alias": null,
          "line_number": 6
        },
        {
          "type": "import",
          "module": "boto3",
          "alias": null,
          "line_number": 7
        },
        {
          "type": "from_import",
          "module": "botocore.exceptions",
          "name": "ClientError",
          "alias": null,
          "line_number": 8
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "TrainingStep",
          "alias": null,
          "line_number": 10
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "Step",
          "alias": null,
          "line_number": 10
        },
        {
          "type": "from_import",
          "module": "sagemaker.inputs",
          "name": "TrainingInput",
          "alias": null,
          "line_number": 11
        },
        {
          "type": "from_import",
          "module": "sagemaker.xgboost",
          "name": "XGBoost",
          "alias": null,
          "line_number": 12
        },
        {
          "type": "from_import",
          "module": "sagemaker.s3",
          "name": "S3Uploader",
          "alias": null,
          "line_number": 13
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.functions",
          "name": "Join",
          "alias": null,
          "line_number": 14
        },
        {
          "type": "from_import",
          "module": "configs.config_xgboost_training_step",
          "name": "XGBoostTrainingConfig",
          "alias": null,
          "line_number": 16
        },
        {
          "type": "from_import",
          "module": "core.base.builder_base",
          "name": "StepBuilderBase",
          "alias": null,
          "line_number": 17
        },
        {
          "type": "from_import",
          "module": "s3_utils",
          "name": "S3PathHandler",
          "alias": null,
          "line_number": 18
        },
        {
          "type": "from_import",
          "module": "core.deps.registry_manager",
          "name": "RegistryManager",
          "alias": null,
          "line_number": 19
        },
        {
          "type": "from_import",
          "module": "core.deps.dependency_resolver",
          "name": "UnifiedDependencyResolver",
          "alias": null,
          "line_number": 20
        },
        {
          "type": "from_import",
          "module": "registry.builder_registry",
          "name": "register_builder",
          "alias": null,
          "line_number": 21
        },
        {
          "type": "from_import",
          "module": "specs.xgboost_training_spec",
          "name": "XGBOOST_TRAINING_SPEC",
          "alias": null,
          "line_number": 25
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.functions",
          "name": "Join",
          "alias": null,
          "line_number": 177
        }
      ],
      "config_class_usage": []
    },
    "config_analysis": {
      "class_name": "xgboost_trainingConfig",
      "fields": {},
      "required_fields": [],
      "optional_fields": [],
      "default_values": {},
      "load_error": "Configuration class not found in /Users/tianpeixie/github_workspace/cursus/src/cursus/steps/configs/config_xgboost_training_step.py. Available classes: ['Any', 'BaseModel', 'BasePipelineConfig', 'Path', 'XGBoostModelHyperparameters', 'XGBoostTrainingConfig', 'datetime']"
    }
  },
  "overall_status": "FAILING",
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_training.py",
    "contract_mapping": "xgboost_training_contract",
    "validation_timestamp": "2025-08-14T09:10:04.999090",
    "validator_version": "1.0.0"
  }
}