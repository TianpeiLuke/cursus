{
  "script_name": "xgboost_training",
  "level1": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "path_usage",
        "message": "Script correctly uses parent directory to construct file path: /opt/ml/code/hyperparams \u2192 /opt/ml/code/hyperparams/hyperparameters.json",
        "details": {
          "script_path": "/opt/ml/code/hyperparams",
          "contract_path": "/opt/ml/code/hyperparams/hyperparameters.json",
          "construction_method": "os.path.join",
          "script": "xgboost_training"
        },
        "recommendation": "Path usage pattern is correct - no action needed"
      },
      {
        "severity": "INFO",
        "category": "testability_compliance",
        "message": "Main function follows testability pattern with all required parameters",
        "details": {
          "script": "xgboost_training",
          "testability_parameters": [
            "job_args",
            "environ_vars",
            "output_paths",
            "input_paths"
          ]
        },
        "recommendation": "No action needed - script follows testability best practices"
      },
      {
        "severity": "WARNING",
        "category": "testability_entry_point",
        "message": "Main function expects environ_vars parameter but no environment collection found in entry point",
        "details": {
          "script": "xgboost_training"
        },
        "recommendation": "Add environment variable collection in __main__ block to pass to main function"
      },
      {
        "severity": "WARNING",
        "category": "testability_parameter_usage",
        "message": "Testability parameters defined but not used: job_args, environ_vars",
        "details": {
          "script": "xgboost_training",
          "unused_parameters": [
            "job_args",
            "environ_vars"
          ],
          "used_parameters": [
            "output_paths",
            "input_paths"
          ]
        },
        "recommendation": "Either use the testability parameters or remove them from function signature"
      },
      {
        "severity": "INFO",
        "category": "testability_container_support",
        "message": "No container detection found - consider adding hybrid mode support",
        "details": {
          "script": "xgboost_training"
        },
        "recommendation": "Add container detection to support both local and container execution"
      },
      {
        "severity": "WARNING",
        "category": "step_type_enhancement_error",
        "message": "Failed to apply step type enhancements: 'str' object has no attribute 'get'",
        "details": {
          "script": "xgboost_training",
          "error": "'str' object has no attribute 'get'"
        },
        "recommendation": "Check step type detection and framework patterns"
      }
    ],
    "script_analysis": {
      "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_training.py",
      "path_references": [
        "path='\\n    Load everything from your pipeline\u2019s XGBoostModelHyperparameters,\\n    plus the two risk-table params this script needs.\\n    ' line_number=81 context='\\nclass XGBoostConfig(XGBoostModelHyperparameters):\\n>>>     \"\"\"\\n    Load everything from your pipeline\u2019s XGBoostModelHyperparameters,\\n    plus the two risk-table params this script needs.' is_hardcoded=True construction_method=None",
        "path='Loads and validates the hyperparameters JSON file.' line_number=98 context='# -------------------------------------------------------------------------\\ndef load_and_validate_config(hparam_path: str) -> dict:\\n>>>     \"\"\"Loads and validates the hyperparameters JSON file.\"\"\"\\n    try:\\n        with open(hparam_path, \"r\") as f:' is_hardcoded=True construction_method=None",
        "path='Finds the first supported data file in a directory.' line_number=134 context='\\ndef find_first_data_file(data_dir: str) -> str:\\n>>>     \"\"\"Finds the first supported data file in a directory.\"\"\"\\n    if not os.path.isdir(data_dir):\\n        return None' is_hardcoded=True construction_method=None",
        "path='.csv' line_number=138 context='        return None\\n    for fname in sorted(os.listdir(data_dir)):\\n>>>         if fname.lower().endswith((\".csv\", \".parquet\", \".json\")):\\n            return os.path.join(data_dir, fname)\\n    return None' is_hardcoded=True construction_method=None",
        "path='.json' line_number=138 context='        return None\\n    for fname in sorted(os.listdir(data_dir)):\\n>>>         if fname.lower().endswith((\".csv\", \".parquet\", \".json\")):\\n            return os.path.join(data_dir, fname)\\n    return None' is_hardcoded=True construction_method=None",
        "path='Loads the training, validation, and test datasets.' line_number=144 context='\\ndef load_datasets(input_path: str) -> tuple:\\n>>>     \"\"\"Loads the training, validation, and test datasets.\"\"\"\\n    train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n    val_file = find_first_data_file(os.path.join(input_path, \"val\"))' is_hardcoded=True construction_method=None",
        "path='train' line_number=145 context='def load_datasets(input_path: str) -> tuple:\\n    \"\"\"Loads the training, validation, and test datasets.\"\"\"\\n>>>     train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n    val_file = find_first_data_file(os.path.join(input_path, \"val\"))\\n    test_file = find_first_data_file(os.path.join(input_path, \"test\"))' is_hardcoded=False construction_method='os.path.join'",
        "path='val' line_number=146 context='    \"\"\"Loads the training, validation, and test datasets.\"\"\"\\n    train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n>>>     val_file = find_first_data_file(os.path.join(input_path, \"val\"))\\n    test_file = find_first_data_file(os.path.join(input_path, \"test\"))\\n' is_hardcoded=False construction_method='os.path.join'",
        "path='test' line_number=147 context='    train_file = find_first_data_file(os.path.join(input_path, \"train\"))\\n    val_file = find_first_data_file(os.path.join(input_path, \"val\"))\\n>>>     test_file = find_first_data_file(os.path.join(input_path, \"test\"))\\n\\n    if not train_file or not val_file or not test_file:' is_hardcoded=False construction_method='os.path.join'",
        "path='Training, validation, or test data file not found in the expected subfolders.' line_number=151 context='    if not train_file or not val_file or not test_file:\\n        raise FileNotFoundError(\\n>>>             \"Training, validation, or test data file not found in the expected subfolders.\"\\n        )\\n' is_hardcoded=True construction_method=None",
        "path='Applies numerical imputation to the datasets.' line_number=179 context='    config: dict, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame\\n) -> tuple:\\n>>>     \"\"\"Applies numerical imputation to the datasets.\"\"\"\\n    imputer = NumericalVariableImputationProcessor(\\n        variables=config[\"tab_field_list\"], strategy=\"mean\"' is_hardcoded=True construction_method=None",
        "path='Fits risk tables on training data and applies them to all splits.' line_number=200 context='    config: dict, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame\\n) -> tuple:\\n>>>     \"\"\"Fits risk tables on training data and applies them to all splits.\"\"\"\\n    risk_processors = {}\\n    train_df_transformed = train_df.copy()' is_hardcoded=True construction_method=None",
        "path='xgboost_model.bst' line_number=359 context='\\n    # Save XGBoost model\\n>>>     model_file = os.path.join(model_path, \"xgboost_model.bst\")\\n    model.save_model(model_file)\\n    logger.info(f\"Saved XGBoost model to {model_file}\")' is_hardcoded=False construction_method='os.path.join'",
        "path='xgboost_model.bst' line_number=359 context='\\n    # Save XGBoost model\\n>>>     model_file = os.path.join(model_path, \"xgboost_model.bst\")\\n    model.save_model(model_file)\\n    logger.info(f\"Saved XGBoost model to {model_file}\")' is_hardcoded=True construction_method=None",
        "path='risk_table_map.pkl' line_number=364 context='\\n    # Save risk tables\\n>>>     risk_map_file = os.path.join(model_path, \"risk_table_map.pkl\")\\n    with open(risk_map_file, \"wb\") as f:\\n        pkl.dump(risk_tables, f)' is_hardcoded=False construction_method='os.path.join'",
        "path='risk_table_map.pkl' line_number=364 context='\\n    # Save risk tables\\n>>>     risk_map_file = os.path.join(model_path, \"risk_table_map.pkl\")\\n    with open(risk_map_file, \"wb\") as f:\\n        pkl.dump(risk_tables, f)' is_hardcoded=True construction_method=None",
        "path='impute_dict.pkl' line_number=370 context='\\n    # Save imputation dictionary\\n>>>     impute_file = os.path.join(model_path, \"impute_dict.pkl\")\\n    with open(impute_file, \"wb\") as f:\\n        pkl.dump(impute_dict, f)' is_hardcoded=False construction_method='os.path.join'",
        "path='impute_dict.pkl' line_number=370 context='\\n    # Save imputation dictionary\\n>>>     impute_file = os.path.join(model_path, \"impute_dict.pkl\")\\n    with open(impute_file, \"wb\") as f:\\n        pkl.dump(impute_dict, f)' is_hardcoded=True construction_method=None",
        "path='feature_importance.json' line_number=376 context='\\n    # Save feature importance\\n>>>     fmap_json = os.path.join(model_path, \"feature_importance.json\")\\n    with open(fmap_json, \"w\") as f:\\n        json.dump(model.get_fscore(), f, indent=2)' is_hardcoded=False construction_method='os.path.join'",
        "path='feature_importance.json' line_number=376 context='\\n    # Save feature importance\\n>>>     fmap_json = os.path.join(model_path, \"feature_importance.json\")\\n    with open(fmap_json, \"w\") as f:\\n        json.dump(model.get_fscore(), f, indent=2)' is_hardcoded=True construction_method=None",
        "path='feature_columns.txt' line_number=382 context='\\n    # Save feature columns with ordering information\\n>>>     feature_columns_file = os.path.join(model_path, \"feature_columns.txt\")\\n    with open(feature_columns_file, \"w\") as f:\\n        # Add a header comment to document the importance of ordering' is_hardcoded=False construction_method='os.path.join'",
        "path='feature_columns.txt' line_number=382 context='\\n    # Save feature columns with ordering information\\n>>>     feature_columns_file = os.path.join(model_path, \"feature_columns.txt\")\\n    with open(feature_columns_file, \"w\") as f:\\n        # Add a header comment to document the importance of ordering' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=395 context='\\n    # Save hyperparameters configuration\\n>>>     hyperparameters_file = os.path.join(model_path, \"hyperparameters.json\")\\n    with open(hyperparameters_file, \"w\") as f:\\n        json.dump(config, f, indent=2, sort_keys=True)' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=395 context='\\n    # Save hyperparameters configuration\\n>>>     hyperparameters_file = os.path.join(model_path, \"hyperparameters.json\")\\n    with open(hyperparameters_file, \"w\") as f:\\n        json.dump(config, f, indent=2, sort_keys=True)' is_hardcoded=True construction_method=None",
        "path='metrics.json' line_number=448 context='        logger.info(f\"F1-Score (micro): {metrics[\\'f1_score_micro\\']}\")\\n        logger.info(f\"F1-Score (macro): {metrics[\\'f1_score_macro\\']}\")\\n>>>     with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\\n        json.dump(metrics, f, indent=2)\\n    # preds' is_hardcoded=False construction_method='os.path.join'",
        "path='metrics.json' line_number=448 context='        logger.info(f\"F1-Score (micro): {metrics[\\'f1_score_micro\\']}\")\\n        logger.info(f\"F1-Score (macro): {metrics[\\'f1_score_macro\\']}\")\\n>>>     with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\\n        json.dump(metrics, f, indent=2)\\n    # preds' is_hardcoded=True construction_method=None",
        "path='predictions.csv' line_number=454 context='    for i in range(y_prob.shape[1]):\\n        df[f\"prob_class_{i}\"] = y_prob[:, i]\\n>>>     df.to_csv(os.path.join(out_dir, \"predictions.csv\"), index=False)\\n\\n' is_hardcoded=False construction_method='os.path.join'",
        "path='predictions.csv' line_number=454 context='    for i in range(y_prob.shape[1]):\\n        df[f\"prob_class_{i}\"] = y_prob[:, i]\\n>>>     df.to_csv(os.path.join(out_dir, \"predictions.csv\"), index=False)\\n\\n' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=464 context='        auc = roc_auc_score(y_true, score)\\n        plt.figure()\\n>>>         plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\\n        plt.plot([0, 1], [0, 1], \"--\")\\n        plt.title(f\"{prefix} ROC\")' is_hardcoded=True construction_method=None",
        "path='roc.jpg' line_number=470 context='        plt.ylabel(\"TPR\")\\n        plt.legend()\\n>>>         plt.savefig(os.path.join(out_dir, f\"{prefix}roc.jpg\"))\\n        plt.close()\\n        precision, recall, _ = precision_recall_curve(y_true, score)' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=475 context='        ap = average_precision_score(y_true, score)\\n        plt.figure()\\n>>>         plt.plot(recall, precision, label=f\"AP={ap:.3f}\")\\n        plt.title(f\"{prefix} PR\")\\n        plt.xlabel(\"Recall\")' is_hardcoded=True construction_method=None",
        "path='pr.jpg' line_number=480 context='        plt.ylabel(\"Precision\")\\n        plt.legend()\\n>>>         plt.savefig(os.path.join(out_dir, f\"{prefix}pr.jpg\"))\\n        plt.close()\\n    else:' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=490 context='                auc = roc_auc_score(y_bin, y_prob[:, i])\\n                plt.figure()\\n>>>                 plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\\n                plt.plot([0, 1], [0, 1], \"--\")\\n                plt.title(f\"{prefix} class {i} ROC\")' is_hardcoded=True construction_method=None",
        "path='_roc.jpg' line_number=496 context='                plt.ylabel(\"TPR\")\\n                plt.legend()\\n>>>                 plt.savefig(os.path.join(out_dir, f\"{prefix}class_{i}_roc.jpg\"))\\n                plt.close()\\n                precision, recall, _ = precision_recall_curve(y_bin, y_prob[:, i])' is_hardcoded=True construction_method=None",
        "path='.3f' line_number=501 context='                ap = average_precision_score(y_bin, y_prob[:, i])\\n                plt.figure()\\n>>>                 plt.plot(recall, precision, label=f\"AP={ap:.3f}\")\\n                plt.title(f\"{prefix} class {i} PR\")\\n                plt.xlabel(\"Recall\")' is_hardcoded=True construction_method=None",
        "path='_pr.jpg' line_number=506 context='                plt.ylabel(\"Precision\")\\n                plt.legend()\\n>>>                 plt.savefig(os.path.join(out_dir, f\"{prefix}class_{i}_pr.jpg\"))\\n                plt.close()\\n' is_hardcoded=True construction_method=None",
        "path='/opt/ml/output/data' line_number=510 context='\\n\\n>>> def evaluate_split(name, df, feats, model, cfg, prefix=\"/opt/ml/output/data\") -> None:\\n    is_bin = cfg.get(\"is_binary\", True)\\n    label = cfg[\"label_name\"]' is_hardcoded=True construction_method=None",
        "path='.tar.gz' line_number=534 context='    plot_curves(y_true, y_prob, out_metrics, f\"{name}_\", is_bin)\\n\\n>>>     tar = os.path.join(prefix, f\"{name}.tar.gz\")\\n    with tarfile.open(tar, \"w:gz\") as t:\\n        t.add(out_base, arcname=name)' is_hardcoded=True construction_method=None",
        "path='\\n    Main function to execute the XGBoost training logic.\\n\\n    Args:\\n        input_paths: Dictionary of input paths with logical names\\n            - \"input_path\": Directory containing train/val/test data\\n            - \"hyperparameters_s3_uri\": Path to hyperparameters directory (now points to /opt/ml/code/hyperparams)\\n        output_paths: Dictionary of output paths with logical names\\n            - \"model_output\": Directory to save model artifacts\\n            - \"evaluation_output\": Directory to save evaluation outputs\\n        environ_vars: Dictionary of environment variables\\n        job_args: Command line arguments\\n    ' line_number=551 context='    job_args: argparse.Namespace,\\n) -> None:\\n>>>     \"\"\"\\n    Main function to execute the XGBoost training logic.\\n' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=576 context='            hparam_path = input_paths[\"hyperparameters_s3_uri\"]\\n            # If it\\'s a directory path, append the filename\\n>>>             if not hparam_path.endswith(\"hyperparameters.json\"):\\n                hparam_path = os.path.join(hparam_path, \"hyperparameters.json\")\\n        else:' is_hardcoded=True construction_method=None",
        "path='hyperparameters.json' line_number=577 context='            # If it\\'s a directory path, append the filename\\n            if not hparam_path.endswith(\"hyperparameters.json\"):\\n>>>                 hparam_path = os.path.join(hparam_path, \"hyperparameters.json\")\\n        else:\\n            # Fallback to source directory if not provided' is_hardcoded=False construction_method='os.path.join'",
        "path='hyperparameters.json' line_number=577 context='            # If it\\'s a directory path, append the filename\\n            if not hparam_path.endswith(\"hyperparameters.json\"):\\n>>>                 hparam_path = os.path.join(hparam_path, \"hyperparameters.json\")\\n        else:\\n            # Fallback to source directory if not provided' is_hardcoded=True construction_method=None",
        "path='/opt/ml/code/hyperparams/hyperparameters.json' line_number=580 context='        else:\\n            # Fallback to source directory if not provided\\n>>>             hparam_path = \"/opt/ml/code/hyperparams/hyperparameters.json\"\\n\\n        logger.info(\"Starting XGBoost training process...\")' is_hardcoded=True construction_method=None",
        "path='Starting XGBoost training process...' line_number=582 context='            hparam_path = \"/opt/ml/code/hyperparams/hyperparameters.json\"\\n\\n>>>         logger.info(\"Starting XGBoost training process...\")\\n        logger.info(f\"Loading configuration from {hparam_path}\")\\n        config = load_and_validate_config(hparam_path)' is_hardcoded=True construction_method=None",
        "path='Loading datasets...' line_number=587 context='        logger.info(\"Configuration loaded successfully\")\\n\\n>>>         logger.info(\"Loading datasets...\")\\n        train_df, val_df, test_df = load_datasets(data_dir)\\n        logger.info(\"Datasets loaded successfully\")' is_hardcoded=True construction_method=None",
        "path='Starting numerical imputation...' line_number=592 context='\\n        # Apply numerical imputation\\n>>>         logger.info(\"Starting numerical imputation...\")\\n        train_df, val_df, test_df, impute_dict = apply_numerical_imputation(\\n            config, train_df, val_df, test_df' is_hardcoded=True construction_method=None",
        "path='Starting risk table mapping...' line_number=599 context='\\n        # Apply risk table mapping\\n>>>         logger.info(\"Starting risk table mapping...\")\\n        train_df, val_df, test_df, risk_tables = fit_and_apply_risk_tables(\\n            config, train_df, val_df, test_df' is_hardcoded=True construction_method=None",
        "path='Preparing DMatrices for XGBoost...' line_number=605 context='        logger.info(\"Risk table mapping completed\")\\n\\n>>>         logger.info(\"Preparing DMatrices for XGBoost...\")\\n        dtrain, dval, feature_columns = prepare_dmatrices(config, train_df, val_df)\\n        logger.info(\"DMatrices prepared successfully\")' is_hardcoded=True construction_method=None",
        "path='Starting model training...' line_number=612 context='        )\\n\\n>>>         logger.info(\"Starting model training...\")\\n        model = train_model(config, dtrain, dval)\\n        logger.info(\"Model training completed\")' is_hardcoded=True construction_method=None",
        "path='Saving model artifacts...' line_number=616 context='        logger.info(\"Model training completed\")\\n\\n>>>         logger.info(\"Saving model artifacts...\")\\n        logger.info(f\"Model path: {model_dir}, Output path: {output_dir}\")\\n        logger.info(f\"Output path exists: {os.path.exists(output_dir)}\")' is_hardcoded=True construction_method=None",
        "path=' does not exist, creating...' line_number=636 context='        logger.info(f\"Checking output directory: {output_dir}\")\\n        if not os.path.exists(output_dir):\\n>>>             logger.warning(f\"Output directory {output_dir} does not exist, creating...\")\\n            os.makedirs(output_dir, exist_ok=True)\\n' is_hardcoded=True construction_method=None",
        "path='All evaluation steps complete.' line_number=657 context='            logger.error(traceback.format_exc())\\n\\n>>>         logger.info(\"All evaluation steps complete.\")\\n        logger.info(\"====== MAIN EXECUTION COMPLETED SUCCESSFULLY ======\")\\n        logger.info(\"Training script finished successfully.\")' is_hardcoded=True construction_method=None",
        "path='Training script finished successfully.' line_number=659 context='        logger.info(\"All evaluation steps complete.\")\\n        logger.info(\"====== MAIN EXECUTION COMPLETED SUCCESSFULLY ======\")\\n>>>         logger.info(\"Training script finished successfully.\")\\n    except Exception as e:\\n        logger.error(f\"FATAL ERROR in main execution: {str(e)}\")' is_hardcoded=True construction_method=None",
        "path='Script starting...' line_number=670 context='# -------------------------------------------------------------------------\\nif __name__ == \"__main__\":\\n>>>     logger.info(\"Script starting...\")\\n\\n    # Container path constants' is_hardcoded=True construction_method=None",
        "path='/opt/ml/input/data' line_number=674 context='    # Container path constants\\n    CONTAINER_PATHS = {\\n>>>         \"INPUT_DATA\": \"/opt/ml/input/data\",\\n        \"MODEL_DIR\": \"/opt/ml/model\",\\n        \"OUTPUT_DATA\": \"/opt/ml/output/data\",' is_hardcoded=True construction_method=None",
        "path='/opt/ml/model' line_number=675 context='    CONTAINER_PATHS = {\\n        \"INPUT_DATA\": \"/opt/ml/input/data\",\\n>>>         \"MODEL_DIR\": \"/opt/ml/model\",\\n        \"OUTPUT_DATA\": \"/opt/ml/output/data\",\\n        \"CONFIG_DIR\": \"/opt/ml/code/hyperparams\",  # Source directory path' is_hardcoded=True construction_method=None",
        "path='/opt/ml/output/data' line_number=676 context='        \"INPUT_DATA\": \"/opt/ml/input/data\",\\n        \"MODEL_DIR\": \"/opt/ml/model\",\\n>>>         \"OUTPUT_DATA\": \"/opt/ml/output/data\",\\n        \"CONFIG_DIR\": \"/opt/ml/code/hyperparams\",  # Source directory path\\n    }' is_hardcoded=True construction_method=None",
        "path='/opt/ml/code/hyperparams' line_number=677 context='        \"MODEL_DIR\": \"/opt/ml/model\",\\n        \"OUTPUT_DATA\": \"/opt/ml/output/data\",\\n>>>         \"CONFIG_DIR\": \"/opt/ml/code/hyperparams\",  # Source directory path\\n    }\\n' is_hardcoded=True construction_method=None"
      ],
      "env_var_accesses": [],
      "imports": [
        "module_name='os' import_alias=None line_number=2 is_from_import=False imported_items=[]",
        "module_name='sys' import_alias=None line_number=3 is_from_import=False imported_items=[]",
        "module_name='argparse' import_alias=None line_number=4 is_from_import=False imported_items=[]",
        "module_name='json' import_alias=None line_number=5 is_from_import=False imported_items=[]",
        "module_name='logging' import_alias=None line_number=6 is_from_import=False imported_items=[]",
        "module_name='traceback' import_alias=None line_number=7 is_from_import=False imported_items=[]",
        "module_name='pathlib' import_alias=None line_number=8 is_from_import=True imported_items=['Path']",
        "module_name='typing' import_alias=None line_number=9 is_from_import=True imported_items=['Dict', 'List', 'Any', 'Optional', 'Union', 'Tuple']",
        "module_name='pandas' import_alias='pd' line_number=11 is_from_import=False imported_items=[]",
        "module_name='numpy' import_alias='np' line_number=12 is_from_import=False imported_items=[]",
        "module_name='pickle' import_alias='pkl' line_number=13 is_from_import=False imported_items=[]",
        "module_name='xgboost' import_alias='xgb' line_number=14 is_from_import=False imported_items=[]",
        "module_name='tarfile' import_alias=None line_number=16 is_from_import=False imported_items=[]",
        "module_name='matplotlib.pyplot' import_alias='plt' line_number=17 is_from_import=False imported_items=[]",
        "module_name='sklearn.metrics' import_alias=None line_number=18 is_from_import=True imported_items=['roc_auc_score', 'average_precision_score', 'f1_score', 'roc_curve', 'precision_recall_curve']",
        "module_name='processing.risk_table_processor' import_alias=None line_number=30 is_from_import=True imported_items=['RiskTableMappingProcessor']",
        "module_name='processing.numerical_imputation_processor' import_alias=None line_number=31 is_from_import=True imported_items=['NumericalVariableImputationProcessor']",
        "module_name='pydantic' import_alias=None line_number=76 is_from_import=True imported_items=['BaseModel', 'Field', 'model_validator']",
        "module_name='hyperparams.hyperparameters_xgboost' import_alias=None line_number=77 is_from_import=True imported_items=['XGBoostModelHyperparameters']"
      ],
      "argument_definitions": [],
      "file_operations": [
        "file_path='<file_object>' operation_type='read' line_number=101 context='    try:\\n        with open(hparam_path, \"r\") as f:\\n>>>             config = json.load(f)\\n\\n        required_keys = [' mode=None method='json.load'",
        "file_path='<file_object>' operation_type='write' line_number=366 context='    risk_map_file = os.path.join(model_path, \"risk_table_map.pkl\")\\n    with open(risk_map_file, \"wb\") as f:\\n>>>         pkl.dump(risk_tables, f)\\n    logger.info(f\"Saved consolidated risk table map to {risk_map_file}\")\\n' mode=None method='pickle.dump'",
        "file_path='<file_object>' operation_type='write' line_number=372 context='    impute_file = os.path.join(model_path, \"impute_dict.pkl\")\\n    with open(impute_file, \"wb\") as f:\\n>>>         pkl.dump(impute_dict, f)\\n    logger.info(f\"Saved imputation dictionary to {impute_file}\")\\n' mode=None method='pickle.dump'",
        "file_path='<file_object>' operation_type='write' line_number=378 context='    fmap_json = os.path.join(model_path, \"feature_importance.json\")\\n    with open(fmap_json, \"w\") as f:\\n>>>         json.dump(model.get_fscore(), f, indent=2)\\n    logger.info(f\"Saved feature importance to {fmap_json}\")\\n' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=397 context='    hyperparameters_file = os.path.join(model_path, \"hyperparameters.json\")\\n    with open(hyperparameters_file, \"w\") as f:\\n>>>         json.dump(config, f, indent=2, sort_keys=True)\\n    logger.info(f\"Saved hyperparameters configuration to {hyperparameters_file}\")\\n' mode=None method='json.dump'",
        "file_path='<file_object>' operation_type='write' line_number=449 context='        logger.info(f\"F1-Score (macro): {metrics[\\'f1_score_macro\\']}\")\\n    with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\\n>>>         json.dump(metrics, f, indent=2)\\n    # preds\\n    df = pd.DataFrame({id_col: ids, label_col: y_true})' mode=None method='json.dump'"
      ],
      "step_type": "Training",
      "framework": "xgboost",
      "step_type_patterns": {
        "error": "Training pattern detection failed: 'str' object has no attribute 'get'"
      }
    },
    "contract": {
      "entry_point": "xgboost_training.py",
      "inputs": {
        "input_path": {
          "path": "/opt/ml/input/data"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/code/hyperparams/hyperparameters.json"
        }
      },
      "outputs": {
        "model_output": {
          "path": "/opt/ml/model"
        },
        "evaluation_output": {
          "path": "/opt/ml/output/data"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    XGBoost training script for tabular data classification that:\n    1. Loads training, validation, and test datasets from split directories\n    2. Applies numerical imputation using mean strategy for missing values\n    3. Fits risk tables on categorical features using training data\n    4. Transforms all datasets using fitted preprocessing artifacts\n    5. Trains XGBoost model with configurable hyperparameters\n    6. Supports both binary and multiclass classification\n    7. Handles class weights for imbalanced datasets\n    8. Evaluates model performance with comprehensive metrics\n    9. Saves model artifacts and preprocessing components\n    10. Generates prediction files and performance visualizations\n    \n    Input Structure:\n    - /opt/ml/input/data: Root directory containing train/val/test subdirectories\n      - /opt/ml/input/data/train: Training data files (.csv, .parquet, .json)\n      - /opt/ml/input/data/val: Validation data files\n      - /opt/ml/input/data/test: Test data files\n    - /opt/ml/input/data/config/hyperparameters.json: Model configuration (optional)\n    \n    Output Structure:\n    - /opt/ml/model: Model artifacts directory\n      - /opt/ml/model/xgboost_model.bst: Trained XGBoost model\n      - /opt/ml/model/risk_table_map.pkl: Risk table mappings for categorical features\n      - /opt/ml/model/impute_dict.pkl: Imputation values for numerical features\n      - /opt/ml/model/feature_importance.json: Feature importance scores\n      - /opt/ml/model/feature_columns.txt: Ordered feature column names\n      - /opt/ml/model/hyperparameters.json: Model hyperparameters\n    - /opt/ml/output/data: Evaluation results directory\n      - /opt/ml/output/data/val.tar.gz: Validation predictions and metrics\n      - /opt/ml/output/data/test.tar.gz: Test predictions and metrics\n    \n    Contract aligned with step specification:\n    - Inputs: input_path (required), hyperparameters_s3_uri (optional)\n    - Outputs: model_output (primary), evaluation_output (secondary)\n    \n    Hyperparameters (via JSON config):\n    - Data fields: tab_field_list, cat_field_list, label_name, id_name\n    - Model: is_binary, num_classes, class_weights\n    - XGBoost: eta, gamma, max_depth, subsample, colsample_bytree, lambda_xgb, alpha_xgb\n    - Training: num_round, early_stopping_rounds\n    - Risk tables: smooth_factor, count_threshold\n    \n    Binary Classification:\n    - Uses binary:logistic objective\n    - Supports scale_pos_weight for class imbalance\n    - Generates ROC and PR curves\n    - Computes AUC-ROC, Average Precision, F1-Score\n    \n    Multiclass Classification:\n    - Uses multi:softprob objective\n    - Supports sample weights for class imbalance\n    - Generates per-class and aggregate metrics\n    - Computes micro/macro averaged metrics\n    \n    Risk Table Processing:\n    - Fits risk tables on categorical features using target correlation\n    - Applies smoothing and count thresholds for robust estimation\n    - Transforms categorical values to risk scores\n    \n    Numerical Imputation:\n    - Uses mean imputation strategy for missing numerical values\n    - Fits imputation on training data only\n    - Applies same imputation to validation and test sets\n    ",
      "framework_requirements": {
        "boto3": ">=1.26.0",
        "xgboost": "==1.7.6",
        "scikit-learn": ">=0.23.2,<1.0.0",
        "pandas": ">=1.2.0,<2.0.0",
        "pyarrow": ">=4.0.0,<6.0.0",
        "beautifulsoup4": ">=4.9.3",
        "flask": ">=2.0.0,<3.0.0",
        "pydantic": ">=2.0.0,<3.0.0",
        "typing-extensions": ">=4.2.0",
        "matplotlib": ">=3.0.0",
        "numpy": ">=1.19.0"
      }
    }
  },
  "level2": {
    "passed": true,
    "issues": [
      {
        "severity": "INFO",
        "category": "step_type_resolution",
        "message": "Step type resolved via registry: XGBoostTraining -> XGBoostTraining -> Training",
        "details": {
          "contract": "xgboost_training_contract",
          "original_spec_type": "XGBoostTraining",
          "canonical_name": "XGBoostTraining",
          "resolved_sagemaker_type": "Training",
          "registry_available": true
        },
        "recommendation": "Using Training step property paths for validation"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output model_output: properties.ModelArtifacts.S3ModelArtifacts",
        "details": {
          "contract": "xgboost_training_contract",
          "logical_name": "model_output",
          "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
          "step_type": "training",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation",
        "message": "Valid property path in output evaluation_output: properties.OutputDataConfig.S3OutputPath",
        "details": {
          "contract": "xgboost_training_contract",
          "logical_name": "evaluation_output",
          "property_path": "properties.OutputDataConfig.S3OutputPath",
          "step_type": "training",
          "validation_source": "SageMaker Documentation v2.92.2",
          "documentation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference"
        },
        "recommendation": "Property path is correctly formatted for the step type"
      },
      {
        "severity": "INFO",
        "category": "property_path_validation_summary",
        "message": "Property path validation completed for xgboost_training_contract",
        "details": {
          "contract": "xgboost_training_contract",
          "step_type": "training",
          "node_type": "internal",
          "total_outputs": 2,
          "outputs_with_property_paths": 2,
          "validation_reference": "https://sagemaker.readthedocs.io/en/v2.92.2/amazon_sagemaker_model_building_pipeline.html#data-dependency-property-reference",
          "documentation_version": "v2.92.2"
        },
        "recommendation": "Validated 2/2 outputs with property paths against SageMaker documentation"
      }
    ],
    "contract": {
      "entry_point": "xgboost_training.py",
      "inputs": {
        "input_path": {
          "path": "/opt/ml/input/data"
        },
        "hyperparameters_s3_uri": {
          "path": "/opt/ml/code/hyperparams/hyperparameters.json"
        }
      },
      "outputs": {
        "model_output": {
          "path": "/opt/ml/model"
        },
        "evaluation_output": {
          "path": "/opt/ml/output/data"
        }
      },
      "arguments": {},
      "environment_variables": {
        "required": [],
        "optional": {}
      },
      "description": "\n    XGBoost training script for tabular data classification that:\n    1. Loads training, validation, and test datasets from split directories\n    2. Applies numerical imputation using mean strategy for missing values\n    3. Fits risk tables on categorical features using training data\n    4. Transforms all datasets using fitted preprocessing artifacts\n    5. Trains XGBoost model with configurable hyperparameters\n    6. Supports both binary and multiclass classification\n    7. Handles class weights for imbalanced datasets\n    8. Evaluates model performance with comprehensive metrics\n    9. Saves model artifacts and preprocessing components\n    10. Generates prediction files and performance visualizations\n    \n    Input Structure:\n    - /opt/ml/input/data: Root directory containing train/val/test subdirectories\n      - /opt/ml/input/data/train: Training data files (.csv, .parquet, .json)\n      - /opt/ml/input/data/val: Validation data files\n      - /opt/ml/input/data/test: Test data files\n    - /opt/ml/input/data/config/hyperparameters.json: Model configuration (optional)\n    \n    Output Structure:\n    - /opt/ml/model: Model artifacts directory\n      - /opt/ml/model/xgboost_model.bst: Trained XGBoost model\n      - /opt/ml/model/risk_table_map.pkl: Risk table mappings for categorical features\n      - /opt/ml/model/impute_dict.pkl: Imputation values for numerical features\n      - /opt/ml/model/feature_importance.json: Feature importance scores\n      - /opt/ml/model/feature_columns.txt: Ordered feature column names\n      - /opt/ml/model/hyperparameters.json: Model hyperparameters\n    - /opt/ml/output/data: Evaluation results directory\n      - /opt/ml/output/data/val.tar.gz: Validation predictions and metrics\n      - /opt/ml/output/data/test.tar.gz: Test predictions and metrics\n    \n    Contract aligned with step specification:\n    - Inputs: input_path (required), hyperparameters_s3_uri (optional)\n    - Outputs: model_output (primary), evaluation_output (secondary)\n    \n    Hyperparameters (via JSON config):\n    - Data fields: tab_field_list, cat_field_list, label_name, id_name\n    - Model: is_binary, num_classes, class_weights\n    - XGBoost: eta, gamma, max_depth, subsample, colsample_bytree, lambda_xgb, alpha_xgb\n    - Training: num_round, early_stopping_rounds\n    - Risk tables: smooth_factor, count_threshold\n    \n    Binary Classification:\n    - Uses binary:logistic objective\n    - Supports scale_pos_weight for class imbalance\n    - Generates ROC and PR curves\n    - Computes AUC-ROC, Average Precision, F1-Score\n    \n    Multiclass Classification:\n    - Uses multi:softprob objective\n    - Supports sample weights for class imbalance\n    - Generates per-class and aggregate metrics\n    - Computes micro/macro averaged metrics\n    \n    Risk Table Processing:\n    - Fits risk tables on categorical features using target correlation\n    - Applies smoothing and count thresholds for robust estimation\n    - Transforms categorical values to risk scores\n    \n    Numerical Imputation:\n    - Uses mean imputation strategy for missing numerical values\n    - Fits imputation on training data only\n    - Applies same imputation to validation and test sets\n    ",
      "framework_requirements": {
        "boto3": ">=1.26.0",
        "xgboost": "==1.7.6",
        "scikit-learn": ">=0.23.2,<1.0.0",
        "pandas": ">=1.2.0,<2.0.0",
        "pyarrow": ">=4.0.0,<6.0.0",
        "beautifulsoup4": ">=4.9.3",
        "flask": ">=2.0.0,<3.0.0",
        "pydantic": ">=2.0.0,<3.0.0",
        "typing-extensions": ">=4.2.0",
        "matplotlib": ">=3.0.0",
        "numpy": ">=1.19.0"
      }
    },
    "specifications": {
      "xgboost_training_spec": {
        "step_type": "XGBoostTraining",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "input_path",
            "dependency_type": "training_data",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "RiskTableMapping",
              "DataLoad",
              "ProcessingStep",
              "StratifiedSampling"
            ],
            "data_type": "S3Uri",
            "description": "Training dataset S3 location with train/val/test subdirectories"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "HyperparameterPrep",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "S3 URI containing hyperparameters configuration file (optional - falls back to source directory)"
          }
        ],
        "outputs": [
          {
            "logical_name": "model_output",
            "output_type": "model_artifacts",
            "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
            "data_type": "S3Uri",
            "description": "Trained XGBoost model artifacts"
          },
          {
            "logical_name": "evaluation_output",
            "output_type": "processing_output",
            "property_path": "properties.OutputDataConfig.S3OutputPath",
            "data_type": "S3Uri",
            "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
          }
        ]
      }
    },
    "unified_specification": {
      "primary_spec": {
        "step_type": "XGBoostTraining",
        "node_type": "internal",
        "dependencies": [
          {
            "logical_name": "input_path",
            "dependency_type": "training_data",
            "required": true,
            "compatible_sources": [
              "TabularPreprocessing",
              "RiskTableMapping",
              "DataLoad",
              "ProcessingStep",
              "StratifiedSampling"
            ],
            "data_type": "S3Uri",
            "description": "Training dataset S3 location with train/val/test subdirectories"
          },
          {
            "logical_name": "hyperparameters_s3_uri",
            "dependency_type": "hyperparameters",
            "required": false,
            "compatible_sources": [
              "HyperparameterPrep",
              "ProcessingStep"
            ],
            "data_type": "S3Uri",
            "description": "S3 URI containing hyperparameters configuration file (optional - falls back to source directory)"
          }
        ],
        "outputs": [
          {
            "logical_name": "model_output",
            "output_type": "model_artifacts",
            "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
            "data_type": "S3Uri",
            "description": "Trained XGBoost model artifacts"
          },
          {
            "logical_name": "evaluation_output",
            "output_type": "processing_output",
            "property_path": "properties.OutputDataConfig.S3OutputPath",
            "data_type": "S3Uri",
            "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
          }
        ]
      },
      "variants": {
        "training": {
          "step_type": "XGBoostTraining",
          "node_type": "internal",
          "dependencies": [
            {
              "logical_name": "input_path",
              "dependency_type": "training_data",
              "required": true,
              "compatible_sources": [
                "TabularPreprocessing",
                "RiskTableMapping",
                "DataLoad",
                "ProcessingStep",
                "StratifiedSampling"
              ],
              "data_type": "S3Uri",
              "description": "Training dataset S3 location with train/val/test subdirectories"
            },
            {
              "logical_name": "hyperparameters_s3_uri",
              "dependency_type": "hyperparameters",
              "required": false,
              "compatible_sources": [
                "HyperparameterPrep",
                "ProcessingStep"
              ],
              "data_type": "S3Uri",
              "description": "S3 URI containing hyperparameters configuration file (optional - falls back to source directory)"
            }
          ],
          "outputs": [
            {
              "logical_name": "model_output",
              "output_type": "model_artifacts",
              "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
              "data_type": "S3Uri",
              "description": "Trained XGBoost model artifacts"
            },
            {
              "logical_name": "evaluation_output",
              "output_type": "processing_output",
              "property_path": "properties.OutputDataConfig.S3OutputPath",
              "data_type": "S3Uri",
              "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
            }
          ]
        }
      },
      "unified_dependencies": {
        "input_path": {
          "logical_name": "input_path",
          "dependency_type": "training_data",
          "required": true,
          "compatible_sources": [
            "TabularPreprocessing",
            "RiskTableMapping",
            "DataLoad",
            "ProcessingStep",
            "StratifiedSampling"
          ],
          "data_type": "S3Uri",
          "description": "Training dataset S3 location with train/val/test subdirectories"
        },
        "hyperparameters_s3_uri": {
          "logical_name": "hyperparameters_s3_uri",
          "dependency_type": "hyperparameters",
          "required": false,
          "compatible_sources": [
            "HyperparameterPrep",
            "ProcessingStep"
          ],
          "data_type": "S3Uri",
          "description": "S3 URI containing hyperparameters configuration file (optional - falls back to source directory)"
        }
      },
      "unified_outputs": {
        "model_output": {
          "logical_name": "model_output",
          "output_type": "model_artifacts",
          "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
          "data_type": "S3Uri",
          "description": "Trained XGBoost model artifacts"
        },
        "evaluation_output": {
          "logical_name": "evaluation_output",
          "output_type": "processing_output",
          "property_path": "properties.OutputDataConfig.S3OutputPath",
          "data_type": "S3Uri",
          "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
        }
      },
      "dependency_sources": {
        "input_path": [
          "training"
        ],
        "hyperparameters_s3_uri": [
          "training"
        ]
      },
      "output_sources": {
        "model_output": [
          "training"
        ],
        "evaluation_output": [
          "training"
        ]
      },
      "variant_count": 1
    }
  },
  "level3": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "dependency_compatibility",
        "message": "Dependency hyperparameters_s3_uri has low compatibility score: 0.420",
        "details": {
          "logical_name": "hyperparameters_s3_uri",
          "specification": "xgboost_training",
          "best_match": {
            "provider": "BatchTransform",
            "output": "transform_output",
            "score": 0.4197368421052632
          },
          "required": false,
          "threshold_info": {
            "mode": "relaxed",
            "thresholds": {
              "pass": "\u2265 0.6",
              "warning": "0.4 - 0.59",
              "error": "0.2 - 0.39",
              "critical": "< 0.2"
            },
            "resolution_threshold": 0.5,
            "description": "Relaxed validation allowing reasonable compatibility matches"
          },
          "score_breakdown": {
            "type_compatibility": 0.2,
            "data_type_compatibility": 0.2,
            "semantic_similarity": 0.019736842105263157,
            "exact_match_bonus": 0.0,
            "source_compatibility": 0.0,
            "keyword_matching": 0.0
          },
          "all_candidates": [
            {
              "provider": "BatchTransform",
              "output": "transform_output",
              "score": 0.4197368421052632
            },
            {
              "provider": "PyTorchModel",
              "output": "model_name",
              "score": 0.31875000000000003
            },
            {
              "provider": "XGBoostModel",
              "output": "model_name",
              "score": 0.31875000000000003
            }
          ]
        },
        "recommendation": "Consider renaming 'hyperparameters_s3_uri' or adding aliases to improve semantic matching; Add 'BatchTransform' to compatible_sources for hyperparameters_s3_uri"
      }
    ],
    "specification": {
      "step_type": "XGBoostTraining",
      "node_type": "internal",
      "dependencies": [
        {
          "logical_name": "input_path",
          "dependency_type": "training_data",
          "required": true,
          "compatible_sources": [
            "TabularPreprocessing",
            "RiskTableMapping",
            "DataLoad",
            "ProcessingStep",
            "StratifiedSampling"
          ],
          "data_type": "S3Uri",
          "description": "Training dataset S3 location with train/val/test subdirectories"
        },
        {
          "logical_name": "hyperparameters_s3_uri",
          "dependency_type": "hyperparameters",
          "required": false,
          "compatible_sources": [
            "HyperparameterPrep",
            "ProcessingStep"
          ],
          "data_type": "S3Uri",
          "description": "S3 URI containing hyperparameters configuration file (optional - falls back to source directory)"
        }
      ],
      "outputs": [
        {
          "logical_name": "model_output",
          "output_type": "model_artifacts",
          "property_path": "properties.ModelArtifacts.S3ModelArtifacts",
          "data_type": "S3Uri",
          "description": "Trained XGBoost model artifacts"
        },
        {
          "logical_name": "evaluation_output",
          "output_type": "processing_output",
          "property_path": "properties.OutputDataConfig.S3OutputPath",
          "data_type": "S3Uri",
          "description": "Model evaluation results and predictions (val.tar.gz, test.tar.gz)"
        }
      ]
    }
  },
  "level4": {
    "passed": true,
    "issues": [
      {
        "severity": "WARNING",
        "category": "configuration_fields",
        "message": "Required configuration field not accessed in builder: project_root_folder",
        "details": {
          "field_name": "project_root_folder",
          "builder": "xgboost_training"
        },
        "recommendation": "Access required field project_root_folder in builder or make it optional"
      },
      {
        "severity": "INFO",
        "category": "required_field_validation",
        "message": "Builder has required fields but no explicit validation logic detected",
        "details": {
          "required_fields": [
            "region",
            "training_entry_point",
            "author",
            "hyperparameters",
            "pipeline_version",
            "service_name",
            "project_root_folder",
            "role",
            "bucket"
          ],
          "builder": "xgboost_training"
        },
        "recommendation": "Consider adding explicit validation logic for required configuration fields"
      }
    ],
    "builder_analysis": {
      "config_accesses": [
        {
          "field_name": "effective_source_dir",
          "line_number": 134,
          "context": "line_134"
        },
        {
          "field_name": "training_entry_point",
          "line_number": 138,
          "context": "line_138"
        },
        {
          "field_name": "framework_version",
          "line_number": 140,
          "context": "line_140"
        },
        {
          "field_name": "py_version",
          "line_number": 141,
          "context": "line_141"
        },
        {
          "field_name": "training_instance_type",
          "line_number": 143,
          "context": "line_143"
        },
        {
          "field_name": "training_instance_count",
          "line_number": 144,
          "context": "line_144"
        },
        {
          "field_name": "training_volume_size",
          "line_number": 145,
          "context": "line_145"
        },
        {
          "field_name": "env",
          "line_number": 165,
          "context": "line_165"
        },
        {
          "field_name": "env",
          "line_number": 166,
          "context": "line_166"
        },
        {
          "field_name": "skip_hyperparameters_s3_uri",
          "line_number": 222,
          "context": "line_222"
        }
      ],
      "validation_calls": [],
      "default_assignments": [],
      "class_definitions": [
        {
          "class_name": "XGBoostTrainingStepBuilder",
          "line_number": 34,
          "base_classes": [
            "StepBuilderBase"
          ],
          "decorators": []
        }
      ],
      "method_definitions": [
        {
          "method_name": "__init__",
          "line_number": 41,
          "args": [
            "self",
            "config",
            "sagemaker_session",
            "role",
            "registry_manager",
            "dependency_resolver"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "validate_configuration",
          "line_number": 83,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_create_estimator",
          "line_number": 117,
          "args": [
            "self",
            "output_path"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_environment_variables",
          "line_number": 152,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_create_data_channels_from_source",
          "line_number": 171,
          "args": [
            "self",
            "base_path"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_inputs",
          "line_number": 193,
          "args": [
            "self",
            "inputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "_get_outputs",
          "line_number": 287,
          "args": [
            "self",
            "outputs"
          ],
          "decorators": [],
          "is_async": false
        },
        {
          "method_name": "create_step",
          "line_number": 356,
          "args": [
            "self"
          ],
          "decorators": [],
          "is_async": false
        }
      ],
      "import_statements": [
        {
          "type": "from_import",
          "module": "typing",
          "name": "Dict",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Optional",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "Any",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "typing",
          "name": "List",
          "alias": null,
          "line_number": 1
        },
        {
          "type": "from_import",
          "module": "pathlib",
          "name": "Path",
          "alias": null,
          "line_number": 2
        },
        {
          "type": "import",
          "module": "logging",
          "alias": null,
          "line_number": 3
        },
        {
          "type": "import",
          "module": "tempfile",
          "alias": null,
          "line_number": 4
        },
        {
          "type": "import",
          "module": "json",
          "alias": null,
          "line_number": 5
        },
        {
          "type": "import",
          "module": "shutil",
          "alias": null,
          "line_number": 6
        },
        {
          "type": "import",
          "module": "boto3",
          "alias": null,
          "line_number": 7
        },
        {
          "type": "from_import",
          "module": "botocore.exceptions",
          "name": "ClientError",
          "alias": null,
          "line_number": 8
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "TrainingStep",
          "alias": null,
          "line_number": 10
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.steps",
          "name": "Step",
          "alias": null,
          "line_number": 10
        },
        {
          "type": "from_import",
          "module": "sagemaker.inputs",
          "name": "TrainingInput",
          "alias": null,
          "line_number": 11
        },
        {
          "type": "from_import",
          "module": "sagemaker.xgboost",
          "name": "XGBoost",
          "alias": null,
          "line_number": 12
        },
        {
          "type": "from_import",
          "module": "sagemaker.s3",
          "name": "S3Uploader",
          "alias": null,
          "line_number": 13
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.functions",
          "name": "Join",
          "alias": null,
          "line_number": 14
        },
        {
          "type": "from_import",
          "module": "configs.config_xgboost_training_step",
          "name": "XGBoostTrainingConfig",
          "alias": null,
          "line_number": 16
        },
        {
          "type": "from_import",
          "module": "core.base.builder_base",
          "name": "StepBuilderBase",
          "alias": null,
          "line_number": 17
        },
        {
          "type": "from_import",
          "module": "s3_utils",
          "name": "S3PathHandler",
          "alias": null,
          "line_number": 18
        },
        {
          "type": "from_import",
          "module": "core.deps.registry_manager",
          "name": "RegistryManager",
          "alias": null,
          "line_number": 19
        },
        {
          "type": "from_import",
          "module": "core.deps.dependency_resolver",
          "name": "UnifiedDependencyResolver",
          "alias": null,
          "line_number": 20
        },
        {
          "type": "from_import",
          "module": "specs.xgboost_training_spec",
          "name": "XGBOOST_TRAINING_SPEC",
          "alias": null,
          "line_number": 24
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.functions",
          "name": "Join",
          "alias": null,
          "line_number": 181
        },
        {
          "type": "from_import",
          "module": "sagemaker.workflow.functions",
          "name": "Join",
          "alias": null,
          "line_number": 330
        }
      ],
      "config_class_usage": []
    },
    "config_analysis": {
      "class_name": "XGBoostTrainingConfig",
      "fields": {
        "author": {
          "type": "<class 'str'>",
          "required": true
        },
        "bucket": {
          "type": "<class 'str'>",
          "required": true
        },
        "role": {
          "type": "<class 'str'>",
          "required": true
        },
        "region": {
          "type": "<class 'str'>",
          "required": true
        },
        "service_name": {
          "type": "<class 'str'>",
          "required": true
        },
        "pipeline_version": {
          "type": "<class 'str'>",
          "required": true
        },
        "model_class": {
          "type": "<class 'str'>",
          "required": false
        },
        "current_date": {
          "type": "<class 'str'>",
          "required": false
        },
        "framework_version": {
          "type": "<class 'str'>",
          "required": false
        },
        "py_version": {
          "type": "<class 'str'>",
          "required": false
        },
        "source_dir": {
          "type": "typing.Optional[str]",
          "required": false
        },
        "project_root_folder": {
          "type": "<class 'str'>",
          "required": true
        },
        "training_entry_point": {
          "type": "<class 'str'>",
          "required": true
        },
        "hyperparameters": {
          "type": "<class 'cursus.steps.hyperparams.hyperparameters_xgboost.XGBoostModelHyperparameters'>",
          "required": true
        },
        "training_instance_type": {
          "type": "<class 'str'>",
          "required": false
        },
        "training_instance_count": {
          "type": "<class 'int'>",
          "required": false
        },
        "training_volume_size": {
          "type": "<class 'int'>",
          "required": false
        },
        "skip_hyperparameters_s3_uri": {
          "type": "<class 'bool'>",
          "required": false
        },
        "aws_region": {
          "type": "property",
          "required": false
        },
        "effective_source_dir": {
          "type": "property",
          "required": false
        },
        "hyperparameter_file": {
          "type": "property",
          "required": false
        },
        "model_extra": {
          "type": "property",
          "required": false
        },
        "model_fields_set": {
          "type": "property",
          "required": false
        },
        "pipeline_description": {
          "type": "property",
          "required": false
        },
        "pipeline_name": {
          "type": "property",
          "required": false
        },
        "pipeline_s3_loc": {
          "type": "property",
          "required": false
        },
        "resolved_source_dir": {
          "type": "property",
          "required": false
        },
        "script_contract": {
          "type": "property",
          "required": false
        },
        "step_catalog": {
          "type": "property",
          "required": false
        }
      },
      "required_fields": [
        "author",
        "bucket",
        "role",
        "region",
        "service_name",
        "pipeline_version",
        "project_root_folder",
        "training_entry_point",
        "hyperparameters"
      ],
      "optional_fields": [
        "model_class",
        "current_date",
        "framework_version",
        "py_version",
        "source_dir",
        "training_instance_type",
        "training_instance_count",
        "training_volume_size",
        "skip_hyperparameters_s3_uri",
        "aws_region",
        "effective_source_dir",
        "hyperparameter_file",
        "model_extra",
        "model_fields_set",
        "pipeline_description",
        "pipeline_name",
        "pipeline_s3_loc",
        "resolved_source_dir",
        "script_contract",
        "step_catalog"
      ],
      "default_values": {
        "author": "PydanticUndefined",
        "bucket": "PydanticUndefined",
        "role": "PydanticUndefined",
        "region": "PydanticUndefined",
        "service_name": "PydanticUndefined",
        "pipeline_version": "PydanticUndefined",
        "model_class": "xgboost",
        "current_date": "PydanticUndefined",
        "framework_version": "1.7-1",
        "py_version": "py3",
        "source_dir": null,
        "project_root_folder": "PydanticUndefined",
        "training_entry_point": "PydanticUndefined",
        "hyperparameters": "PydanticUndefined",
        "training_instance_type": "ml.m5.4xlarge",
        "training_instance_count": 1,
        "training_volume_size": 30,
        "skip_hyperparameters_s3_uri": true
      }
    }
  },
  "overall_status": "PASSING",
  "scoring": {
    "overall_score": 100.0,
    "quality_rating": "Excellent",
    "level_scores": {
      "level1_script_contract": 100.0,
      "level2_contract_spec": 100.0,
      "level3_spec_dependencies": 100.0,
      "level4_builder_config": 100.0
    }
  },
  "metadata": {
    "script_path": "/Users/tianpeixie/github_workspace/cursus/src/cursus/steps/scripts/xgboost_training.py",
    "validation_timestamp": "2025-09-29T22:16:23.355610",
    "validator_version": "1.0.0"
  }
}