{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Pipeline Test - Individual Step Testing\n",
        "\n",
        "This notebook tests each pipeline step individually to ensure they work correctly in isolation.\n",
        "\n",
        "**Pipeline Steps:**\n",
        "1. XGBoost Training\n",
        "2. XGBoost Model Evaluation\n",
        "3. Model Calibration\n",
        "\n",
        "**This notebook covers:**\n",
        "- Mock step tester implementation\n",
        "- Individual step testing functions\n",
        "- Step validation and error handling\n",
        "- Output verification and data flow checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Function and Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockStepTester:\n",
        "    \"\"\"Mock implementation for testing individual pipeline steps.\"\"\"\n",
        "    \n",
        "    def __init__(self, workspace_dir):\n",
        "        self.workspace_dir = Path(workspace_dir)\n",
        "        self.execution_times = {}\n",
        "        self.step_results = {}\n",
        "        \n",
        "        # Ensure workspace directory exists\n",
        "        self.workspace_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"MockStepTester initialized with workspace: {self.workspace_dir}\")\n",
        "    \n",
        "    def test_xgboost_training(self, config):\n",
        "        \"\"\"Test XGBoost training step.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"TESTING XGBOOST TRAINING STEP\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Load and validate training data\n",
        "            input_path = Path(config['input_data_path'])\n",
        "            if not input_path.exists():\n",
        "                raise FileNotFoundError(f\"Training data not found: {input_path}\")\n",
        "            \n",
        "            train_data = pd.read_csv(input_path)\n",
        "            print(f\"✓ Loaded training data: {train_data.shape}\")\n",
        "            \n",
        "            # Validate required columns\n",
        "            target_col = config['target_column']\n",
        "            feature_cols = config['feature_columns']\n",
        "            \n",
        "            if target_col not in train_data.columns:\n",
        "                raise ValueError(f\"Target column '{target_col}' not found in data\")\n",
        "            \n",
        "            missing_features = [col for col in feature_cols if col not in train_data.columns]\n",
        "            if missing_features:\n",
        "                raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
        "            \n",
        "            print(f\"✓ Validated columns: {len(feature_cols)} features, target='{target_col}'\")\n",
        "            \n",
        "            # Simulate training process\n",
        "            print(\"Training XGBoost model...\")\n",
        "            time.sleep(1.0)  # Simulate training time\n",
        "            \n",
        "            # Create mock model output\n",
        "            model_path = Path(config['output_model_path'])\n",
        "            model_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "            # Save mock model metadata\n",
        "            model_info = {\n",
        "                'model_type': 'XGBoost',\n",
        "                'hyperparameters': config['hyperparameters'],\n",
        "                'training_samples': len(train_data),\n",
        "                'features': feature_cols,\n",
        "                'target_column': target_col,\n",
        "                'training_timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            \n",
        "            # Save model (mock)\n",
        "            with open(model_path, 'w') as f:\n",
        "                json.dump(model_info, f, indent=2)\n",
        "            \n",
        "            execution_time = time.time() - start_time\n",
        "            self.execution_times['XGBoostTraining'] = execution_time\n",
        "            self.step_results['XGBoostTraining'] = {\n",
        "                'status': 'success',\n",
        "                'model_path': str(model_path),\n",
        "                'training_samples': len(train_data),\n",
        "                'execution_time': execution_time\n",
        "            }\n",
        "            \n",
        "            print(f\"✓ XGBoost Training completed successfully in {execution_time:.2f}s\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            print(f\"✗ XGBoost Training failed: {e}\")\n",
        "            self.execution_times['XGBoostTraining'] = execution_time\n",
        "            self.step_results['XGBoostTraining'] = {\n",
        "                'status': 'failed', \n",
        "                'error': str(e),\n",
        "                'execution_time': execution_time\n",
        "            }\n",
        "            return False\n",
        "    \n",
        "    def test_xgboost_eval(self, config):\n",
        "        \"\"\"Test XGBoost model evaluation step.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"TESTING XGBOOST MODEL EVALUATION STEP\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Check if model exists (from previous step)\n",
        "            model_path = Path(config['model_path'])\n",
        "            if not model_path.exists():\n",
        "                raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "            \n",
        "            # Load evaluation data\n",
        "            eval_path = Path(config['eval_data_path'])\n",
        "            eval_data = pd.read_csv(eval_path)\n",
        "            print(f\"✓ Loaded evaluation data: {eval_data.shape}\")\n",
        "            \n",
        "            # Simulate model evaluation\n",
        "            print(\"Evaluating model on test data...\")\n",
        "            time.sleep(0.5)  # Simulate evaluation time\n",
        "            \n",
        "            # Generate mock predictions\n",
        "            n_samples = len(eval_data)\n",
        "            np.random.seed(42)\n",
        "            predictions_proba = np.random.beta(2, 2, n_samples)\n",
        "            predictions_class = (predictions_proba > 0.5).astype(int)\n",
        "            \n",
        "            # Save predictions\n",
        "            pred_df = pd.DataFrame({\n",
        "                'prediction_proba': predictions_proba,\n",
        "                'prediction_class': predictions_class\n",
        "            })\n",
        "            \n",
        "            pred_path = Path(config['output_predictions_path'])\n",
        "            pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            pred_df.to_csv(pred_path, index=False)\n",
        "            \n",
        "            # Generate mock metrics\n",
        "            metrics = {\n",
        "                'accuracy': 0.85,\n",
        "                'precision': 0.82,\n",
        "                'recall': 0.88,\n",
        "                'f1_score': 0.85,\n",
        "                'auc_roc': 0.91\n",
        "            }\n",
        "            \n",
        "            # Save metrics\n",
        "            metrics_path = Path(config['output_metrics_path'])\n",
        "            with open(metrics_path, 'w') as f:\n",
        "                json.dump(metrics, f, indent=2)\n",
        "            \n",
        "            execution_time = time.time() - start_time\n",
        "            self.execution_times['XGBoostModelEval'] = execution_time\n",
        "            self.step_results['XGBoostModelEval'] = {\n",
        "                'status': 'success',\n",
        "                'metrics': metrics,\n",
        "                'eval_samples': n_samples,\n",
        "                'execution_time': execution_time\n",
        "            }\n",
        "            \n",
        "            print(f\"✓ XGBoost Evaluation completed successfully in {execution_time:.2f}s\")\n",
        "            print(f\"  Accuracy: {metrics['accuracy']:.3f}, AUC-ROC: {metrics['auc_roc']:.3f}\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            print(f\"✗ XGBoost Evaluation failed: {e}\")\n",
        "            self.execution_times['XGBoostModelEval'] = execution_time\n",
        "            self.step_results['XGBoostModelEval'] = {\n",
        "                'status': 'failed', \n",
        "                'error': str(e),\n",
        "                'execution_time': execution_time\n",
        "            }\n",
        "            return False\n",
        "    \n",
        "    def test_model_calibration(self, config):\n",
        "        \"\"\"Test model calibration step.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"TESTING MODEL CALIBRATION STEP\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Load predictions from previous step\n",
        "            pred_path = Path(config['predictions_path'])\n",
        "            predictions = pd.read_csv(pred_path)\n",
        "            print(f\"✓ Loaded predictions: {predictions.shape}\")\n",
        "            \n",
        "            # Simulate calibration process\n",
        "            print(f\"Calibrating predictions using {config['calibration_method']} method...\")\n",
        "            time.sleep(0.3)  # Simulate calibration time\n",
        "            \n",
        "            # Generate mock calibrated predictions\n",
        "            original_proba = predictions['prediction_proba'].values\n",
        "            calibrated_proba = 0.1 + 0.8 * original_proba  # Simple transformation\n",
        "            calibrated_class = (calibrated_proba > 0.5).astype(int)\n",
        "            \n",
        "            # Create calibrated predictions dataframe\n",
        "            calibrated_df = pd.DataFrame({\n",
        "                'original_proba': original_proba,\n",
        "                'calibrated_proba': calibrated_proba,\n",
        "                'calibrated_class': calibrated_class\n",
        "            })\n",
        "            \n",
        "            # Save calibrated predictions\n",
        "            calib_pred_path = Path(config['output_calibrated_predictions_path'])\n",
        "            calib_pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            calibrated_df.to_csv(calib_pred_path, index=False)\n",
        "            \n",
        "            # Save mock calibrated model\n",
        "            calibrated_model_info = {\n",
        "                'calibration_method': config['calibration_method'],\n",
        "                'calibrated_samples': len(predictions),\n",
        "                'calibration_improvement': 0.03\n",
        "            }\n",
        "            \n",
        "            calib_model_path = Path(config['output_calibrated_model_path'])\n",
        "            with open(calib_model_path, 'w') as f:\n",
        "                json.dump(calibrated_model_info, f, indent=2)\n",
        "            \n",
        "            execution_time = time.time() - start_time\n",
        "            self.execution_times['ModelCalibration'] = execution_time\n",
        "            self.step_results['ModelCalibration'] = {\n",
        "                'status': 'success',\n",
        "                'calibration_method': config['calibration_method'],\n",
        "                'calibrated_samples': len(predictions),\n",
        "                'execution_time': execution_time\n",
        "            }\n",
        "            \n",
        "            print(f\"✓ Model Calibration completed successfully in {execution_time:.2f}s\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            print(f\"✗ Model Calibration failed: {e}\")\n",
        "            self.execution_times['ModelCalibration'] = execution_time\n",
        "            self.step_results['ModelCalibration'] = {\n",
        "                'status': 'failed', \n",
        "                'error': str(e),\n",
        "                'execution_time': execution_time\n",
        "            }\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup environment and imports.\"\"\"\n",
        "    print(\"=== SETUP AND IMPORTS ===\")\n",
        "    \n",
        "    # Add cursus to path\n",
        "    sys.path.append(str(Path.cwd().parent.parent.parent / 'src'))\n",
        "    \n",
        "    # Import Cursus components\n",
        "    try:\n",
        "        from cursus.validation.runtime.jupyter.notebook_interface import NotebookInterface\n",
        "        from cursus.validation.runtime.core.data_flow_manager import DataFlowManager\n",
        "        from cursus.steps.registry.step_names import STEP_NAMES\n",
        "        print(\"✓ Successfully imported Cursus components\")\n",
        "        cursus_available = True\n",
        "    except ImportError as e:\n",
        "        print(f\"⚠ Import error: {e}\")\n",
        "        print(\"Using mock implementations for testing...\")\n",
        "        cursus_available = False\n",
        "    \n",
        "    print(f\"Individual step testing started at {datetime.now()}\")\n",
        "    return cursus_available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_configuration():\n",
        "    \"\"\"Load pipeline configuration and validate environment.\"\"\"\n",
        "    print(\"\\n=== LOAD CONFIGURATION AND VALIDATE ENVIRONMENT ===\")\n",
        "    \n",
        "    # Define directory structure\n",
        "    BASE_DIR = Path.cwd()\n",
        "    DATA_DIR = BASE_DIR / 'data'\n",
        "    CONFIG_DIR = BASE_DIR / 'configs'\n",
        "    OUTPUTS_DIR = BASE_DIR / 'outputs'\n",
        "    WORKSPACE_DIR = OUTPUTS_DIR / 'workspace'\n",
        "    LOGS_DIR = OUTPUTS_DIR / 'logs'\n",
        "    RESULTS_DIR = OUTPUTS_DIR / 'results'\n",
        "    \n",
        "    # Load pipeline configuration\n",
        "    pipeline_config_path = CONFIG_DIR / 'pipeline_config.json'\n",
        "    if pipeline_config_path.exists():\n",
        "        with open(pipeline_config_path, 'r') as f:\n",
        "            pipeline_config = json.load(f)\n",
        "        print(f\"✓ Loaded pipeline configuration: {pipeline_config_path}\")\n",
        "        \n",
        "        # Extract step configurations\n",
        "        step_configs = pipeline_config['step_configurations']\n",
        "        pipeline_metadata = pipeline_config['pipeline_metadata']\n",
        "        \n",
        "        print(f\"Pipeline: {pipeline_metadata['name']}\")\n",
        "        print(f\"Steps loaded: {list(step_configs.keys())}\")\n",
        "    else:\n",
        "        print(f\"⚠ Pipeline configuration not found: {pipeline_config_path}\")\n",
        "        print(\"Please run 02_pipeline_configuration.ipynb first!\")\n",
        "        step_configs = {}\n",
        "        pipeline_metadata = {}\n",
        "    \n",
        "    # Validate required files exist\n",
        "    required_files = [\n",
        "        DATA_DIR / 'train_data.csv',\n",
        "        DATA_DIR / 'eval_data.csv',\n",
        "        DATA_DIR / 'dataset_metadata.json'\n",
        "    ]\n",
        "    \n",
        "    missing_files = []\n",
        "    for file_path in required_files:\n",
        "        if file_path.exists():\n",
        "            print(f\"✓ Required file exists: {file_path.name}\")\n",
        "        else:\n",
        "            print(f\"⚠ Required file missing: {file_path}\")\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    if missing_files:\n",
        "        print(\"\\nPlease run previous notebooks to generate required files!\")\n",
        "    else:\n",
        "        print(\"\\n✓ All required files are available for testing\")\n",
        "    \n",
        "    return {\n",
        "        'step_configs': step_configs,\n",
        "        'pipeline_metadata': pipeline_metadata,\n",
        "        'directories': {\n",
        "            'BASE_DIR': BASE_DIR,\n",
        "            'DATA_DIR': DATA_DIR,\n",
        "            'CONFIG_DIR': CONFIG_DIR,\n",
        "            'OUTPUTS_DIR': OUTPUTS_DIR,\n",
        "            'WORKSPACE_DIR': WORKSPACE_DIR,\n",
        "            'LOGS_DIR': LOGS_DIR,\n",
        "            'RESULTS_DIR': RESULTS_DIR\n",
        "        },\n",
        "        'missing_files': missing_files\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_individual_step_tests(config_data):\n",
        "    \"\"\"Run individual step tests.\"\"\"\n",
        "    print(\"\\n=== RUN INDIVIDUAL STEP TESTS ===\")\n",
        "    \n",
        "    step_configs = config_data['step_configs']\n",
        "    directories = config_data['directories']\n",
        "    \n",
        "    if not step_configs:\n",
        "        print(\"Cannot run tests without step configurations!\")\n",
        "        return None\n",
        "    \n",
        "    # Initialize step tester\n",
        "    step_tester = MockStepTester(directories['WORKSPACE_DIR'])\n",
        "    \n",
        "    print(\"RUNNING INDIVIDUAL STEP TESTS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Test Step 1: XGBoost Training\n",
        "    if 'XGBoostTraining' in step_configs:\n",
        "        training_success = step_tester.test_xgboost_training(step_configs['XGBoostTraining'])\n",
        "    else:\n",
        "        print(\"⚠ XGBoostTraining configuration not found\")\n",
        "        training_success = False\n",
        "    \n",
        "    # Test Step 2: XGBoost Model Evaluation (depends on Step 1)\n",
        "    if 'XGBoostModelEval' in step_configs and training_success:\n",
        "        eval_success = step_tester.test_xgboost_eval(step_configs['XGBoostModelEval'])\n",
        "    else:\n",
        "        if not training_success:\n",
        "            print(\"\\n⚠ Skipping XGBoost Evaluation due to training failure\")\n",
        "        else:\n",
        "            print(\"\\n⚠ XGBoostModelEval configuration not found\")\n",
        "        eval_success = False\n",
        "    \n",
        "    # Test Step 3: Model Calibration (depends on Step 2)\n",
        "    if 'ModelCalibration' in step_configs and eval_success:\n",
        "        calibration_success = step_tester.test_model_calibration(step_configs['ModelCalibration'])\n",
        "    else:\n",
        "        if not eval_success:\n",
        "            print(\"\\n⚠ Skipping Model Calibration due to evaluation failure\")\n",
        "        else:\n",
        "            print(\"\\n⚠ ModelCalibration configuration not found\")\n",
        "        calibration_success = False\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INDIVIDUAL STEP TESTS COMPLETED\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return step_tester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_test_summary(step_tester, config_data):\n",
        "    \"\"\"Generate test results summary.\"\"\"\n",
        "    print(\"\\n=== TEST RESULTS SUMMARY ===\")\n",
        "    \n",
        "    if not step_tester:\n",
        "        print(\"Cannot generate summary without test results!\")\n",
        "        return\n",
        "    \n",
        "    step_configs = config_data['step_configs']\n",
        "    pipeline_metadata = config_data['pipeline_metadata']\n",
        "    directories = config_data['directories']\n",
        "    \n",
        "    print(\"INDIVIDUAL STEP TEST SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    total_steps = len(step_configs)\n",
        "    successful_steps = sum(1 for result in step_tester.step_results.values() \n",
        "                          if result['status'] == 'success')\n",
        "    failed_steps = total_steps - successful_steps\n",
        "    success_rate = successful_steps / total_steps * 100 if total_steps > 0 else 0\n",
        "    total_time = sum(step_tester.execution_times.values())\n",
        "    \n",
        "    print(f\"Total Steps: {total_steps}\")\n",
        "    print(f\"Successful Steps: {successful_steps}\")\n",
        "    print(f\"Failed Steps: {failed_steps}\")\n",
        "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
        "    print(f\"Total Execution Time: {total_time:.2f}s\")\n",
        "    \n",
        "    print(\"\\nStep-by-Step Results:\")\n",
        "    for step_name in ['XGBoostTraining', 'XGBoostModelEval', 'ModelCalibration']:\n",
        "        if step_name in step_tester.step_results:\n",
        "            result = step_tester.step_results[step_name]\n",
        "            exec_time = step_tester.execution_times.get(step_name, 0)\n",
        "            status_icon = \"✓\" if result['status'] == 'success' else \"✗\"\n",
        "            print(f\"  {status_icon} {step_name}: {result['status']} ({exec_time:.2f}s)\")\n",
        "            \n",
        "            if result['status'] == 'failed' and 'error' in result:\n",
        "                print(f\"    Error: {result['error']}\")\n",
        "            elif result['status'] == 'success':\n",
        "                if step_name == 'XGBoostTraining':\n",
        "                    print(f\"    Training samples: {result.get('training_samples', 'N/A')}\")\n",
        "                elif step_name == 'XGBoostModelEval':\n",
        "                    metrics = result.get('metrics', {})\n",
        "                    print(f\"    Accuracy: {metrics.get('accuracy', 'N/A'):.3f}\")\n",
        "                    print(f\"    AUC-ROC: {metrics.get('auc_roc', 'N/A'):.3f}\")\n",
        "                elif step_name == 'ModelCalibration':\n",
        "                    print(f\"    Method: {result.get('calibration_method', 'N/A')}\")\n",
        "                    print(f\"    Samples: {result.get('calibrated_samples', 'N/A')}\")\n",
        "        else:\n",
        "            print(f\"  - {step_name}: not tested\")\n",
        "    \n",
        "    # Save test results\n",
        "    test_results = {\n",
        "        'test_timestamp': datetime.now().isoformat(),\n",
        "        'test_type': 'individual_step_testing',\n",
        "        'pipeline_name': pipeline_metadata.get('name', 'Unknown'),\n",
        "        'total_steps': total_steps,\n",
        "        'successful_steps': successful_steps,\n",
        "        'failed_steps': failed_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'total_execution_time': total_time,\n",
        "        'step_results': step_tester.step_results,\n",
        "        'execution_times': step_tester.execution_times\n",
        "    }\n",
        "    \n",
        "    results_path = directories['RESULTS_DIR'] / 'individual_step_test_results.json'\n",
        "    results_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(test_results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n✓ Test results saved: {results_path}\")\n",
        "    \n",
        "    if success_rate == 100:\n",
        "        print(\"\\n🎉 All individual step tests passed!\")\n",
        "        print(\"Ready for end-to-end pipeline testing.\")\n",
        "        print(\"Next: Run 04_end_to_end_pipeline_test.ipynb\")\n",
        "    else:\n",
        "        print(f\"\\n⚠ {failed_steps} step(s) failed. Please review and fix issues before proceeding.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_output_files(directories):\n",
        "    \"\"\"Verify that expected output files were created.\"\"\"\n",
        "    print(\"\\n=== OUTPUT FILE VERIFICATION ===\")\n",
        "    \n",
        "    WORKSPACE_DIR = directories['WORKSPACE_DIR']\n",
        "    \n",
        "    print(\"OUTPUT FILE VERIFICATION\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    expected_outputs = [\n",
        "        WORKSPACE_DIR / 'xgboost_model.pkl',\n",
        "        WORKSPACE_DIR / 'predictions.csv',\n",
        "        WORKSPACE_DIR / 'eval_metrics.json',\n",
        "        WORKSPACE_DIR / 'calibrated_model.pkl',\n",
        "        WORKSPACE_DIR / 'calibrated_predictions.csv'\n",
        "    ]\n",
        "    \n",
        "    created_files = []\n",
        "    missing_files = []\n",
        "    \n",
        "    for file_path in expected_outputs:\n",
        "        if file_path.exists():\n",
        "            file_size = file_path.stat().st_size\n",
        "            print(f\"✓ {file_path.name} ({file_size} bytes)\")\n",
        "            created_files.append(file_path)\n",
        "        else:\n",
        "            print(f\"✗ {file_path.name} (missing)\")\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    print(f\"\\nFiles created: {len(created_files)}/{len(expected_outputs)}\")\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"Missing files: {[f.name for f in missing_files]}\")\n",
        "    else:\n",
        "        print(\"All expected output files were created successfully!\")\n",
        "    \n",
        "    # Show workspace contents\n",
        "    print(f\"\\nWorkspace contents ({WORKSPACE_DIR}):\")\n",
        "    if WORKSPACE_DIR.exists():\n",
        "        workspace_files = list(WORKSPACE_DIR.glob('*'))\n",
        "        for file_path in sorted(workspace_files):\n",
        "            if file_path.is_file():\n",
        "                size = file_path.stat().st_size\n",
        "                print(f\"  {file_path.name} ({size} bytes)\")\n",
        "    else:\n",
        "        print(\"  Workspace directory does not exist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Script Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Main function to run all tests.\"\"\"\n",
        "\n",
        "print(\"XGBoost Pipeline Individual Step Testing\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "cursus_available = setup_environment()\n",
        "\n",
        "config_data = load_configuration()\n",
        "\n",
        "if config_data['missing_files']:\n",
        "    print(\"Cannot proceed with missing required files!\")\n",
        "\n",
        "step_tester = run_individual_step_tests(config_data)\n",
        "\n",
        "generate_test_summary(step_tester, config_data)\n",
        "\n",
        "verify_output_files(config_data['directories'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "print(\"INDIVIDUAL STEP TESTING COMPLETED\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "XGBoost Pipeline Individual Step Testing Script\n",
        "\n",
        "This script contains all the code for testing individual pipeline steps.\n",
        "It can be run directly or converted to a Jupyter notebook.\n",
        "\"\"\"\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}