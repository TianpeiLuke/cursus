{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Pipeline - End-to-End Testing\n",
        "\n",
        "This notebook performs comprehensive end-to-end testing of the complete XGBoost pipeline.\n",
        "\n",
        "**Pipeline Components:**\n",
        "1. Pipeline DAG validation and dependency resolution\n",
        "2. Sequential step execution with proper data flow\n",
        "3. Error handling and rollback mechanisms\n",
        "4. Output verification and result analysis\n",
        "\n",
        "**This notebook covers:**\n",
        "- Complete pipeline execution from start to finish\n",
        "- Dependency validation and execution ordering\n",
        "- Data flow verification between steps\n",
        "- Comprehensive result analysis and reporting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Function and Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EndToEndPipelineExecutor:\n",
        "    \"\"\"End-to-end pipeline executor with dependency validation and error handling.\"\"\"\n",
        "    \n",
        "    def __init__(self, config_data):\n",
        "        self.config_data = config_data\n",
        "        self.step_configurations = config_data['step_configurations']\n",
        "        self.pipeline_dag = config_data['pipeline_dag']\n",
        "        self.directories = config_data['directories']\n",
        "        \n",
        "        # Execution tracking\n",
        "        self.execution_results = {}\n",
        "        self.execution_times = {}\n",
        "        self.pipeline_start_time = None\n",
        "        self.pipeline_end_time = None\n",
        "        \n",
        "        # Ensure workspace exists\n",
        "        self.directories['WORKSPACE_DIR'].mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"EndToEndPipelineExecutor initialized with workspace: {self.directories['WORKSPACE_DIR']}\")\n",
        "    \n",
        "    def validate_pipeline_dag(self):\n",
        "        \"\"\"Validate pipeline DAG structure and dependencies.\"\"\"\n",
        "        print(\"\\n=== VALIDATE PIPELINE DAG ===\")\n",
        "        \n",
        "        steps = list(self.step_configurations.keys())\n",
        "        edges = self.pipeline_dag.get('edges', [])\n",
        "        \n",
        "        print(f\"Pipeline steps: {steps}\")\n",
        "        print(f\"Pipeline edges: {edges}\")\n",
        "        \n",
        "        # Validate all steps in edges exist in configurations\n",
        "        edge_steps = set()\n",
        "        for edge in edges:\n",
        "            edge_steps.add(edge['from'])\n",
        "            edge_steps.add(edge['to'])\n",
        "        \n",
        "        missing_steps = edge_steps - set(steps)\n",
        "        if missing_steps:\n",
        "            raise ValueError(f\"Steps in DAG edges not found in configurations: {missing_steps}\")\n",
        "        \n",
        "        # Check for cycles (simple check)\n",
        "        dependencies = {}\n",
        "        for edge in edges:\n",
        "            if edge['to'] not in dependencies:\n",
        "                dependencies[edge['to']] = []\n",
        "            dependencies[edge['to']].append(edge['from'])\n",
        "        \n",
        "        print(\"âœ“ Pipeline DAG validation passed\")\n",
        "        return dependencies\n",
        "    \n",
        "    def get_execution_order(self, dependencies):\n",
        "        \"\"\"Determine execution order based on dependencies.\"\"\"\n",
        "        print(\"\\n=== DETERMINE EXECUTION ORDER ===\")\n",
        "        \n",
        "        steps = list(self.step_configurations.keys())\n",
        "        execution_order = []\n",
        "        remaining_steps = set(steps)\n",
        "        \n",
        "        while remaining_steps:\n",
        "            # Find steps with no unresolved dependencies\n",
        "            ready_steps = []\n",
        "            for step in remaining_steps:\n",
        "                step_deps = dependencies.get(step, [])\n",
        "                if all(dep in execution_order for dep in step_deps):\n",
        "                    ready_steps.append(step)\n",
        "            \n",
        "            if not ready_steps:\n",
        "                raise ValueError(f\"Circular dependency detected. Remaining steps: {remaining_steps}\")\n",
        "            \n",
        "            # Add ready steps to execution order\n",
        "            for step in ready_steps:\n",
        "                execution_order.append(step)\n",
        "                remaining_steps.remove(step)\n",
        "        \n",
        "        print(f\"Execution order: {execution_order}\")\n",
        "        return execution_order\n",
        "    \n",
        "    def execute_step(self, step_name, step_config):\n",
        "        \"\"\"Execute a single pipeline step.\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EXECUTING STEP: {step_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            if step_name == 'XGBoostTraining':\n",
        "                success = self._execute_xgboost_training(step_config)\n",
        "            elif step_name == 'XGBoostModelEval':\n",
        "                success = self._execute_xgboost_eval(step_config)\n",
        "            elif step_name == 'ModelCalibration':\n",
        "                success = self._execute_model_calibration(step_config)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown step: {step_name}\")\n",
        "            \n",
        "            execution_time = time.time() - start_time\n",
        "            self.execution_times[step_name] = execution_time\n",
        "            \n",
        "            if success:\n",
        "                self.execution_results[step_name] = {\n",
        "                    'status': 'success',\n",
        "                    'execution_time': execution_time,\n",
        "                    'timestamp': datetime.now().isoformat()\n",
        "                }\n",
        "                print(f\"âœ“ {step_name} completed successfully in {execution_time:.2f}s\")\n",
        "                return True\n",
        "            else:\n",
        "                self.execution_results[step_name] = {\n",
        "                    'status': 'failed',\n",
        "                    'execution_time': execution_time,\n",
        "                    'timestamp': datetime.now().isoformat(),\n",
        "                    'error': 'Step execution failed'\n",
        "                }\n",
        "                print(f\"âœ— {step_name} failed after {execution_time:.2f}s\")\n",
        "                return False\n",
        "                \n",
        "        except Exception as e:\n",
        "            execution_time = time.time() - start_time\n",
        "            self.execution_times[step_name] = execution_time\n",
        "            self.execution_results[step_name] = {\n",
        "                'status': 'error',\n",
        "                'execution_time': execution_time,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'error': str(e)\n",
        "            }\n",
        "            print(f\"âœ— {step_name} error after {execution_time:.2f}s: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def _execute_xgboost_training(self, config):\n",
        "        \"\"\"Execute XGBoost training step.\"\"\"\n",
        "        # Load training data\n",
        "        input_path = Path(config['input_data_path'])\n",
        "        train_data = pd.read_csv(input_path)\n",
        "        print(f\"âœ“ Loaded training data: {train_data.shape}\")\n",
        "        \n",
        "        # Validate columns\n",
        "        target_col = config['target_column']\n",
        "        feature_cols = config['feature_columns']\n",
        "        \n",
        "        if target_col not in train_data.columns:\n",
        "            raise ValueError(f\"Target column '{target_col}' not found\")\n",
        "        \n",
        "        missing_features = [col for col in feature_cols if col not in train_data.columns]\n",
        "        if missing_features:\n",
        "            raise ValueError(f\"Missing feature columns: {missing_features}\")\n",
        "        \n",
        "        # Simulate training\n",
        "        print(\"Training XGBoost model...\")\n",
        "        time.sleep(2.0)  # Simulate longer training time\n",
        "        \n",
        "        # Save model\n",
        "        model_path = Path(config['output_model_path'])\n",
        "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        model_info = {\n",
        "            'model_type': 'XGBoost',\n",
        "            'hyperparameters': config['hyperparameters'],\n",
        "            'training_samples': len(train_data),\n",
        "            'features': feature_cols,\n",
        "            'target_column': target_col,\n",
        "            'training_timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        with open(model_path, 'w') as f:\n",
        "            json.dump(model_info, f, indent=2)\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def _execute_xgboost_eval(self, config):\n",
        "        \"\"\"Execute XGBoost model evaluation step.\"\"\"\n",
        "        # Check model exists\n",
        "        model_path = Path(config['model_path'])\n",
        "        if not model_path.exists():\n",
        "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "        \n",
        "        # Load evaluation data\n",
        "        eval_path = Path(config['eval_data_path'])\n",
        "        eval_data = pd.read_csv(eval_path)\n",
        "        print(f\"âœ“ Loaded evaluation data: {eval_data.shape}\")\n",
        "        \n",
        "        # Simulate evaluation\n",
        "        print(\"Evaluating model...\")\n",
        "        time.sleep(1.0)\n",
        "        \n",
        "        # Generate predictions\n",
        "        n_samples = len(eval_data)\n",
        "        np.random.seed(42)\n",
        "        predictions_proba = np.random.beta(2, 2, n_samples)\n",
        "        predictions_class = (predictions_proba > 0.5).astype(int)\n",
        "        \n",
        "        pred_df = pd.DataFrame({\n",
        "            'prediction_proba': predictions_proba,\n",
        "            'prediction_class': predictions_class\n",
        "        })\n",
        "        \n",
        "        pred_path = Path(config['output_predictions_path'])\n",
        "        pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        pred_df.to_csv(pred_path, index=False)\n",
        "        \n",
        "        # Generate metrics\n",
        "        metrics = {\n",
        "            'accuracy': 0.87,\n",
        "            'precision': 0.84,\n",
        "            'recall': 0.90,\n",
        "            'f1_score': 0.87,\n",
        "            'auc_roc': 0.93\n",
        "        }\n",
        "        \n",
        "        metrics_path = Path(config['output_metrics_path'])\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        \n",
        "        print(f\"  Accuracy: {metrics['accuracy']:.3f}, AUC-ROC: {metrics['auc_roc']:.3f}\")\n",
        "        return True\n",
        "    \n",
        "    def _execute_model_calibration(self, config):\n",
        "        \"\"\"Execute model calibration step.\"\"\"\n",
        "        # Load predictions\n",
        "        pred_path = Path(config['predictions_path'])\n",
        "        predictions = pd.read_csv(pred_path)\n",
        "        print(f\"âœ“ Loaded predictions: {predictions.shape}\")\n",
        "        \n",
        "        # Simulate calibration\n",
        "        print(f\"Calibrating predictions using {config['calibration_method']}...\")\n",
        "        time.sleep(0.5)\n",
        "        \n",
        "        # Generate calibrated predictions\n",
        "        original_proba = predictions['prediction_proba'].values\n",
        "        calibrated_proba = 0.15 + 0.7 * original_proba  # Different transformation\n",
        "        calibrated_class = (calibrated_proba > 0.5).astype(int)\n",
        "        \n",
        "        calibrated_df = pd.DataFrame({\n",
        "            'original_proba': original_proba,\n",
        "            'calibrated_proba': calibrated_proba,\n",
        "            'calibrated_class': calibrated_class\n",
        "        })\n",
        "        \n",
        "        calib_pred_path = Path(config['output_calibrated_predictions_path'])\n",
        "        calib_pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        calibrated_df.to_csv(calib_pred_path, index=False)\n",
        "        \n",
        "        # Save calibrated model\n",
        "        calibrated_model_info = {\n",
        "            'calibration_method': config['calibration_method'],\n",
        "            'calibrated_samples': len(predictions),\n",
        "            'calibration_improvement': 0.05\n",
        "        }\n",
        "        \n",
        "        calib_model_path = Path(config['output_calibrated_model_path'])\n",
        "        with open(calib_model_path, 'w') as f:\n",
        "            json.dump(calibrated_model_info, f, indent=2)\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    def execute_pipeline(self):\n",
        "        \"\"\"Execute the complete pipeline.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"STARTING END-TO-END PIPELINE EXECUTION\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        self.pipeline_start_time = time.time()\n",
        "        \n",
        "        try:\n",
        "            # Validate DAG\n",
        "            dependencies = self.validate_pipeline_dag()\n",
        "            \n",
        "            # Determine execution order\n",
        "            execution_order = self.get_execution_order(dependencies)\n",
        "            \n",
        "            # Execute steps in order\n",
        "            for step_name in execution_order:\n",
        "                step_config = self.step_configurations[step_name]\n",
        "                success = self.execute_step(step_name, step_config)\n",
        "                \n",
        "                if not success:\n",
        "                    print(f\"\\nâš  Pipeline execution stopped due to failure in {step_name}\")\n",
        "                    break\n",
        "            \n",
        "            self.pipeline_end_time = time.time()\n",
        "            total_time = self.pipeline_end_time - self.pipeline_start_time\n",
        "            \n",
        "            print(f\"\\n\" + \"=\"*80)\n",
        "            print(\"PIPELINE EXECUTION COMPLETED\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Total execution time: {total_time:.2f}s\")\n",
        "            \n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            self.pipeline_end_time = time.time()\n",
        "            print(f\"\\nâœ— Pipeline execution failed: {e}\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup environment and imports.\"\"\"\n",
        "    print(\"=== END-TO-END PIPELINE TESTING SETUP ===\")\n",
        "    \n",
        "    # Add cursus to path\n",
        "    sys.path.append(str(Path.cwd().parent.parent.parent / 'src'))\n",
        "    \n",
        "    # Import Cursus components\n",
        "    try:\n",
        "        from cursus.validation.runtime.jupyter.notebook_interface import NotebookInterface\n",
        "        from cursus.validation.runtime.core.data_flow_manager import DataFlowManager\n",
        "        from cursus.validation.runtime.core.pipeline_executor import PipelineExecutor\n",
        "        from cursus.steps.registry.step_names import STEP_NAMES\n",
        "        print(\"âœ“ Successfully imported Cursus components\")\n",
        "        cursus_available = True\n",
        "    except ImportError as e:\n",
        "        print(f\"âš  Import error: {e}\")\n",
        "        print(\"Using mock implementations for testing...\")\n",
        "        cursus_available = False\n",
        "    \n",
        "    print(f\"End-to-end pipeline testing started at {datetime.now()}\")\n",
        "    return cursus_available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pipeline_configuration():\n",
        "    \"\"\"Load and validate pipeline configuration.\"\"\"\n",
        "    print(\"\\n=== LOAD PIPELINE CONFIGURATION ===\")\n",
        "    \n",
        "    # Define directory structure\n",
        "    BASE_DIR = Path.cwd()\n",
        "    DATA_DIR = BASE_DIR / 'data'\n",
        "    CONFIG_DIR = BASE_DIR / 'configs'\n",
        "    OUTPUTS_DIR = BASE_DIR / 'outputs'\n",
        "    WORKSPACE_DIR = OUTPUTS_DIR / 'workspace'\n",
        "    LOGS_DIR = OUTPUTS_DIR / 'logs'\n",
        "    RESULTS_DIR = OUTPUTS_DIR / 'results'\n",
        "    \n",
        "    # Load pipeline configuration\n",
        "    pipeline_config_path = CONFIG_DIR / 'pipeline_config.json'\n",
        "    if not pipeline_config_path.exists():\n",
        "        raise FileNotFoundError(f\"Pipeline configuration not found: {pipeline_config_path}\")\n",
        "    \n",
        "    with open(pipeline_config_path, 'r') as f:\n",
        "        pipeline_config = json.load(f)\n",
        "    \n",
        "    print(f\"âœ“ Loaded pipeline configuration: {pipeline_config_path}\")\n",
        "    \n",
        "    # Extract components\n",
        "    step_configurations = pipeline_config['step_configurations']\n",
        "    pipeline_metadata = pipeline_config['pipeline_metadata']\n",
        "    pipeline_dag = pipeline_config.get('pipeline_dag', {})\n",
        "    \n",
        "    print(f\"Pipeline: {pipeline_metadata['name']}\")\n",
        "    print(f\"Steps: {list(step_configurations.keys())}\")\n",
        "    print(f\"Dependencies: {pipeline_dag.get('edges', [])}\")\n",
        "    \n",
        "    # Validate required files exist\n",
        "    required_files = [\n",
        "        DATA_DIR / 'train_data.csv',\n",
        "        DATA_DIR / 'eval_data.csv',\n",
        "        DATA_DIR / 'dataset_metadata.json'\n",
        "    ]\n",
        "    \n",
        "    missing_files = []\n",
        "    for file_path in required_files:\n",
        "        if file_path.exists():\n",
        "            print(f\"âœ“ Required file exists: {file_path.name}\")\n",
        "        else:\n",
        "            print(f\"âš  Required file missing: {file_path}\")\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(f\"Missing required files: {[f.name for f in missing_files]}\")\n",
        "    \n",
        "    return {\n",
        "        'step_configurations': step_configurations,\n",
        "        'pipeline_metadata': pipeline_metadata,\n",
        "        'pipeline_dag': pipeline_dag,\n",
        "        'directories': {\n",
        "            'BASE_DIR': BASE_DIR,\n",
        "            'DATA_DIR': DATA_DIR,\n",
        "            'CONFIG_DIR': CONFIG_DIR,\n",
        "            'OUTPUTS_DIR': OUTPUTS_DIR,\n",
        "            'WORKSPACE_DIR': WORKSPACE_DIR,\n",
        "            'LOGS_DIR': LOGS_DIR,\n",
        "            'RESULTS_DIR': RESULTS_DIR\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_pipeline_results(executor, config_data):\n",
        "    \"\"\"Generate comprehensive pipeline execution results.\"\"\"\n",
        "    print(\"\\n=== GENERATE PIPELINE RESULTS ===\")\n",
        "    \n",
        "    total_time = executor.pipeline_end_time - executor.pipeline_start_time if executor.pipeline_end_time else 0\n",
        "    successful_steps = sum(1 for result in executor.execution_results.values() \n",
        "                          if result['status'] == 'success')\n",
        "    total_steps = len(executor.execution_results)\n",
        "    success_rate = successful_steps / total_steps * 100 if total_steps > 0 else 0\n",
        "    \n",
        "    print(\"END-TO-END PIPELINE RESULTS\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Total Steps: {total_steps}\")\n",
        "    print(f\"Successful Steps: {successful_steps}\")\n",
        "    print(f\"Failed Steps: {total_steps - successful_steps}\")\n",
        "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
        "    print(f\"Total Pipeline Time: {total_time:.2f}s\")\n",
        "    \n",
        "    print(\"\\nStep-by-Step Results:\")\n",
        "    for step_name, result in executor.execution_results.items():\n",
        "        status_icon = \"âœ“\" if result['status'] == 'success' else \"âœ—\"\n",
        "        exec_time = result['execution_time']\n",
        "        print(f\"  {status_icon} {step_name}: {result['status']} ({exec_time:.2f}s)\")\n",
        "        \n",
        "        if result['status'] != 'success' and 'error' in result:\n",
        "            print(f\"    Error: {result['error']}\")\n",
        "    \n",
        "    # Save results\n",
        "    pipeline_results = {\n",
        "        'test_timestamp': datetime.now().isoformat(),\n",
        "        'test_type': 'end_to_end_pipeline_execution',\n",
        "        'pipeline_name': config_data['pipeline_metadata'].get('name', 'Unknown'),\n",
        "        'total_steps': total_steps,\n",
        "        'successful_steps': successful_steps,\n",
        "        'failed_steps': total_steps - successful_steps,\n",
        "        'success_rate': success_rate,\n",
        "        'total_pipeline_time': total_time,\n",
        "        'step_results': executor.execution_results,\n",
        "        'execution_times': executor.execution_times,\n",
        "        'pipeline_start_time': executor.pipeline_start_time,\n",
        "        'pipeline_end_time': executor.pipeline_end_time\n",
        "    }\n",
        "    \n",
        "    results_path = config_data['directories']['RESULTS_DIR'] / 'end_to_end_pipeline_results.json'\n",
        "    results_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(pipeline_results, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nâœ“ Pipeline results saved: {results_path}\")\n",
        "    \n",
        "    if success_rate == 100:\n",
        "        print(\"\\nðŸŽ‰ End-to-end pipeline execution successful!\")\n",
        "        print(\"All steps completed successfully with proper dependency handling.\")\n",
        "    else:\n",
        "        print(f\"\\nâš  Pipeline execution incomplete. {total_steps - successful_steps} step(s) failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_pipeline_outputs():\n",
        "    \"\"\"Verify all expected pipeline outputs were created.\"\"\"\n",
        "    print(\"\\n=== VERIFY PIPELINE OUTPUTS ===\")\n",
        "    \n",
        "    BASE_DIR = Path.cwd()\n",
        "    WORKSPACE_DIR = BASE_DIR / 'outputs' / 'workspace'\n",
        "    \n",
        "    expected_outputs = [\n",
        "        WORKSPACE_DIR / 'xgboost_model.pkl',\n",
        "        WORKSPACE_DIR / 'predictions.csv',\n",
        "        WORKSPACE_DIR / 'eval_metrics.json',\n",
        "        WORKSPACE_DIR / 'calibrated_model.pkl',\n",
        "        WORKSPACE_DIR / 'calibrated_predictions.csv'\n",
        "    ]\n",
        "    \n",
        "    print(\"PIPELINE OUTPUT VERIFICATION\")\n",
        "    print(\"=\"*35)\n",
        "    \n",
        "    created_files = []\n",
        "    missing_files = []\n",
        "    \n",
        "    for file_path in expected_outputs:\n",
        "        if file_path.exists():\n",
        "            file_size = file_path.stat().st_size\n",
        "            print(f\"âœ“ {file_path.name} ({file_size} bytes)\")\n",
        "            created_files.append(file_path)\n",
        "        else:\n",
        "            print(f\"âœ— {file_path.name} (missing)\")\n",
        "            missing_files.append(file_path)\n",
        "    \n",
        "    print(f\"\\nFiles created: {len(created_files)}/{len(expected_outputs)}\")\n",
        "    \n",
        "    if missing_files:\n",
        "        print(f\"Missing files: {[f.name for f in missing_files]}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(\"âœ“ All expected pipeline outputs were created successfully!\")\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_end_to_end_pipeline_test():\n",
        "    \"\"\"Run the complete end-to-end pipeline test.\"\"\"\n",
        "    print(\"STARTING END-TO-END PIPELINE TEST\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    try:\n",
        "        # Load configuration\n",
        "        config_data = load_pipeline_configuration()\n",
        "        \n",
        "        # Create executor\n",
        "        executor = EndToEndPipelineExecutor(config_data)\n",
        "        \n",
        "        # Execute pipeline\n",
        "        success = executor.execute_pipeline()\n",
        "        \n",
        "        # Generate results\n",
        "        generate_pipeline_results(executor, config_data)\n",
        "        \n",
        "        return success\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âœ— End-to-end test failed: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Script Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Main execution function.\"\"\"\n",
        "\n",
        "cursus_available = setup_environment()\n",
        "\n",
        "success = run_end_to_end_pipeline_test()\n",
        "\n",
        "outputs_verified = verify_pipeline_outputs()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"END-TO-END PIPELINE TEST SUMMARY\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "if success and outputs_verified:\n",
        "    print(\"ðŸŽ‰ End-to-end pipeline test PASSED!\")\n",
        "    print(\"âœ“ Pipeline executed successfully\")\n",
        "    print(\"âœ“ All outputs verified\")\n",
        "    print(\"\\nReady for performance analysis!\")\n",
        "else:\n",
        "    print(\"âš  End-to-end pipeline test FAILED!\")\n",
        "    if not success:\n",
        "        print(\"âœ— Pipeline execution failed\")\n",
        "    if not outputs_verified:\n",
        "        print(\"âœ— Output verification failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "End-to-End Pipeline Testing Script\n",
        "\n",
        "This script tests the complete XGBoost 3-step pipeline execution with proper\n",
        "dependency validation, data flow verification, and error handling.\n",
        "\"\"\"\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}