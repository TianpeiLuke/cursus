{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Pipeline - Performance Analysis\n",
        "\n",
        "This notebook analyzes the performance characteristics of the XGBoost pipeline execution.\n",
        "\n",
        "**Analysis Components:**\n",
        "1. Execution time analysis across all pipeline steps\n",
        "2. Resource utilization and bottleneck identification\n",
        "3. Performance metrics and trend analysis\n",
        "4. Optimization recommendations\n",
        "\n",
        "**This notebook covers:**\n",
        "- Step-by-step performance profiling\n",
        "- Resource usage analysis\n",
        "- Performance visualization and reporting\n",
        "- Optimization insights and recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Function and Class Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerformanceAnalyzer:\n",
        "    \"\"\"Comprehensive performance analyzer for pipeline testing results.\"\"\"\n",
        "    \n",
        "    def __init__(self, results_data):\n",
        "        self.results_data = results_data\n",
        "        self.analysis_results = {}\n",
        "        \n",
        "        # Create output directory for visualizations\n",
        "        self.viz_dir = Path.cwd() / 'outputs' / 'visualizations'\n",
        "        self.viz_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        print(f\"PerformanceAnalyzer initialized with visualization output: {self.viz_dir}\")\n",
        "    \n",
        "    def analyze_execution_times(self):\n",
        "        \"\"\"Analyze execution times across different test modes.\"\"\"\n",
        "        print(\"\\n=== ANALYZE EXECUTION TIMES ===\")\n",
        "        \n",
        "        execution_data = []\n",
        "        \n",
        "        # Extract individual step testing times\n",
        "        if 'individual_step_testing' in self.results_data:\n",
        "            individual_results = self.results_data['individual_step_testing']\n",
        "            for step_name, exec_time in individual_results.get('execution_times', {}).items():\n",
        "                execution_data.append({\n",
        "                    'step_name': step_name,\n",
        "                    'execution_time': exec_time,\n",
        "                    'test_mode': 'Individual Step Testing',\n",
        "                    'status': individual_results['step_results'][step_name]['status']\n",
        "                })\n",
        "        \n",
        "        # Extract end-to-end pipeline times\n",
        "        if 'end_to_end_pipeline' in self.results_data:\n",
        "            pipeline_results = self.results_data['end_to_end_pipeline']\n",
        "            for step_name, exec_time in pipeline_results.get('execution_times', {}).items():\n",
        "                execution_data.append({\n",
        "                    'step_name': step_name,\n",
        "                    'execution_time': exec_time,\n",
        "                    'test_mode': 'End-to-End Pipeline',\n",
        "                    'status': pipeline_results['step_results'][step_name]['status']\n",
        "                })\n",
        "        \n",
        "        if not execution_data:\n",
        "            print(\"⚠ No execution time data available for analysis\")\n",
        "            return\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(execution_data)\n",
        "        \n",
        "        # Generate execution time analysis\n",
        "        print(\"EXECUTION TIME ANALYSIS\")\n",
        "        print(\"=\"*30)\n",
        "        \n",
        "        # Summary statistics\n",
        "        summary_stats = df.groupby(['test_mode', 'step_name'])['execution_time'].agg([\n",
        "            'mean', 'std', 'min', 'max'\n",
        "        ]).round(3)\n",
        "        \n",
        "        print(\"Summary Statistics (seconds):\")\n",
        "        print(summary_stats)\n",
        "        \n",
        "        # Store analysis results\n",
        "        self.analysis_results['execution_times'] = {\n",
        "            'raw_data': execution_data,\n",
        "            'summary_statistics': summary_stats.to_dict(),\n",
        "            'total_individual_time': df[df['test_mode'] == 'Individual Step Testing']['execution_time'].sum(),\n",
        "            'total_pipeline_time': df[df['test_mode'] == 'End-to-End Pipeline']['execution_time'].sum()\n",
        "        }\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def analyze_success_rates(self):\n",
        "        \"\"\"Analyze success rates across different test modes.\"\"\"\n",
        "        print(\"\\n=== ANALYZE SUCCESS RATES ===\")\n",
        "        \n",
        "        success_data = []\n",
        "        \n",
        "        # Individual step testing success rates\n",
        "        if 'individual_step_testing' in self.results_data:\n",
        "            individual_results = self.results_data['individual_step_testing']\n",
        "            success_data.append({\n",
        "                'test_mode': 'Individual Step Testing',\n",
        "                'total_steps': individual_results['total_steps'],\n",
        "                'successful_steps': individual_results['successful_steps'],\n",
        "                'failed_steps': individual_results['failed_steps'],\n",
        "                'success_rate': individual_results['success_rate']\n",
        "            })\n",
        "        \n",
        "        # End-to-end pipeline success rates\n",
        "        if 'end_to_end_pipeline' in self.results_data:\n",
        "            pipeline_results = self.results_data['end_to_end_pipeline']\n",
        "            success_data.append({\n",
        "                'test_mode': 'End-to-End Pipeline',\n",
        "                'total_steps': pipeline_results['total_steps'],\n",
        "                'successful_steps': pipeline_results['successful_steps'],\n",
        "                'failed_steps': pipeline_results['failed_steps'],\n",
        "                'success_rate': pipeline_results['success_rate']\n",
        "            })\n",
        "        \n",
        "        if not success_data:\n",
        "            print(\"⚠ No success rate data available for analysis\")\n",
        "            return\n",
        "        \n",
        "        print(\"SUCCESS RATE ANALYSIS\")\n",
        "        print(\"=\"*25)\n",
        "        \n",
        "        for data in success_data:\n",
        "            print(f\"\\n{data['test_mode']}:\")\n",
        "            print(f\"  Total Steps: {data['total_steps']}\")\n",
        "            print(f\"  Successful: {data['successful_steps']}\")\n",
        "            print(f\"  Failed: {data['failed_steps']}\")\n",
        "            print(f\"  Success Rate: {data['success_rate']:.1f}%\")\n",
        "        \n",
        "        self.analysis_results['success_rates'] = success_data\n",
        "        return success_data\n",
        "    \n",
        "    def analyze_data_flow(self):\n",
        "        \"\"\"Analyze data flow and file sizes throughout the pipeline.\"\"\"\n",
        "        print(\"\\n=== ANALYZE DATA FLOW ===\")\n",
        "        \n",
        "        BASE_DIR = Path.cwd()\n",
        "        WORKSPACE_DIR = BASE_DIR / 'outputs' / 'workspace'\n",
        "        DATA_DIR = BASE_DIR / 'data'\n",
        "        \n",
        "        file_analysis = []\n",
        "        \n",
        "        # Analyze input data files\n",
        "        input_files = [\n",
        "            ('train_data.csv', DATA_DIR / 'train_data.csv'),\n",
        "            ('eval_data.csv', DATA_DIR / 'eval_data.csv')\n",
        "        ]\n",
        "        \n",
        "        for file_name, file_path in input_files:\n",
        "            if file_path.exists():\n",
        "                file_size = file_path.stat().st_size\n",
        "                file_analysis.append({\n",
        "                    'file_name': file_name,\n",
        "                    'file_path': str(file_path),\n",
        "                    'file_size_bytes': file_size,\n",
        "                    'file_size_kb': file_size / 1024,\n",
        "                    'file_type': 'input_data'\n",
        "                })\n",
        "        \n",
        "        # Analyze output files\n",
        "        output_files = [\n",
        "            ('xgboost_model.pkl', WORKSPACE_DIR / 'xgboost_model.pkl'),\n",
        "            ('predictions.csv', WORKSPACE_DIR / 'predictions.csv'),\n",
        "            ('eval_metrics.json', WORKSPACE_DIR / 'eval_metrics.json'),\n",
        "            ('calibrated_model.pkl', WORKSPACE_DIR / 'calibrated_model.pkl'),\n",
        "            ('calibrated_predictions.csv', WORKSPACE_DIR / 'calibrated_predictions.csv')\n",
        "        ]\n",
        "        \n",
        "        for file_name, file_path in output_files:\n",
        "            if file_path.exists():\n",
        "                file_size = file_path.stat().st_size\n",
        "                file_analysis.append({\n",
        "                    'file_name': file_name,\n",
        "                    'file_path': str(file_path),\n",
        "                    'file_size_bytes': file_size,\n",
        "                    'file_size_kb': file_size / 1024,\n",
        "                    'file_type': 'output_data'\n",
        "                })\n",
        "        \n",
        "        if file_analysis:\n",
        "            df_files = pd.DataFrame(file_analysis)\n",
        "            \n",
        "            print(\"DATA FLOW ANALYSIS\")\n",
        "            print(\"=\"*20)\n",
        "            print(f\"Total files analyzed: {len(file_analysis)}\")\n",
        "            print(f\"Input files: {len([f for f in file_analysis if f['file_type'] == 'input_data'])}\")\n",
        "            print(f\"Output files: {len([f for f in file_analysis if f['file_type'] == 'output_data'])}\")\n",
        "            \n",
        "            total_input_size = df_files[df_files['file_type'] == 'input_data']['file_size_kb'].sum()\n",
        "            total_output_size = df_files[df_files['file_type'] == 'output_data']['file_size_kb'].sum()\n",
        "            \n",
        "            print(f\"Total input data size: {total_input_size:.2f} KB\")\n",
        "            print(f\"Total output data size: {total_output_size:.2f} KB\")\n",
        "            \n",
        "            self.analysis_results['data_flow'] = {\n",
        "                'file_analysis': file_analysis,\n",
        "                'total_input_size_kb': total_input_size,\n",
        "                'total_output_size_kb': total_output_size,\n",
        "                'data_expansion_ratio': total_output_size / total_input_size if total_input_size > 0 else 0\n",
        "            }\n",
        "        else:\n",
        "            print(\"⚠ No files found for data flow analysis\")\n",
        "    \n",
        "    def create_visualizations(self):\n",
        "        \"\"\"Create comprehensive performance visualizations.\"\"\"\n",
        "        print(\"\\n=== CREATE VISUALIZATIONS ===\")\n",
        "        \n",
        "        # Create execution time comparison chart\n",
        "        if 'execution_times' in self.analysis_results:\n",
        "            self._create_execution_time_chart()\n",
        "        \n",
        "        # Create success rate comparison chart\n",
        "        if 'success_rates' in self.analysis_results:\n",
        "            self._create_success_rate_chart()\n",
        "        \n",
        "        # Create data flow visualization\n",
        "        if 'data_flow' in self.analysis_results:\n",
        "            self._create_data_flow_chart()\n",
        "        \n",
        "        # Create comprehensive dashboard\n",
        "        self._create_performance_dashboard()\n",
        "    \n",
        "    def _create_execution_time_chart(self):\n",
        "        \"\"\"Create execution time comparison chart.\"\"\"\n",
        "        execution_data = self.analysis_results['execution_times']['raw_data']\n",
        "        df = pd.DataFrame(execution_data)\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Bar chart comparing execution times by step\n",
        "        step_comparison = df.pivot_table(\n",
        "            values='execution_time', \n",
        "            index='step_name', \n",
        "            columns='test_mode', \n",
        "            aggfunc='mean'\n",
        "        )\n",
        "        \n",
        "        step_comparison.plot(kind='bar', ax=ax1, rot=45)\n",
        "        ax1.set_title('Execution Time by Step and Test Mode')\n",
        "        ax1.set_ylabel('Execution Time (seconds)')\n",
        "        ax1.legend(title='Test Mode')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Box plot showing execution time distribution\n",
        "        sns.boxplot(data=df, x='step_name', y='execution_time', hue='test_mode', ax=ax2)\n",
        "        ax2.set_title('Execution Time Distribution')\n",
        "        ax2.set_ylabel('Execution Time (seconds)')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.viz_dir / 'execution_time_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"✓ Execution time chart saved: {self.viz_dir / 'execution_time_analysis.png'}\")\n",
        "    \n",
        "    def _create_success_rate_chart(self):\n",
        "        \"\"\"Create success rate comparison chart.\"\"\"\n",
        "        success_data = self.analysis_results['success_rates']\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Success rate comparison\n",
        "        test_modes = [data['test_mode'] for data in success_data]\n",
        "        success_rates = [data['success_rate'] for data in success_data]\n",
        "        \n",
        "        bars = ax1.bar(test_modes, success_rates, color=['lightblue', 'lightgreen'])\n",
        "        ax1.set_title('Success Rate Comparison')\n",
        "        ax1.set_ylabel('Success Rate (%)')\n",
        "        ax1.set_ylim(0, 100)\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, rate in zip(bars, success_rates):\n",
        "            height = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                    f'{rate:.1f}%', ha='center', va='bottom')\n",
        "        \n",
        "        # Step success breakdown\n",
        "        step_data = []\n",
        "        for data in success_data:\n",
        "            step_data.extend([\n",
        "                {'test_mode': data['test_mode'], 'status': 'Successful', 'count': data['successful_steps']},\n",
        "                {'test_mode': data['test_mode'], 'status': 'Failed', 'count': data['failed_steps']}\n",
        "            ])\n",
        "        \n",
        "        df_steps = pd.DataFrame(step_data)\n",
        "        step_pivot = df_steps.pivot_table(values='count', index='test_mode', columns='status', fill_value=0)\n",
        "        \n",
        "        step_pivot.plot(kind='bar', stacked=True, ax=ax2, color=['lightcoral', 'lightgreen'])\n",
        "        ax2.set_title('Step Success/Failure Breakdown')\n",
        "        ax2.set_ylabel('Number of Steps')\n",
        "        ax2.legend(title='Status')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.viz_dir / 'success_rate_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"✓ Success rate chart saved: {self.viz_dir / 'success_rate_analysis.png'}\")\n",
        "    \n",
        "    def _create_data_flow_chart(self):\n",
        "        \"\"\"Create data flow visualization.\"\"\"\n",
        "        file_analysis = self.analysis_results['data_flow']['file_analysis']\n",
        "        df_files = pd.DataFrame(file_analysis)\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # File size comparison\n",
        "        input_files = df_files[df_files['file_type'] == 'input_data']\n",
        "        output_files = df_files[df_files['file_type'] == 'output_data']\n",
        "        \n",
        "        # Bar chart of file sizes\n",
        "        all_files = pd.concat([input_files, output_files])\n",
        "        colors = ['lightblue' if ft == 'input_data' else 'lightcoral' for ft in all_files['file_type']]\n",
        "        \n",
        "        bars = ax1.bar(range(len(all_files)), all_files['file_size_kb'], color=colors)\n",
        "        ax1.set_title('File Sizes Throughout Pipeline')\n",
        "        ax1.set_ylabel('File Size (KB)')\n",
        "        ax1.set_xticks(range(len(all_files)))\n",
        "        ax1.set_xticklabels(all_files['file_name'], rotation=45, ha='right')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add legend\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor='lightblue', label='Input Data'),\n",
        "            Patch(facecolor='lightcoral', label='Output Data')\n",
        "        ]\n",
        "        ax1.legend(handles=legend_elements)\n",
        "        \n",
        "        # Pie chart of data distribution\n",
        "        total_input = self.analysis_results['data_flow']['total_input_size_kb']\n",
        "        total_output = self.analysis_results['data_flow']['total_output_size_kb']\n",
        "        \n",
        "        sizes = [total_input, total_output]\n",
        "        labels = ['Input Data', 'Output Data']\n",
        "        colors = ['lightblue', 'lightcoral']\n",
        "        \n",
        "        ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "        ax2.set_title('Data Size Distribution')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.viz_dir / 'data_flow_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"✓ Data flow chart saved: {self.viz_dir / 'data_flow_analysis.png'}\")\n",
        "    \n",
        "    def _create_performance_dashboard(self):\n",
        "        \"\"\"Create comprehensive performance dashboard.\"\"\"\n",
        "        fig = plt.figure(figsize=(20, 12))\n",
        "        \n",
        "        # Create grid layout\n",
        "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "        \n",
        "        # Title\n",
        "        fig.suptitle('XGBoost Pipeline Performance Dashboard', fontsize=20, fontweight='bold')\n",
        "        \n",
        "        # 1. Execution time summary (top left)\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        if 'execution_times' in self.analysis_results:\n",
        "            total_individual = self.analysis_results['execution_times']['total_individual_time']\n",
        "            total_pipeline = self.analysis_results['execution_times']['total_pipeline_time']\n",
        "            \n",
        "            ax1.bar(['Individual\\nStep Testing', 'End-to-End\\nPipeline'], \n",
        "                   [total_individual, total_pipeline], \n",
        "                   color=['lightblue', 'lightgreen'])\n",
        "            ax1.set_title('Total Execution Time')\n",
        "            ax1.set_ylabel('Time (seconds)')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 2. Success rate summary (top middle)\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        if 'success_rates' in self.analysis_results:\n",
        "            success_data = self.analysis_results['success_rates']\n",
        "            modes = [data['test_mode'] for data in success_data]\n",
        "            rates = [data['success_rate'] for data in success_data]\n",
        "            \n",
        "            bars = ax2.bar(modes, rates, color=['lightblue', 'lightgreen'])\n",
        "            ax2.set_title('Success Rates')\n",
        "            ax2.set_ylabel('Success Rate (%)')\n",
        "            ax2.set_ylim(0, 100)\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            \n",
        "            for bar, rate in zip(bars, rates):\n",
        "                height = bar.get_height()\n",
        "                ax2.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
        "                        f'{rate:.1f}%', ha='center', va='bottom')\n",
        "        \n",
        "        # 3. Data flow summary (top right)\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\n",
        "        if 'data_flow' in self.analysis_results:\n",
        "            input_size = self.analysis_results['data_flow']['total_input_size_kb']\n",
        "            output_size = self.analysis_results['data_flow']['total_output_size_kb']\n",
        "            \n",
        "            ax3.bar(['Input Data', 'Output Data'], [input_size, output_size], \n",
        "                   color=['lightblue', 'lightcoral'])\n",
        "            ax3.set_title('Data Volume')\n",
        "            ax3.set_ylabel('Size (KB)')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 4. Step-by-step execution times (middle row)\n",
        "        ax4 = fig.add_subplot(gs[1, :])\n",
        "        if 'execution_times' in self.analysis_results:\n",
        "            execution_data = self.analysis_results['execution_times']['raw_data']\n",
        "            df = pd.DataFrame(execution_data)\n",
        "            \n",
        "            step_comparison = df.pivot_table(\n",
        "                values='execution_time', \n",
        "                index='step_name', \n",
        "                columns='test_mode', \n",
        "                aggfunc='mean'\n",
        "            )\n",
        "            \n",
        "            step_comparison.plot(kind='bar', ax=ax4, rot=0)\n",
        "            ax4.set_title('Step-by-Step Execution Time Comparison')\n",
        "            ax4.set_ylabel('Execution Time (seconds)')\n",
        "            ax4.legend(title='Test Mode')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "        \n",
        "        # 5. Performance metrics summary (bottom)\n",
        "        ax5 = fig.add_subplot(gs[2, :])\n",
        "        ax5.axis('off')\n",
        "        \n",
        "        # Create summary text\n",
        "        summary_text = self._generate_performance_summary()\n",
        "        ax5.text(0.05, 0.95, summary_text, transform=ax5.transAxes, \n",
        "                fontsize=12, verticalalignment='top', fontfamily='monospace',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "        \n",
        "        plt.savefig(self.viz_dir / 'performance_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        \n",
        "        print(f\"✓ Performance dashboard saved: {self.viz_dir / 'performance_dashboard.png'}\")\n",
        "    \n",
        "    def _generate_performance_summary(self):\n",
        "        \"\"\"Generate performance summary text.\"\"\"\n",
        "        summary_lines = [\"PERFORMANCE SUMMARY\", \"=\" * 50]\n",
        "        \n",
        "        # Execution time summary\n",
        "        if 'execution_times' in self.analysis_results:\n",
        "            total_individual = self.analysis_results['execution_times']['total_individual_time']\n",
        "            total_pipeline = self.analysis_results['execution_times']['total_pipeline_time']\n",
        "            \n",
        "            summary_lines.extend([\n",
        "                f\"Total Individual Step Testing Time: {total_individual:.2f}s\",\n",
        "                f\"Total End-to-End Pipeline Time: {total_pipeline:.2f}s\",\n",
        "                f\"Pipeline Overhead: {total_pipeline - total_individual:.2f}s\",\n",
        "                \"\"\n",
        "            ])\n",
        "        \n",
        "        # Success rate summary\n",
        "        if 'success_rates' in self.analysis_results:\n",
        "            for data in self.analysis_results['success_rates']:\n",
        "                summary_lines.append(\n",
        "                    f\"{data['test_mode']}: {data['successful_steps']}/{data['total_steps']} \"\n",
        "                    f\"steps successful ({data['success_rate']:.1f}%)\"\n",
        "                )\n",
        "            summary_lines.append(\"\")\n",
        "        \n",
        "        # Data flow summary\n",
        "        if 'data_flow' in self.analysis_results:\n",
        "            input_size = self.analysis_results['data_flow']['total_input_size_kb']\n",
        "            output_size = self.analysis_results['data_flow']['total_output_size_kb']\n",
        "            expansion_ratio = self.analysis_results['data_flow']['data_expansion_ratio']\n",
        "            \n",
        "            summary_lines.extend([\n",
        "                f\"Input Data Size: {input_size:.2f} KB\",\n",
        "                f\"Output Data Size: {output_size:.2f} KB\",\n",
        "                f\"Data Expansion Ratio: {expansion_ratio:.2f}x\",\n",
        "                \"\"\n",
        "            ])\n",
        "        \n",
        "        # Test completion status\n",
        "        summary_lines.extend([\n",
        "            f\"Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            f\"Visualizations saved to: {self.viz_dir}\"\n",
        "        ])\n",
        "        \n",
        "        return \"\\n\".join(summary_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_environment():\n",
        "    \"\"\"Setup environment and imports.\"\"\"\n",
        "    print(\"=== PERFORMANCE ANALYSIS SETUP ===\")\n",
        "    \n",
        "    # Add cursus to path\n",
        "    sys.path.append(str(Path.cwd().parent.parent.parent / 'src'))\n",
        "    \n",
        "    # Import Cursus components\n",
        "    try:\n",
        "        from cursus.validation.runtime.jupyter.notebook_interface import NotebookInterface\n",
        "        from cursus.validation.runtime.core.data_flow_manager import DataFlowManager\n",
        "        print(\"✓ Successfully imported Cursus components\")\n",
        "        cursus_available = True\n",
        "    except ImportError as e:\n",
        "        print(f\"⚠ Import error: {e}\")\n",
        "        print(\"Using standard libraries for analysis...\")\n",
        "        cursus_available = False\n",
        "    \n",
        "    # Set up plotting\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    print(f\"Performance analysis started at {datetime.now()}\")\n",
        "    return cursus_available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_test_results():\n",
        "    \"\"\"Load all test results from previous notebooks.\"\"\"\n",
        "    print(\"\\n=== LOAD TEST RESULTS ===\")\n",
        "    \n",
        "    BASE_DIR = Path.cwd()\n",
        "    RESULTS_DIR = BASE_DIR / 'outputs' / 'results'\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Load individual step test results\n",
        "    individual_results_path = RESULTS_DIR / 'individual_step_test_results.json'\n",
        "    if individual_results_path.exists():\n",
        "        with open(individual_results_path, 'r') as f:\n",
        "            results['individual_step_testing'] = json.load(f)\n",
        "        print(f\"✓ Loaded individual step test results\")\n",
        "    else:\n",
        "        print(f\"⚠ Individual step test results not found: {individual_results_path}\")\n",
        "    \n",
        "    # Load end-to-end pipeline results\n",
        "    pipeline_results_path = RESULTS_DIR / 'end_to_end_pipeline_results.json'\n",
        "    if pipeline_results_path.exists():\n",
        "        with open(pipeline_results_path, 'r') as f:\n",
        "            results['end_to_end_pipeline'] = json.load(f)\n",
        "        print(f\"✓ Loaded end-to-end pipeline results\")\n",
        "    else:\n",
        "        print(f\"⚠ End-to-end pipeline results not found: {pipeline_results_path}\")\n",
        "    \n",
        "    # Load dataset metadata\n",
        "    dataset_metadata_path = BASE_DIR / 'data' / 'dataset_metadata.json'\n",
        "    if dataset_metadata_path.exists():\n",
        "        with open(dataset_metadata_path, 'r') as f:\n",
        "            results['dataset_metadata'] = json.load(f)\n",
        "        print(f\"✓ Loaded dataset metadata\")\n",
        "    else:\n",
        "        print(f\"⚠ Dataset metadata not found: {dataset_metadata_path}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_analysis_results(analysis_results):\n",
        "    \"\"\"Save comprehensive analysis results.\"\"\"\n",
        "    print(\"\\n=== SAVE ANALYSIS RESULTS ===\")\n",
        "    \n",
        "    BASE_DIR = Path.cwd()\n",
        "    RESULTS_DIR = BASE_DIR / 'outputs' / 'results'\n",
        "    \n",
        "    # Prepare results for JSON serialization\n",
        "    serializable_results = {}\n",
        "    for key, value in analysis_results.items():\n",
        "        if isinstance(value, dict):\n",
        "            serializable_results[key] = value\n",
        "        else:\n",
        "            serializable_results[key] = str(value)\n",
        "    \n",
        "    # Add metadata\n",
        "    final_results = {\n",
        "        'analysis_timestamp': datetime.now().isoformat(),\n",
        "        'analysis_type': 'performance_analysis',\n",
        "        'analysis_results': serializable_results,\n",
        "        'visualizations_directory': str(Path.cwd() / 'outputs' / 'visualizations')\n",
        "    }\n",
        "    \n",
        "    results_path = RESULTS_DIR / 'performance_analysis_results.json'\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(final_results, f, indent=2)\n",
        "    \n",
        "    print(f\"✓ Performance analysis results saved: {results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_final_report():\n",
        "    \"\"\"Generate final performance analysis report.\"\"\"\n",
        "    print(\"\\n=== GENERATE FINAL REPORT ===\")\n",
        "    \n",
        "    BASE_DIR = Path.cwd()\n",
        "    RESULTS_DIR = BASE_DIR / 'outputs' / 'results'\n",
        "    \n",
        "    # Load all results\n",
        "    all_results = {}\n",
        "    result_files = [\n",
        "        'individual_step_test_results.json',\n",
        "        'end_to_end_pipeline_results.json',\n",
        "        'performance_analysis_results.json'\n",
        "    ]\n",
        "    \n",
        "    for result_file in result_files:\n",
        "        result_path = RESULTS_DIR / result_file\n",
        "        if result_path.exists():\n",
        "            with open(result_path, 'r') as f:\n",
        "                all_results[result_file.replace('.json', '')] = json.load(f)\n",
        "    \n",
        "    print(\"FINAL PERFORMANCE REPORT\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Overall test summary\n",
        "    if 'individual_step_test_results' in all_results:\n",
        "        individual_results = all_results['individual_step_test_results']\n",
        "        print(f\"Individual Step Testing: {individual_results['success_rate']:.1f}% success rate\")\n",
        "    \n",
        "    if 'end_to_end_pipeline_results' in all_results:\n",
        "        pipeline_results = all_results['end_to_end_pipeline_results']\n",
        "        print(f\"End-to-End Pipeline: {pipeline_results['success_rate']:.1f}% success rate\")\n",
        "    \n",
        "    # Performance insights\n",
        "    if 'performance_analysis_results' in all_results:\n",
        "        perf_results = all_results['performance_analysis_results']\n",
        "        print(f\"Performance analysis completed at: {perf_results['analysis_timestamp']}\")\n",
        "        print(f\"Visualizations available in: {perf_results['visualizations_directory']}\")\n",
        "    \n",
        "    print(\"\\n✓ XGBoost 3-step pipeline testing completed successfully!\")\n",
        "    print(\"✓ All performance metrics analyzed and visualized\")\n",
        "    print(\"✓ Ready for production deployment validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_performance_analysis():\n",
        "    \"\"\"Run comprehensive performance analysis.\"\"\"\n",
        "    print(\"STARTING PERFORMANCE ANALYSIS\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    try:\n",
        "        # Load test results\n",
        "        results_data = load_test_results()\n",
        "        \n",
        "        if not results_data:\n",
        "            print(\"⚠ No test results available for analysis\")\n",
        "            return False\n",
        "        \n",
        "        # Create analyzer\n",
        "        analyzer = PerformanceAnalyzer(results_data)\n",
        "        \n",
        "        # Run analyses\n",
        "        analyzer.analyze_execution_times()\n",
        "        analyzer.analyze_success_rates()\n",
        "        analyzer.analyze_data_flow()\n",
        "        \n",
        "        # Create visualizations\n",
        "        analyzer.create_visualizations()\n",
        "        \n",
        "        # Save comprehensive analysis results\n",
        "        save_analysis_results(analyzer.analysis_results)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Performance analysis failed: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Script Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Main execution function.\"\"\"\n",
        "\n",
        "cursus_available = setup_environment()\n",
        "\n",
        "success = run_performance_analysis()\n",
        "\n",
        "if success:\n",
        "    # Generate final report\n",
        "    generate_final_report()\n",
        "        \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PERFORMANCE ANALYSIS COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"🎉 All analyses completed!\")\n",
        "    print(\"📊 Visualizations generated!\")\n",
        "    print(\"📋 Reports saved!\")\n",
        "else:\n",
        "    print(\"\\n⚠ Performance analysis failed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Performance Analysis Script\n",
        "\n",
        "This script analyzes the performance of the XGBoost 3-step pipeline execution,\n",
        "generates visualizations, and provides comprehensive reporting.\n",
        "\"\"\"\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}