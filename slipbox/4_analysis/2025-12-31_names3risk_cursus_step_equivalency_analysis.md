---
tags:
  - analysis
  - step-equivalency
  - names3risk-pipeline
  - cursus-framework
  - step-mapping
  - gap-analysis
keywords:
  - step equivalency
  - Names3Risk pipeline
  - Cursus steps
  - functionality mapping
  - step creation recommendations
  - fraud name detection
  - character-level tokenization
  - bimodal LSTM
date of note: 2025-12-31
---

# Names3Risk Pipeline vs Cursus Framework Step Equivalency Analysis

## Executive Summary

This analysis compares the Names3Risk pipeline with existing Cursus framework steps to identify equivalencies, partial coverage, and unique functionalities. Names3Risk is a fraud detection model that analyzes customer names, email addresses, and billing information using character-level text processing combined with tabular fraud features.

### **Pipeline Compatibility Overview**

The Names3Risk pipeline demonstrates **65% compatibility** with existing Cursus steps, with complete equivalencies for data loading and partial coverage for model training. The primary gap is the unique character-level tokenization approach, which differs fundamentally from Cursus's BERT-based text processing.

**Key Compatibility Findings**:
- ‚úÖ **Complete Equivalency (100%)**: CradleDataLoading for multi-regional MDS + Andes data fetching
- ‚ö†Ô∏è **Partial Coverage (70%)**: PyTorchTraining supports LSTM/Transformer architectures but lacks character-level preprocessing
- ‚ùå **Missing Functionality (30%)**: Character-level tokenization for name/email analysis, name-specific text preprocessing

**Recommendation**: Implement 2 new Cursus steps following the 80/20 base-extension pattern to achieve full framework integration while preserving Names3Risk's specialized fraud name detection capabilities.

## Cursus Framework Step Inventory

### Available Step Categories

Based on analysis of Cursus framework capabilities:

#### **Data Loading Steps**
- **CradleDataLoading**: Multi-source data integration (MDS, Andes, EDX) with SQL transformation
- **DummyDataLoading**: Mock data loading for testing

#### **Text Processing Steps**
- **BertTokenizeProcessor**: BERT-based subword tokenization
- **DialogueSplitterProcessor**: Multi-turn dialogue splitting
- **DialogueChunkerProcessor**: Long text chunking with overlap
- **HTMLNormalizerProcessor**: HTML tag cleaning
- **EmojiRemoverProcessor**: Emoji and special character removal
- **TextNormalizationProcessor**: Unicode normalization and text cleaning

#### **Tabular Processing Steps**
- **NumericalVariableImputationProcessor**: Statistical imputation (mean, median, mode)
- **RiskTableMappingProcessor**: Categorical risk encoding with WOE/target rate
- **CategoricalLabelProcessor**: Binary label encoding
- **MultiClassLabelProcessor**: Multi-class label encoding

#### **Training Steps**
- **PyTorchTraining**: General PyTorch model training with Lightning
- **XGBoostTraining**: XGBoost model training
- **LightGBMTraining**: LightGBM model training
- **DummyTraining**: Mock training for testing

#### **Model Types Supported**
- **BimodalBert**: Text + Tabular fusion with BERT
- **BimodalCNN**: Text + Tabular fusion with CNN
- **BimodalBertMoE**: Mixture of Experts with BERT
- **BimodalBertGateFusion**: Gate-based fusion
- **BimodalBertCrossAttn**: Cross-attention fusion
- **TextBertClassification**: Text-only BERT classification
- **TextLSTM**: Text-only LSTM classification

#### **Model Processing Steps**
- **ModelCalibration**: Probability calibration (Isotonic, Platt, GAM)
- **ModelEvaluation**: Metrics computation (AUC, PR-AUC, F1, etc.)
- **Package**: Model packaging for deployment
- **Registration**: Model registration in MIMS

## Names3Risk Pipeline Step Equivalency Analysis

### Pipeline Component Breakdown

Names3Risk pipeline consists of 3 main components:

1. **Data Fetching** (`fetch_data.py`): Multi-regional MDS + Andes data loading
2. **Text Preprocessing** (implicit in `train.py`): Character-level tokenization
3. **Model Training** (`train.py`): LSTM/Transformer with bimodal architecture

### Complete Equivalencies (100% Coverage)

#### 1. **Data Fetching ‚Üî CradleDataLoading**

**Names3Risk Implementation**: `fetch_data.py` with `SAISEDXLoadJob` class

**Cursus Equivalent**: `CradleDataLoading` step

**Coverage**: ‚úÖ **Complete (100%)**

**Functionality Match**:

| Functionality | Names3Risk Implementation | Cursus Implementation | Match |
|---------------|--------------------------|----------------------|-------|
| MDS data source | ‚úÖ Multi-regional MDS (FORTRESS service) | ‚úÖ Configurable MDS service | ‚úÖ |
| Andes data source | ‚úÖ D_CUSTOMERS table (booker provider) | ‚úÖ Configurable Andes tables | ‚úÖ |
| Regional support | ‚úÖ NA (org_id=1), EU (org_id=2), FE (org_id=9) | ‚úÖ Configurable regions | ‚úÖ |
| SQL transformation | ‚úÖ Complex SQL with joins and filtering | ‚úÖ Configurable transform_sql | ‚úÖ |
| Output format | ‚úÖ Parquet files | ‚úÖ Multiple formats (Parquet, CSV, TSV) | ‚úÖ |
| Job splitting | ‚úÖ split_job parameter with days_per_split | ‚úÖ JobSplitOptions with configurable splits | ‚úÖ |
| Data filtering | ‚úÖ Fraud-specific filters (status, random sampling) | ‚úÖ SQL-based filtering | ‚úÖ |
| Schema definition | ‚úÖ output_schema with field types | ‚úÖ output_schema with field metadata | ‚úÖ |

**Detailed Names3Risk Data Loading Logic**:

```python
# Names3Risk MDS configuration
mds_data_source_properties=MdsDataSourceProperties(
    service_name="FORTRESS",
    org_id=org_id,  # 1 for NA, 2 for EU, 9 for FE
    region=self.region,
    output_schema=[Field(field_name=col, field_type="STRING") for col in mds_vars],
    use_hourly_edx_data_set=False,
)

# Names3Risk Andes configuration
andes_data_source_properties=AndesDataSourceProperties(
    provider="booker",
    table_name="D_CUSTOMERS",
    andes3_enabled=True,
)

# Names3Risk SQL transformation with fraud filtering
transform_sql=f"""
    WITH features AS (
        SELECT
            RAW_MDS.*,
            D_CUSTOMERS.status AS status,
            ROW_NUMBER() OVER (PARTITION BY RAW_MDS.objectId ORDER BY RAW_MDS.transactionDate) AS dedup
        FROM RAW_MDS
            INNER JOIN D_CUSTOMERS ON RAW_MDS.customerId = D_CUSTOMERS.customer_id
        WHERE ABS(daysSinceFirstCompletedOrder) < 1e-12
    )
    SELECT *
    FROM features
    WHERE dedup = 1
        AND ((status = 'N' AND RAND() < 0.5) OR status IN ('F', 'I'))
"""
```

**Cursus Equivalent Configuration**:

```python
# Cursus CradleDataLoadingConfig
CradleDataLoadingConfig(
    job_type="training",  # or "validation", "testing"
    data_sources_spec=DataSourcesSpecificationConfig(
        start_date="2025-02-15T00:00:00",
        end_date="2025-05-15T00:00:00",
        data_sources=[
            DataSourceConfig(
                data_source_name="D_CUSTOMERS",
                data_source_type="ANDES",
                andes_data_source_properties=AndesDataSourceConfig(
                    provider="booker",
                    table_name="D_CUSTOMERS",
                    andes3_enabled=True,
                ),
            ),
            DataSourceConfig(
                data_source_name="RAW_MDS",
                data_source_type="MDS",
                mds_data_source_properties=MdsDataSourceConfig(
                    service_name="FORTRESS",
                    org_id=1,  # NA region
                    region="NA",
                    output_schema=[{"field_name": col, "field_type": "STRING"} for col in mds_vars],
                    use_hourly_edx_data_set=False,
                ),
            ),
        ],
    ),
    transform_spec=TransformSpecificationConfig(
        transform_sql="""
            WITH features AS (
                SELECT
                    RAW_MDS.*,
                    D_CUSTOMERS.status AS status,
                    ROW_NUMBER() OVER (PARTITION BY RAW_MDS.objectId ORDER BY RAW_MDS.transactionDate) AS dedup
                FROM RAW_MDS
                    INNER JOIN D_CUSTOMERS ON RAW_MDS.customerId = D_CUSTOMERS.customer_id
                WHERE ABS(daysSinceFirstCompletedOrder) < 1e-12
            )
            SELECT *
            FROM features
            WHERE dedup = 1
                AND ((status = 'N' AND RAND() < 0.5) OR status IN ('F', 'I'))
        """,
        job_split_options=JobSplitOptionsConfig(
            split_job=False,  # or True with days_per_split=30, merge_sql="SELECT * FROM INPUT"
        ),
    ),
    output_spec=OutputSpecificationConfig(
        output_schema=list(mds_vars) + ["status"],
        output_format="PARQUET",
        output_save_mode="ERRORIFEXISTS",
        keep_dot_in_output_schema=False,
        include_header_in_s3_output=True,
    ),
    cradle_job_spec=CradleJobSpecificationConfig(
        cluster_type="LARGE",
        cradle_account="BRP-ML-Payment-Generate-Data",
        job_retry_count=0,
    ),
)
```

**Recommendation**: **Direct replacement** - use existing Cursus CradleDataLoading step with Names3Risk-specific configuration.

### Partial Coverage with Significant Gaps

#### 2. **Text Preprocessing ‚Üî Cursus Text Processing Pipeline**

**Names3Risk Implementation**: Character-level tokenization in `tokenizer.py` with `OrderTextTokenizer`

**Cursus Equivalent**: BERT-based text processing pipeline (BertTokenizeProcessor)

**Coverage**: ‚ö†Ô∏è **Partial (30%)**

**Functionality Comparison**:

| Feature | Names3Risk Implementation | Cursus Implementation | Coverage |
|---------|--------------------------|----------------------|----------|
| **Tokenization Approach** | Character-level vocabulary | BERT subword tokenization | ‚ùå **0%** - Fundamentally different |
| **Text Sources** | Name/email concatenation | Any text field | ‚úÖ **100%** - Configurable |
| **Special Tokens** | PAD, UNK tokens | [PAD], [UNK], [CLS], [SEP] | ‚ö†Ô∏è **50%** - Partial overlap |
| **Vocabulary Building** | Character frequency-based | Pre-trained BERT vocab | ‚ùå **0%** - Different approach |
| **Missing Value Handling** | "[MISSING]" placeholder | Configurable defaults | ‚úÖ **100%** - Both support |
| **Field Concatenation** | Pipe-separated (emailAddress\|billingAddressName\|...) | Configurable separator | ‚úÖ **100%** - Supported |
| **Text Cleaning** | None (raw characters) | HTML, emoji, normalization | ‚ö†Ô∏è **0%** - Names3Risk skips cleaning |

**Detailed Names3Risk Text Processing Logic**:

```python
# Names3Risk text concatenation
text=pl.concat_str(
    [
        pl.col("emailAddress").fill_null("[MISSING]"),
        pl.col("billingAddressName").fill_null("[MISSING]"),
        pl.col("customerName").fill_null("[MISSING]"),
        pl.col("paymentAccountHolderName").fill_null("[MISSING]"),
    ],
    separator="|",
)

# Names3Risk character-level tokenization
class OrderTextTokenizer:
    def __init__(self, min_freq=2):
        self.min_freq = min_freq
        self.char_to_idx = {}
        self.idx_to_char = {}
        self.vocab_size = 0
        self.pad_token = 0
        self.unk_token = 1
        
    def train(self, texts: List[str]) -> "OrderTextTokenizer":
        # Count character frequencies
        char_freq = {}
        for text in texts:
            for char in text:
                char_freq[char] = char_freq.get(char, 0) + 1
        
        # Build vocabulary from frequent characters
        self.char_to_idx = {"<PAD>": self.pad_token, "<UNK>": self.unk_token}
        self.idx_to_char = {self.pad_token: "<PAD>", self.unk_token: "<UNK>"}
        
        idx = 2
        for char, freq in sorted(char_freq.items(), key=lambda x: -x[1]):
            if freq >= self.min_freq:
                self.char_to_idx[char] = idx
                self.idx_to_char[idx] = char
                idx += 1
        
        self.vocab_size = len(self.char_to_idx)
        return self
    
    def encode(self, text: str, max_length: int = 512) -> List[int]:
        tokens = [self.char_to_idx.get(char, self.unk_token) for char in text[:max_length]]
        # Pad to max_length
        tokens.extend([self.pad_token] * (max_length - len(tokens)))
        return tokens
```

**Cursus Text Processing (BERT-based)**:

```python
# Cursus text processing pipeline
text_pipeline = build_text_pipeline_from_steps(
    processing_steps=[
        "dialogue_splitter",      # Split into dialogue turns
        "html_normalizer",        # Clean HTML tags
        "emoji_remover",          # Remove emojis
        "text_normalizer",        # Unicode normalization
        "dialogue_chunker",       # Chunk long text
        "tokenizer",              # BERT tokenization
    ],
    tokenizer=AutoTokenizer.from_pretrained("bert-base-multilingual-cased"),
    max_sen_len=512,
    chunk_trancate=False,
    max_total_chunks=5,
)

# Apply to text field
dataset.add_pipeline("text_field", text_pipeline)
```

**Coverage Analysis**:

‚úÖ **Covered Functionalities**:
- Text field concatenation with configurable separators
- Missing value handling with placeholders
- Maximum length truncation
- Batch processing support

‚ùå **Uncovered Functionalities**:
- Character-level vocabulary building from training data
- Character-level tokenization (vs subword tokenization)
- Frequency-based vocabulary filtering (min_freq threshold)
- Character-specific padding and unknown token handling
- Raw text processing without normalization or cleaning

**Recommendation**: **New step required** - Implement `CharacterLevelTextPreprocessing` step to support Names3Risk's unique tokenization approach.

#### 3. **Model Training ‚Üî PyTorchTraining**

**Names3Risk Implementation**: `train.py` with LSTM/Transformer bimodal architecture

**Cursus Equivalent**: `PyTorchTraining` step with bimodal model support

**Coverage**: ‚ö†Ô∏è **Partial (70%)**

**Functionality Comparison**:

| Feature | Names3Risk Implementation | Cursus Implementation | Coverage |
|---------|--------------------------|----------------------|----------|
| **PyTorch Framework** | ‚úÖ Native PyTorch | ‚úÖ PyTorch Lightning | ‚úÖ **100%** |
| **Bimodal Architecture** | ‚úÖ Text + Tabular | ‚úÖ BimodalBert, BimodalCNN, etc. | ‚úÖ **100%** |
| **LSTM Support** | ‚úÖ LSTM2Risk model | ‚úÖ TextLSTM model | ‚úÖ **100%** |
| **Transformer Support** | ‚úÖ Transformer2Risk model | ‚úÖ BimodalBert models | ‚úÖ **100%** |
| **Training Loop** | ‚úÖ Custom loop with tqdm | ‚úÖ PyTorch Lightning trainer | ‚úÖ **100%** |
| **Optimizer** | ‚úÖ AdamW | ‚úÖ Configurable (Adam, AdamW, SGD) | ‚úÖ **100%** |
| **Scheduler** | ‚úÖ OneCycleLR | ‚úÖ Multiple schedulers | ‚úÖ **100%** |
| **Loss Function** | ‚úÖ BCELoss | ‚úÖ Configurable (BCE, CrossEntropy, Focal) | ‚úÖ **100%** |
| **Metrics** | ‚úÖ AUC (torcheval) | ‚úÖ AUC, F1, Precision, Recall | ‚úÖ **100%** |
| **Early Stopping** | ‚ùå Manual stopping | ‚úÖ Built-in early stopping | ‚ö†Ô∏è **50%** |
| **Checkpointing** | ‚úÖ Manual save per epoch | ‚úÖ Automatic checkpointing | ‚úÖ **100%** |
| **Mixed Precision** | ‚ùå Not supported | ‚úÖ AMP support | ‚ö†Ô∏è **50%** |
| **Distributed Training** | ‚ùå Single GPU | ‚úÖ DDP, FSDP support | ‚ö†Ô∏è **0%** |
| **Character Embeddings** | ‚úÖ Custom vocab embeddings | ‚ùå BERT embeddings only | ‚ùå **0%** |
| **Per-Region Evaluation** | ‚úÖ Marketplace-specific AUC | ‚ö†Ô∏è Custom metric computation | ‚ö†Ô∏è **50%** |
| **JSON Logging** | ‚úÖ JSONLogger | ‚úÖ TensorBoard, MLflow | ‚ö†Ô∏è **50%** |

**Detailed Names3Risk Training Logic**:

```python
# Names3Risk training configuration
BATCH_SIZE = 512
EPOCHS = 10
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Names3Risk model architecture
config = lstm2risk.LSTMConfig(n_tab_features=len(tabular_features))
config.n_embed = tokenizer.vocab_size

model = lstm2risk.LSTM2Risk(config).to(DEVICE)

# Names3Risk training loop
def train_loop(model, dataloader, loss_fn, optimizer, scheduler):
    model.train()
    auc = BinaryAUROC()
    
    for batch in tqdm(dataloader):
        batch = {k: v.to(DEVICE) for k, v in batch.items()}
        pred = model(batch)
        loss = loss_fn(pred, batch["label"])
        
        auc.update(pred.view(-1), batch["label"].view(-1))
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        scheduler.step()
    
    return auc.compute()

# Names3Risk per-region evaluation
mp_auc = {}
for (mp,), df_mp in df_test.group_by("marketplaceCountryCode"):
    dataset = create_dataset(df_mp)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)
    mp_auc[f"{mp}_auc"] = test_loop(model, dataloader).item()
```

**Cursus PyTorchTraining Equivalent**:

```python
# Cursus training configuration
config = Config(
    model_class="bimodal_lstm",  # Or "bimodal_bert", "bimodal_cnn"
    text_name="text",
    label_name="label",
    batch_size=512,
    max_epochs=10,
    lr=1e-3,
    optimizer="AdamW",
    metric_choices=["auroc", "f1_score"],
    early_stop_metric="val/auroc",
    early_stop_patience=3,
    
    # Text processing
    tokenizer="bert-base-multilingual-cased",
    max_sen_len=512,
    
    # Tabular features
    tab_field_list=tabular_features,
    cat_field_list=categorical_features,
    
    # Model architecture
    hidden_common_dim=100,
    num_classes=2,
    is_binary=True,
)

# Cursus training execution
trainer = model_train(
    model,
    config.model_dump(),
    train_dataloader,
    val_dataloader,
    device="auto",
    model_log_path=paths["checkpoint"],
    early_stop_metric=config.early_stop_metric,
)

# Cursus evaluation with metrics
metric_test = compute_metrics(
    test_predict_labels,
    test_true_labels,
    ["auroc", "average_precision", "f1_score"],
    task="binary",
    num_classes=2,
    stage="test",
)
```

**Coverage Analysis**:

‚úÖ **Covered Functionalities**:
- PyTorch-based model training with Lightning infrastructure
- Bimodal architecture support (text + tabular)
- LSTM and Transformer model types
- AdamW optimizer with learning rate scheduling
- Binary classification with BCE loss
- AUC metric computation
- Model checkpointing and saving

‚ö†Ô∏è **Partially Covered Functionalities**:
- Early stopping (Cursus has built-in, Names3Risk manual)
- Distributed training (Cursus supports DDP/FSDP, Names3Risk single GPU)
- Mixed precision training (Cursus supports AMP, Names3Risk doesn't)
- Per-region evaluation (Cursus needs custom implementation)
- Logging format (Cursus uses TensorBoard, Names3Risk uses JSON)

‚ùå **Uncovered Functionalities**:
- Character-level embeddings (Names3Risk uses custom vocab, Cursus uses BERT)
- Custom collate functions for character tokens
- Marketplace-specific AUC computation as built-in metric

**Recommendation**: **Extend existing step** - Use Cursus PyTorchTraining with custom character tokenizer pipeline and custom per-region evaluation callback.

## Unique Names3Risk Functionalities Not Covered by Cursus

### 1. **Character-Level Tokenization for Name Analysis**

**Unique Operations**:
- **Character Vocabulary Building**: Build vocabulary from character frequencies in training data
- **Frequency Filtering**: Filter characters by minimum frequency threshold (default: 2)
- **Character Encoding**: Map each character to unique integer ID
- **Character Padding**: Pad sequences to fixed length with <PAD> token
- **Character UNK Handling**: Map unknown characters to <UNK> token

**Business Value**: Character-level analysis captures name structure patterns that subword tokenization misses, critical for detecting synthetic names and typosquatting.

**Example**:
```python
# Character-level tokenization captures letter-by-letter patterns
"John Smith" ‚Üí [J, o, h, n, ' ', S, m, i, t, h] ‚Üí [10, 15, 8, 14, 1, 19, 13, 9, 20, 8]

# vs BERT subword tokenization
"John Smith" ‚Üí ['John', 'Smith'] ‚Üí [2345, 6789]

# Character-level can detect:
# - Repeated characters: "Jooohn Smiith"
# - Character substitutions: "J0hn Sm1th" (0 for o, 1 for i)
# - Name structure: consonant/vowel patterns
# - Keyboard proximity: "Jonh Smuth" (typos)
```

### 2. **Name-Specific Text Preprocessing**

**Unique Operations**:
- **Multi-Field Name Concatenation**: Combine email, billing name, customer name, payment name
- **Separator-Based Structure**: Use "|" separator to preserve field boundaries
- **No Text Normalization**: Preserve raw character patterns (no lowercasing, no unicode normalization)
- **Name Field Selection**: Specific to fraud-relevant name fields

**Business Value**: Preserving raw character patterns and field structure enables detection of name inconsistencies across different fields (e.g., email vs billing name mismatch).

**Example**:
```python
# Names3Risk text concatenation preserves field boundaries
text = "john.smith@email.com|John Smith|JOHN SMITH|J. Smith"
#      ^-- email              ^-- billing  ^-- customer ^-- payment

# Can detect:
# - Case inconsistencies across fields
# - Name format variations
# - Missing fields (handled with [MISSING])
# - Character-level differences between fields
```

### 3. **Fraud-Specific Data Filtering and Sampling**

**Unique Operations**:
- **Fraud Status Filtering**: Filter by customer status ('N'=normal, 'F'=fraud, 'I'=investigation)
- **First Order Selection**: Keep only first completed order per customer (daysSinceFirstCompletedOrder < 1e-12)
- **Balanced Sampling**: 50% random sampling of normal cases, 100% retention of fraud cases
- **Duplicate Removal**: Deduplicate by customerId, keeping first order by transactionDate
- **Amazon Email Filtering**: Exclude Amazon employee emails (contains "amazon.")

**Business Value**: Creates balanced training dataset focused on first-order fraud patterns while excluding internal test accounts.

**Example**:
```sql
-- Names3Risk fraud-specific filtering
WHERE dedup = 1  -- First order only
  AND ((status = 'N' AND RAND() < 0.5) OR status IN ('F', 'I'))  -- 50% normal, 100% fraud
  AND NOT LOWER(emailDomain) LIKE '%amazon.%'  -- No Amazon emails
```

### 4. **Per-Marketplace Model Evaluation**

**Unique Operations**:
- **Regional AUC Computation**: Separate AUC metrics for each marketplace (US, UK, DE, JP, etc.)
- **Cross-Regional Validation**: Train on all regions, evaluate per-region performance
- **Marketplace Distribution Analysis**: Track data distribution across marketplaces
- **Region-Specific Logging**: Log per-marketplace metrics to JSON

**Business Value**: Enables monitoring of model performance degradation in specific regions and detection of region-specific fraud patterns.

**Example**:
```python
# Names3Risk per-marketplace evaluation
mp_auc = {}
for (mp,), df_mp in df_test.group_by("marketplaceCountryCode"):
    mp_auc[f"{mp}_auc"] = test_loop(model, dataloader).item()

# Output: {'US_auc': 0.8523, 'UK_auc': 0.8412, 'DE_auc': 0.8234, 'JP_auc': 0.8156, ...}
```

### 5. **Bimodal LSTM/Transformer Architecture**

**Unique Operations**:
- **Character Embedding Layer**: Trainable embeddings for character-level tokens
- **Text Projection**: Map character sequences to dense representations
- **Tabular Projection**: Map tabular features to dense representations
- **Concatenation Fusion**: Simple concatenation of text and tabular embeddings
- **Shared Classifier**: Single classifier head for fused representations

**Business Value**: Lightweight architecture optimized for name-based fraud detection with minimal preprocessing overhead.

**Example Architecture**:
```python
# Names3Risk LSTM2Risk architecture
class LSTM2Risk(nn.Module):
    def __init__(self, config):
        self.embedding = nn.Embedding(config.n_embed, config.embed_dim)  # Character embeddings
        self.lstm = nn.LSTM(config.embed_dim, config.hidden_dim, bidirectional=True)
        self.text_projection = nn.Linear(config.hidden_dim * 2, config.common_dim)
        self.tab_projection = nn.Linear(config.n_tab_features, config.common_dim)
        self.classifier = nn.Linear(config.common_dim * 2, 1)
    
    def forward(self, batch):
        # Text processing (character-level)
        text_emb = self.embedding(batch["text"])
        lstm_out, _ = self.lstm(text_emb)
        text_repr = self.text_projection(lstm_out[:, -1, :])
        
        # Tabular processing
        tab_repr = self.tab_projection(batch["tabular"])
        
        # Fusion and classification
        fused = torch.cat([text_repr, tab_repr], dim=-1)
        logits = self.classifier(fused)
        return torch.sigmoid(logits)
```

## Recommended New Cursus Steps

Based on the analysis, we recommend implementing 2 new Cursus steps following the 80/20 base-extension pattern.

### 1A. **CharacterLevelTextPreprocessing Step** (Base Sharable)

**Purpose**: Handle general character-level text tokenization for any domain requiring character-level analysis (names, URLs, code, DNA sequences, etc.)

**Primary Names3Risk Implementation**: 
- `tokenizer.py` (`OrderTextTokenizer` class)
- `train.py` (tokenizer training and application)

**Sharable Features** (80% of functionality):

#### **1. Character Vocabulary Building from Training Data**
**Names3Risk Implementation**: Frequency-based vocabulary construction
```python
class CharacterTokenizer:
    def __init__(self, min_freq=2):
        self.min_freq = min_freq
        self.char_to_idx = {}
        self.idx_to_char = {}
        
    def train(self, texts: List[str]) -> "CharacterTokenizer":
        # Count character frequencies
        char_freq = {}
        for text in texts:
            for char in text:
                char_freq[char] = char_freq.get(char, 0) + 1
        
        # Build vocabulary from frequent characters
        self.char_to_idx = {"<PAD>": 0, "<UNK>": 1}
        self.idx_to_char = {0: "<PAD>", 1: "<UNK>"}
        
        idx = 2
        for char, freq in sorted(char_freq.items(), key=lambda x: -x[1]):
            if freq >= self.min_freq:
                self.char_to_idx[char] = idx
                self.idx_to_char[idx] = char
                idx += 1
        
        self.vocab_size = len(self.char_to_idx)
        return self
```

**Operations**:
- Character frequency analysis across training corpus
- Vocabulary filtering by minimum frequency threshold
- Special token handling (<PAD>, <UNK>)
- Vocabulary serialization for inference

#### **2. Character-Level Encoding and Decoding**
**Names3Risk Implementation**: Character-to-index mapping with padding
```python
def encode(self, text: str, max_length: int = 512) -> List[int]:
    # Character-to-index mapping
    tokens = [self.char_to_idx.get(char, self.unk_token) for char in text[:max_length]]
    # Pad to max_length
    tokens.extend([self.pad_token] * (max_length - len(tokens)))
    return tokens

def decode(self, token_ids: List[int]) -> str:
    # Index-to-character mapping
    chars = [self.idx_to_char.get(idx, "<UNK>") for idx in token_ids]
    # Remove padding
    return "".join(chars).replace("<PAD>", "").replace("<UNK>", "?")
```

**Operations**:
- Character-to-index conversion with UNK handling
- Sequence truncation at max_length
- Padding to fixed length
- Batch encoding support
- Index-to-character decoding

#### **3. Configurable Parameters**
**Cursus Configuration**:
```python
CharacterLevelTextPreprocessingConfig(
    text_field_names=["text"],  # Fields to tokenize
    min_freq=2,                 # Minimum character frequency
    max_length=512,             # Maximum sequence length
    special_tokens={"pad": "<PAD>", "unk": "<UNK>"},
    preserve_case=True,         # Don't lowercase
    vocabulary_path=None,       # Optional pre-built vocab
)
```

**Operations**:
- Configurable text fields
- Adjustable vocabulary filtering
- Flexible sequence length
- Custom special tokens
- Case preservation option

#### **4. Vocabulary Persistence**
**Operations**:
- Save vocabulary to JSON/pickle
- Load pre-trained vocabulary
- Version compatibility checking
- Vocabulary statistics logging

#### **5. Batch Processing Support**
**Operations**:
- Efficient batch encoding
- Parallel tokenization for large datasets
- Memory-efficient streaming mode
- Progress tracking with logging

**Key Interfaces**:
```python
class CharacterLevelTextPreprocessor(Processor):
    def fit(self, texts: pd.Series) -> "CharacterLevelTextPreprocessor":
        """Build character vocabulary from training data."""
        
    def transform(self, texts: pd.Series) -> np.ndarray:
        """Encode texts to character token IDs."""
        
    def save_vocabulary(self, path: str) -> None:
        """Save vocabulary to file."""
        
    def load_vocabulary(self, path: str) -> None:
        """Load pre-trained vocabulary."""
```

**Integration with Cursus Pipeline**:
```python
# Add to text processing pipeline
char_processor = CharacterLevelTextPreprocessor(
    min_freq=2,
    max_length=512,
    preserve_case=True,
)

# Fit on training data
char_processor.fit(train_dataset.DataReader["text"])

# Add to dataset pipelines
train_dataset.add_pipeline("text", char_processor)
val_dataset.add_pipeline("text", char_processor)
test_dataset.add_pipeline("text", char_processor)
```

### 1B. **FraudNameTextPreprocessing Step** (Fraud-Specific Extension)

**Purpose**: Handle fraud-specific name text preprocessing (multi-field concatenation, domain filtering, etc.)

**Primary Names3Risk Implementation**: 
- `fetch_data.py` (text field concatenation logic)
- `train.py` (field-specific preprocessing)

**Fraud-Specific Features** (20% of functionality):

#### **1. Multi-Field Name Concatenation**
**Names3Risk Implementation**: Pipe-separated field concatenation
```python
def concatenate_name_fields(self, df: pd.DataFrame) -> pd.Series:
    """Concatenate multiple name fields with separator."""
    return pl.concat_str(
        [
            pl.col("emailAddress").fill_null("[MISSING]"),
            pl.col("billingAddressName").fill_null("[MISSING]"),
            pl.col("customerName").fill_null("[MISSING]"),
            pl.col("paymentAccountHolderName").fill_null("[MISSING]"),
        ],
        separator="|",
    )
```

**Operations**:
- Multi-field text concatenation
- Configurable field order
- Separator-based structure preservation
- Missing value placeholder

#### **2. Email Domain Filtering**
**Operations**:
- Extract email domain
- Filter internal/test domains (amazon.com)
- Domain-based fraud indicators
- Top-level domain validation

#### **3. Name Consistency Analysis**
**Operations**:
- Cross-field name matching
- Edit distance computation
- Format variation detection
- Character-level alignment

**Cursus Configuration**:
```python
FraudNameTextPreprocessingConfig(
    name_fields=[
        "emailAddress",
        "billingAddressName",
        "customerName",
        "paymentAccountHolderName",
    ],
    separator="|",
    missing_placeholder="[MISSING]",
    exclude_domains=["amazon.com", "amazon.dev"],
    compute_consistency_features=True,
)
```

**Integration Example**:
```python
# Fraud-specific preprocessing
fraud_preprocessor = FraudNameTextPreprocessor(
    name_fields=["emailAddress", "billingAddressName", "customerName"],
    separator="|",
    exclude_domains=["amazon.com"],
)

# Apply to dataset
fraud_preprocessor.fit(train_dataset.DataReader)
train_dataset.add_pipeline("text", fraud_preprocessor)

# Then apply character tokenization
char_processor = CharacterLevelTextPreprocessor(min_freq=2, max_length=512)
char_processor.fit(train_dataset.DataReader["text"])
train_dataset.add_pipeline("text", char_processor)
```

## Implementation Roadmap

### Phase 1: Base Character-Level Tokenization (2 weeks)
**Deliverables**:
1. CharacterLevelTextPreprocessor class with fit/transform interface
2. Vocabulary building and persistence
3. Encoding/decoding methods
4. Unit tests with 90%+ coverage
5. Integration tests with existing Cursus pipeline
6. Documentation and examples

**Success Criteria**:
- ‚úÖ Passes all unit tests
- ‚úÖ Integrates with PipelineDataset
- ‚úÖ Matches Names3Risk tokenization behavior
- ‚úÖ Performance: <1s per 10K samples

### Phase 2: Fraud-Specific Extension (1 week)
**Deliverables**:
1. FraudNameTextPreprocessor class
2. Multi-field concatenation logic
3. Domain filtering functionality
4. Unit tests with 90%+ coverage
5. Integration tests
6. Documentation

**Success Criteria**:
- ‚úÖ Correctly concatenates name fields
- ‚úÖ Filters test/internal domains
- ‚úÖ Integrates with CharacterLevelTextPreprocessor
- ‚úÖ Passes Names3Risk validation tests

### Phase 3: PyTorch Model Integration (1 week)
**Deliverables**:
1. Character embedding layer support in PyTorchTraining
2. Custom collate function for character tokens
3. Per-region evaluation callback
4. Integration tests
5. End-to-end Names3Risk pipeline test

**Success Criteria**:
- ‚úÖ Supports custom vocabulary embeddings
- ‚úÖ Per-marketplace AUC computation
- ‚úÖ Matches Names3Risk training results
- ‚úÖ Full pipeline runs successfully

### Phase 4: Documentation and Migration (1 week)
**Deliverables**:
1. Step configuration guide
2. Migration guide from Names3Risk to Cursus
3. Performance benchmarks
4. Troubleshooting guide
5. Example notebooks

**Success Criteria**:
- ‚úÖ Complete documentation coverage
- ‚úÖ Migration guide tested
- ‚úÖ Performance within 10% of Names3Risk
- ‚úÖ Team training completed

## Migration Strategy

### Short-Term (0-3 months): Hybrid Approach
**Goal**: Run both Names3Risk and Cursus pipelines in parallel

**Actions**:
1. Implement CharacterLevelTextPreprocessing and FraudNameTextPreprocessing steps
2. Create Cursus configuration equivalent to Names3Risk
3. Run shadow mode: Cursus pipeline alongside existing Names3Risk
4. Compare predictions and metrics (target: <1% difference in AUC)
5. Validate model performance across all marketplaces

**Validation Criteria**:
- AUC difference < 0.01 across all marketplaces
- Prediction correlation > 0.99
- Latency within 10% of Names3Risk

### Medium-Term (3-6 months): Gradual Migration
**Goal**: Migrate 50% of Names3Risk traffic to Cursus

**Actions**:
1. Deploy Cursus pipeline to production with 10% traffic
2. Monitor A/B test results (AUC, precision, recall, false positive rate)
3. Gradually increase traffic to 50%
4. Optimize performance bottlenecks
5. Implement per-marketplace monitoring dashboards

**Success Metrics**:
- No degradation in fraud detection rate
- Improved operational efficiency (fewer pipeline failures)
- Reduced maintenance overhead

### Long-Term (6-12 months): Full Migration
**Goal**: Deprecate Names3Risk pipeline, 100% on Cursus

**Actions**:
1. Migrate remaining 50% traffic to Cursus
2. Deprecate Names3Risk codebase
3. Consolidate monitoring and alerting
4. Document lessons learned
5. Train team on Cursus framework

**Success Criteria**:
- ‚úÖ 100% traffic on Cursus
- ‚úÖ Names3Risk codebase archived
- ‚úÖ Zero production incidents during migration
- ‚úÖ Team fully trained on Cursus

## Risk Mitigation

### Technical Risks

#### **Risk 1: Character Tokenization Performance Degradation**
**Probability**: Medium | **Impact**: High

**Mitigation**:
- Implement vocabulary caching
- Use vectorized operations for encoding
- Profile and optimize hot paths
- Add performance benchmarks to CI/CD

**Contingency**: Fall back to Names3Risk tokenizer if performance unacceptable

#### **Risk 2: Model Performance Degradation**
**Probability**: Low | **Impact**: Critical

**Mitigation**:
- Run extensive A/B tests before migration
- Monitor per-marketplace AUC closely
- Implement automatic rollback triggers
- Keep Names3Risk pipeline as backup

**Contingency**: Immediate rollback to Names3Risk if AUC drops >0.01

#### **Risk 3: Integration Complexity**
**Probability**: Medium | **Impact**: Medium

**Mitigation**:
- Extensive integration testing
- Shadow mode for validation
- Incremental rollout strategy
- Detailed documentation

**Contingency**: Extend timeline if integration issues discovered

### Operational Risks

#### **Risk 4: Team Learning Curve**
**Probability**: Medium | **Impact**: Medium

**Mitigation**:
- Comprehensive documentation
- Hands-on training sessions
- Migration runbook
- Dedicated support channel

**Contingency**: Hire Cursus expert or extend training period

#### **Risk 5: Production Incidents During Migration**
**Probability**: Low | **Impact**: High

**Mitigation**:
- Phased rollout (10% ‚Üí 50% ‚Üí 100%)
- Automatic rollback mechanisms
- 24/7 monitoring during migration
- Incident response plan

**Contingency**: Immediate rollback to Names3Risk, incident postmortem

## Conclusion

The Names3Risk pipeline demonstrates **65% compatibility** with existing Cursus framework steps, with complete equivalency for data loading (CradleDataLoading) and partial coverage for model training (PyTorchTraining). The primary gap is character-level text tokenization, which requires implementing 2 new steps following the 80/20 base-extension pattern:

1. **CharacterLevelTextPreprocessing** (base, 80%): General-purpose character tokenization
2. **FraudNameTextPreprocessing** (extension, 20%): Fraud-specific name preprocessing

**Key Recommendations**:

1. ‚úÖ **Use Existing CradleDataLoading**: Direct replacement with Names3Risk-specific SQL configuration
2. ‚ö†Ô∏è **Implement Character Tokenization Step**: Required for name-level fraud detection
3. ‚ö†Ô∏è **Extend PyTorchTraining**: Add character embedding support and per-region evaluation
4. üìã **Phased Migration**: 0-3 months shadow mode, 3-6 months gradual rollout, 6-12 months full migration

**Expected Benefits**:
- üéØ Standardized pipeline infrastructure
- üîß Reduced maintenance overhead
- üìä Improved monitoring and observability
- üöÄ Faster feature development
- üîÑ Better code reusability across fraud models

**Timeline**: 5 weeks total development + 12 months phased migration

---

## Appendix A: Names3Risk vs TSA Comparison

| Aspect | Names3Risk | TSA (Time Series Autoencoder) |
|--------|-----------|-------------------------------|
| **Data Source** | MDS + Andes | EDX + Andes |
| **Text Processing** | Character-level tokenization | No text processing |
| **Model Type** | Bimodal LSTM/Transformer | Tabular Autoencoder |
| **Primary Gap** | Character tokenization | Time series processing |
| **Cursus Compatibility** | 65% | 75% |
| **Required New Steps** | 2 (CharLevel + Fraud) | 1 (TimeSeriesPreprocessing) |
| **Migration Complexity** | Medium | Low |

## Appendix B: References

**Names3Risk Documentation**:
- [Names3Risk Model Design](../1_design/names3risk_model_design.md): Comprehensive model architecture documentation

**Names3Risk Source Code**:
- `projects/names3risk_legacy/fetch_data.py`: Data loading and Cradle job configuration
- `projects/names3risk_legacy/train.py`: Model training and evaluation
- `projects/names3risk_legacy/tokenizer.py`: Character-level tokenization
- `projects/names3risk_legacy/lstm2risk.py`: LSTM model architecture
- `projects/names3risk_legacy/transformer2risk.py`: Transformer model architecture

**Cursus Framework Documentation**:
- `slipbox/0_developer_guide/`: Developer guides and best practices
- `slipbox/1_design/`: Design documents and architecture patterns
- `src/cursus/steps/scripts/pytorch_training.py`: PyTorch training script
- `src/cursus/steps/configs/config_cradle_data_loading_step.py`: Cradle configuration

**Related Analyses**:
- TSA Equivalency Analysis: `slipbox/4_analysis/2024-11-XX_tsa_cursus_step_equivalency_analysis.md`
- Cursus Package Overview: `slipbox/00_entry_points/cursus_package_overview.md`

---

**Document Version**: 1.0  
**Last Updated**: 2025-12-31  
**Authors**: AI Analysis System  
**Reviewers**: [Pending]  
**Status**: Draft - Ready for Review
