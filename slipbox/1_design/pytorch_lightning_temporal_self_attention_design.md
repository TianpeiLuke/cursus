---
tags:
  - design
  - implementation
  - pytorch-lightning
  - temporal-self-attention
  - deep-learning
  - distributed-training
keywords:
  - pytorch lightning
  - temporal self attention
  - order attention
  - feature attention
  - DDP training
  - FSDP training
  - model checkpointing
  - metrics computation
  - ONNX export
topics:
  - machine learning framework
  - distributed training
  - model lifecycle management
  - attention mechanisms
  - fraud detection
language: python
date of note: 2025-10-21
---

# PyTorch Lightning Temporal Self-Attention Design

## Overview

This document outlines the design and implementation of PyTorch Lightning versions of the Temporal Self-Attention (TSA) models. The Lightning implementation preserves the exact TSA model architectures while adding enterprise-grade training infrastructure, distributed training capabilities, automatic checkpointing, comprehensive metrics computation, and ONNX export functionality.

## Related Documents
- **[Original TSA Model Design](./temporal_self_attention_model_design.md)** - Core TSA architecture and components
- **[TSA Cursus Step Equivalency Analysis](../4_analysis/2025-10-20_tsa_cursus_step_equivalency_analysis.md)** - Integration analysis
- **[PyTorch BSM Lightning Models](../../dockers/pytorch_bsm/lightning_models/)** - Reference Lightning implementation patterns

## Design Principles

### Architecture Preservation
- **Exact Model Compatibility**: Maintain identical TSA model behavior and outputs
- **Component Reusability**: Extract attention modules as standalone, reusable components
- **Configuration Compatibility**: Support existing TSA hyperparameters and configurations
- **Backward Compatibility**: Generate identical results to original TSA training

### Lightning Integration Benefits
- **Simplified Training**: Automatic handling of training loops, validation, and testing
- **Distributed Training**: Built-in DDP/FSDP support for multi-GPU and multi-node training
- **Automatic Checkpointing**: Best model selection, early stopping, and resumption
- **Comprehensive Logging**: TensorBoard integration with real-time metrics tracking
- **Production Ready**: ONNX export, model versioning, and deployment utilities

### Modular Design Philosophy
- **Separation of Concerns**: Clear separation between attention mechanisms, training logic, and utilities
- **Component Extraction**: Order and feature attention as standalone modules
- **Flexible Composition**: Easy creation of new TSA variants by combining modules
- **Testing Isolation**: Individual components can be tested independently

## Architecture Overview

```mermaid
flowchart TD
    subgraph "üèóÔ∏è Lightning Module Architecture"
        TSAClass["TemporalSelfAttentionClassification<br/>‚Ä¢ Single-sequence Lightning module<br/>‚Ä¢ Wraps OrderFeatureAttentionClassifier<br/>‚Ä¢ Standard training/validation/test steps"]
        DualTSA["DualSequenceTSA<br/>‚Ä¢ Dual-sequence Lightning module<br/>‚Ä¢ Wraps TwoSeqMoEOrderFeatureAttentionClassifier<br/>‚Ä¢ Gate function integration"]
    end
    
    subgraph "üß© Modular Components"
        OrderAttn["OrderAttentionModule<br/>‚Ä¢ Temporal sequence processing<br/>‚Ä¢ Multi-layer attention stack<br/>‚Ä¢ Time encoding integration<br/>‚Ä¢ Dummy token mechanism"]
        FeatureAttn["FeatureAttentionModule<br/>‚Ä¢ Current transaction processing<br/>‚Ä¢ Feature interaction learning<br/>‚Ä¢ Engineered feature integration<br/>‚Ä¢ Pre-norm attention layers"]
        CoreComp["TSA Core Components<br/>‚Ä¢ TimeEncode (temporal encoding)<br/>‚Ä¢ AttentionLayer (multi-head attention)<br/>‚Ä¢ FeatureAggregation (dimensionality reduction)<br/>‚Ä¢ MixtureOfExperts (sparse routing)"]
    end
    
    subgraph "üìä Training Infrastructure"
        Trainer["Lightning Trainer<br/>‚Ä¢ DDP/FSDP strategy selection<br/>‚Ä¢ Automatic mixed precision<br/>‚Ä¢ Gradient clipping<br/>‚Ä¢ Learning rate scheduling"]
        Callbacks["Training Callbacks<br/>‚Ä¢ ModelCheckpoint (best model selection)<br/>‚Ä¢ EarlyStopping (patience-based)<br/>‚Ä¢ LearningRateMonitor<br/>‚Ä¢ DeviceStatsMonitor"]
        Logger["TensorBoard Logger<br/>‚Ä¢ Real-time metrics tracking<br/>‚Ä¢ Loss curves visualization<br/>‚Ä¢ Attention weight analysis<br/>‚Ä¢ Hyperparameter logging"]
    end
    
    subgraph "üìà Metrics & Evaluation"
        TSAMetrics["TSA Metrics Module<br/>‚Ä¢ AUC-ROC (primary metric)<br/>‚Ä¢ PR-AUC (fraud detection)<br/>‚Ä¢ Precision, Recall, F1<br/>‚Ä¢ Accuracy, Specificity"]
        Plots["Visualization Module<br/>‚Ä¢ ROC curves (binary/multiclass)<br/>‚Ä¢ Precision-Recall curves<br/>‚Ä¢ TensorBoard integration<br/>‚Ä¢ Score distribution analysis"]
        ScoreGen["Score Generation<br/>‚Ä¢ Deployment score files<br/>‚Ä¢ Percentile mapping<br/>‚Ä¢ Fraud probability output<br/>‚Ä¢ Label coordination"]
    end
    
    subgraph "üîß Utilities & Support"
        Losses["Custom Loss Functions<br/>‚Ä¢ FocalLoss (class imbalance)<br/>‚Ä¢ CyclicalFocalLoss (dynamic training)<br/>‚Ä¢ CrossEntropyLoss (standard)<br/>‚Ä¢ BCEWithLogitsLoss (binary)"]
        DataUtils["Data Handling<br/>‚Ä¢ Single/dual sequence support<br/>‚Ä¢ Temporal sequence processing<br/>‚Ä¢ Padding mask coordination<br/>‚Ä¢ Batch processing utilities"]
        Export["ONNX Export<br/>‚Ä¢ Model serialization<br/>‚Ä¢ Input/output specification<br/>‚Ä¢ Dynamic axes support<br/>‚Ä¢ Deployment optimization"]
    end
    
    subgraph "üöÄ Deployment & Production"
        Inference["Inference Pipeline<br/>‚Ä¢ Online inference support<br/>‚Ä¢ Batch prediction<br/>‚Ä¢ Score file generation<br/>‚Ä¢ Model serving utilities"]
        Checkpoints["Model Management<br/>‚Ä¢ FSDP-aware checkpointing<br/>‚Ä¢ Model versioning<br/>‚Ä¢ Artifact management<br/>‚Ä¢ Configuration persistence"]
    end
    
    TSAClass --> OrderAttn
    TSAClass --> FeatureAttn
    DualTSA --> OrderAttn
    DualTSA --> FeatureAttn
    
    OrderAttn --> CoreComp
    FeatureAttn --> CoreComp
    
    TSAClass --> Trainer
    DualTSA --> Trainer
    Trainer --> Callbacks
    Trainer --> Logger
    
    TSAClass --> TSAMetrics
    DualTSA --> TSAMetrics
    TSAMetrics --> Plots
    TSAMetrics --> ScoreGen
    
    TSAClass --> Losses
    DualTSA --> Losses
    TSAClass --> DataUtils
    DualTSA --> DataUtils
    
    TSAClass --> Export
    DualTSA --> Export
    Export --> Inference
    Callbacks --> Checkpoints
```

## Module Structure

### File Organization

```
dockers/pytorch_temporal_self_attention/lightning_models/
‚îú‚îÄ‚îÄ __init__.py                                         # Package initialization
‚îú‚îÄ‚îÄ pl_temporal_self_attention_classification.py       # Single-sequence Lightning module
‚îú‚îÄ‚îÄ pl_dual_sequence_tsa.py                            # Dual-sequence Lightning module
‚îú‚îÄ‚îÄ pl_order_attention.py                              # Order attention module
‚îú‚îÄ‚îÄ pl_feature_attention.py                            # Feature attention module
‚îú‚îÄ‚îÄ pl_tsa_components.py                               # Core TSA components
‚îú‚îÄ‚îÄ pl_tsa_metrics.py                                  # TSA-specific metrics
‚îú‚îÄ‚îÄ pl_tsa_losses.py                                   # Custom loss functions
‚îú‚îÄ‚îÄ pl_tsa_utils.py                                    # Utilities and data handling
‚îú‚îÄ‚îÄ pl_tsa_train.py                                    # Training script
‚îî‚îÄ‚îÄ dist_utils.py                                      # Distributed training utilities
```

### Component Dependencies

```mermaid
graph TD
    A[pl_tsa_components.py] --> B[pl_order_attention.py]
    A --> C[pl_feature_attention.py]
    B --> D[pl_temporal_self_attention_classification.py]
    C --> D
    B --> E[pl_dual_sequence_tsa.py]
    C --> E
    F[pl_tsa_losses.py] --> D
    F --> E
    G[pl_tsa_metrics.py] --> D
    G --> E
    H[pl_tsa_utils.py] --> D
    H --> E
    I[dist_utils.py] --> J[pl_tsa_train.py]
    D --> J
    E --> J
```

## Core Lightning Modules

### TemporalSelfAttentionClassification

The primary Lightning module for single-sequence TSA processing, wrapping the `OrderFeatureAttentionClassifier`.

#### Key Features
- **Single Sequence Processing**: Handles standard temporal sequence classification
- **Lightning Integration**: Full training/validation/test step implementation
- **Metric Computation**: Comprehensive evaluation with fraud detection metrics
- **Checkpointing**: Automatic best model selection based on validation AUC
- **ONNX Export**: Production deployment support

#### Configuration Schema
```python
TSA_SINGLE_CONFIG = {
    # Model Architecture
    "n_cat_features": 53,                    # Number of categorical features
    "n_num_features": 47,                    # Number of numerical features
    "n_classes": 2,                          # Number of output classes
    "n_embedding": 1352,                     # Embedding table size
    "seq_len": 51,                           # Sequence length
    "n_engineered_num_features": 0,          # Engineered features count
    "dim_embedding_table": 128,              # Embedding dimension
    "dim_attn_feedforward": 64,              # Attention feedforward dimension
    
    # Attention Configuration
    "num_heads": 1,                          # Multi-head attention heads
    "n_layers_order": 6,                     # Order attention layers
    "n_layers_feature": 6,                   # Feature attention layers
    "dropout": 0.1,                          # Dropout rate
    "use_moe": True,                         # Enable Mixture of Experts
    "num_experts": 5,                        # Number of experts
    "use_time_seq": True,                    # Enable temporal encoding
    "emb_tbl_use_bias": True,               # Use embedding bias
    
    # Training Configuration
    "lr": 1e-5,                             # Learning rate
    "scheduler_maxlr": 1e-3,                # Scheduler max learning rate
    "weight_decay": 0.0,                    # Weight decay
    "beta1": 0.9,                           # Adam beta1
    "beta2": 0.999,                         # Adam beta2
    "max_epochs": 70,                       # Maximum epochs
    "patience": 10,                         # Early stopping patience
    
    # Loss Configuration
    "loss": "CrossEntropyLoss",             # Loss function type
    "loss_gamma": 2.0,                      # Focal loss gamma
    "loss_alpha": 0.25,                     # Focal loss alpha
    "loss_reduction": "mean",               # Loss reduction method
    
    # Data Configuration
    "use_attn_mask": False,                 # Use attention mask
    "use_key_padding_mask": True,           # Use key padding mask
    "batch_size": 1024,                     # Training batch size
    
    # Metrics Configuration
    "primary_metric": "auroc",              # Primary metric for model selection
    "metric_choices": ["accuracy", "precision", "recall", "f1_score", "auroc", "pr_auc"]
}
```

### DualSequenceTSA

Advanced Lightning module for dual-sequence processing with gate function, wrapping the `TwoSeqMoEOrderFeatureAttentionClassifier`.

#### Key Features
- **Dual Sequence Processing**: Handles Customer ID (CID) and Credit Card ID (CCID) sequences
- **Gate Function**: Dynamic sequence importance weighting
- **MoE Integration**: Mixture of Experts for improved model capacity
- **Cross-Sequence Validation**: Ensures temporal consistency across sequences
- **Advanced Metrics**: Gate score analysis and sequence importance tracking

#### Configuration Schema
```python
TSA_DUAL_CONFIG = {
    # Inherits from TSA_SINGLE_CONFIG with additions:
    
    # Dual Sequence Specific
    "gate_embedding_dim": 16,               # Gate function embedding dimension
    "gate_hidden_dim": 256,                 # Gate function hidden dimension
    "gate_threshold": 0.05,                 # Minimum gate score threshold
    
    # Sequence Processing
    "cid_sequence_key": "x_seq_cat_cid",    # Customer ID sequence key
    "ccid_sequence_key": "x_seq_cat_ccid",  # Credit Card ID sequence key
    "time_cid_key": "time_to_last_cid",     # CID temporal key
    "time_ccid_key": "time_to_last_ccid",   # CCID temporal key
    
    # Gate Function Configuration
    "gate_layers": 1,                       # Gate attention layers
    "gate_use_moe": False,                  # Disable MoE for gate function
    "gate_use_time_seq": False,             # Disable temporal for gate function
    
    # Advanced Metrics
    "track_gate_scores": True,              # Track gate score statistics
    "gate_analysis": True,                  # Enable gate function analysis
    "sequence_importance_logging": True      # Log sequence importance metrics
}
```

## Modular Components

### OrderAttentionModule

Standalone module for temporal sequence processing, extracted from the original `OrderAttentionLayer`.

#### Key Features
- **Temporal Processing**: Handles time-aware sequence attention
- **Feature Aggregation**: MLP-based feature dimensionality reduction
- **Multi-Layer Attention**: Configurable attention layer depth
- **Dummy Token**: Learnable sequence-level representation
- **Flexible Configuration**: Supports various attention configurations

#### Implementation Details

The OrderAttentionModule will be a Lightning-compatible wrapper around the existing `OrderAttentionLayer` from the TSA implementation. Based on the actual TSA model structure:

```python
class OrderAttentionModule(nn.Module):
    """
    Lightning-compatible wrapper for OrderAttentionLayer.
    
    This module wraps the existing OrderAttentionLayer from TSA scripts
    to provide a standalone, reusable component for Lightning modules.
    """
    
    def __init__(self, config: Dict[str, Union[int, float, str, bool]]):
        super().__init__()
        
        # Import the actual OrderAttentionLayer from TSA scripts
        from ...tsa.scripts.basic_blocks import OrderAttentionLayer
        
        # Core configuration matching TSA model
        self.n_cat_features = config["n_cat_features"]
        self.n_num_features = config["n_num_features"]
        self.n_embedding = config["n_embedding"]
        self.seq_len = config.get("seq_len", 51)
        self.dim_embed = 2 * config["dim_embedding_table"]
        self.use_time_seq = config.get("use_time_seq", True)
        self.return_seq = config.get("return_seq", False)
        
        # Shared embedding table
        self.embedding = nn.Embedding(
            self.n_embedding + 2, 
            config["dim_embedding_table"], 
            padding_idx=0
        )
        
        # Use the actual OrderAttentionLayer from TSA
        self.order_attention = OrderAttentionLayer(
            self.n_cat_features,
            self.n_num_features,
            self.n_embedding,
            self.seq_len,
            self.dim_embed,
            config.get("dim_attn_feedforward", 64),
            self.embedding,
            config.get("num_heads", 1),
            config.get("dropout", 0.1),
            config.get("n_layers_order", 6),
            config.get("emb_tbl_use_bias", True),
            config.get("use_moe", True),
            config.get("num_experts", 5),
            self.use_time_seq,
            self.return_seq,
        )
    
    def forward(
        self, 
        x_cat: torch.Tensor, 
        x_num: torch.Tensor, 
        time_seq: Optional[torch.Tensor] = None,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass using the original OrderAttentionLayer.
        
        Args:
            x_cat: Categorical features [B, L, n_cat_features]
            x_num: Numerical features [B, L, n_num_features]
            time_seq: Time sequence information [B, L, 1]
            attn_mask: Attention mask [L, L]
            key_padding_mask: Key padding mask [B, L]
            
        Returns:
            Sequence representation [B, dim_embed]
        """
        return self.order_attention(x_cat, x_num, time_seq, attn_mask, key_padding_mask)
```

### FeatureAttentionModule

Standalone module for current transaction feature processing, extracted from the original `FeatureAttentionLayer`.

#### Key Features
- **Current Transaction Focus**: Processes the most recent transaction features
- **Pre-Norm Architecture**: Uses pre-normalization for improved training stability
- **Engineered Feature Integration**: Combines sequence features with static features
- **Cross-Feature Interactions**: Learns complex feature relationships through attention

#### Implementation Details
```python
class FeatureAttentionModule(nn.Module):
    """
    Standalone Feature Attention module for current transaction processing.
    
    Based on the original FeatureAttentionLayer from TSA, this module provides:
    - Current transaction feature attention processing
    - Integration of categorical, numerical, and engineered features
    - Pre-normalization attention layers for training stability
    - Cross-feature interaction learning
    """
    
    def __init__(self, config: Dict[str, Union[int, float, str, bool]]):
        super().__init__()
        
        # Core configuration
        self.n_cat_features = config["n_cat_features"]
        self.n_num_features = config["n_num_features"]
        self.n_embedding = config["n_embedding"]
        self.n_engineered_num_features = config.get("n_engineered_num_features", 0)
        self.dim_embed = 2 * config["dim_embedding_table"]
        self.embedding_table_dim = config["dim_embedding_table"]
        
        # Embedding tables
        self.embedding = nn.Embedding(
            self.n_embedding + 2, 
            self.embedding_table_dim, 
            padding_idx=0
        )
        
        self.embedding_engineered = nn.Embedding(
            self.n_engineered_num_features + 1, 
            self.embedding_table_dim, 
            padding_idx=0
        )
        
        # Pre-norm attention layers
        self.layer_stack_feature = nn.ModuleList([
            AttentionLayerPreNorm(
                dim_embed=self.embedding_table_dim,
                dim_attn_feedforward=config.get("dim_attn_feedforward", 64),
                num_heads=config.get("num_heads", 1),
                dropout=config.get("dropout", 0.1),
                use_moe=config.get("use_moe", True),
                num_experts=config.get("num_experts", 5)
            )
            for _ in range(config.get("n_layers_feature", 6))
        ])
        
        # Layer normalization
        self.layer_norm_engineered = nn.LayerNorm(self.embedding_table_dim)
        
        # Optional embedding bias
        if config.get("emb_tbl_use_bias", True):
            self.emb_tbl_bias = nn.Parameter(
                torch.randn(self.n_cat_features + self.n_num_features, self.embedding_table_dim)
            )
            
            if self.n_engineered_num_features > 0:
                self.engineered_emb_tbl_bias = nn.Parameter(
                    torch.randn(self.n_engineered_num_features, self.embedding_table_dim)
                )
            else:
                self.engineered_emb_tbl_bias = None
        else:
            self.emb_tbl_bias = None
            self.engineered_emb_tbl_bias = None
    
    def forward(
        self, 
        x_cat: torch.Tensor, 
        x_num: torch.Tensor, 
        x_engineered: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass for feature attention processing.
        
        Args:
            x_cat: Categorical features [B, L, n_cat_features]
            x_num: Numerical features [B, L, n_num_features]
            x_engineered: Engineered features [B, n_engineered_num_features]
            
        Returns:
            Feature representation [B, embedding_table_dim]
        """
        B, L = x_cat.shape[:2]
        
        # Extract last order (current transaction) features
        cat_indices = x_cat.int()
        x_cat_all = self.embedding(cat_indices)  # [B, L, n_cat_features, embed_dim]
        x_cat_last = x_cat_all[:, -1, :, :]  # [B, n_cat_features, embed_dim]
        
        # Numerical feature embeddings for last order
        num_indices = torch.arange(
            self.n_embedding - self.n_num_features + 1,
            self.n_embedding + 1,
            device=x_cat.device
        ).repeat(B, L).view(B, L, -1)
        
        x_num_all = self.embedding(num_indices) * x_num.unsqueeze(-1)
        x_num_last = x_num_all[:, -1, :, :]  # [B, n_num_features, embed_dim]
        
        # Combine categorical and numerical features
        x_last = torch.cat([x_cat_last, x_num_last], dim=1)  # [B, n_cat+n_num, embed_dim]
        
        # Apply bias if configured
        if self.emb_tbl_bias is not None:
            x_last = x_last + self.emb_tbl_bias[None, :, :]
        
        # Add engineered features if provided
        if self.n_engineered_num_features > 0 and x_engineered is not None:
            x_engineered = x_engineered.float()
            engineered_indices = torch.arange(
                1, self.n_engineered_num_features + 1, 
                device=x_cat.device
            )
            
            x_engineered_emb = self.embedding_engineered(engineered_indices) * x_engineered.unsqueeze(-1)
            
            if self.engineered_emb_tbl_bias is not None:
                x_engineered_emb = x_engineered_emb + self.engineered_emb_tbl_bias[None, :, :]
            
            # Add dummy engineered token
            dummy_engineered = self.embedding_engineered(
                torch.zeros(B, 1, dtype=torch.int, device=x_cat.device)
            )
            
            x_last = torch.cat([x_last, x_engineered_emb, dummy_engineered], dim=1)
        else:
            # Add dummy engineered token even if no engineered features
            dummy_engineered = self.embedding_engineered(
                torch.zeros(B, 1, dtype=torch.int, device=x_cat.device)
            )
            x_last = torch.cat([x_last, dummy_engineered], dim=1)
        
        # Transpose for attention layers (seq_len, B, E)
        x_last = x_last.permute(1, 0, 2)
        x_last = self.layer_norm_engineered(x_last)
        
        # Multi-layer feature attention processing
        for att_layer_feature in self.layer_stack_feature:
            x_last = att_layer_feature(x_last, None, None)
        
        # Return final feature representation (dummy token)
        return x_last[-1, :, :]  # [B, embedding_table_dim]
```

## Core TSA Components

### TimeEncode

Learnable temporal position encoding module that captures both short-term and long-term temporal patterns.

```python
class TimeEncode(nn.Module):
    """
    Learnable temporal position encoding using periodic functions.
    
    This module encodes temporal information using a combination of:
    - Linear transformation for direct time representation
    - Sinusoidal functions for periodic patterns
    - Learnable parameters for domain adaptation
    """
    
    def __init__(self, time_dim: int, device=None, dtype=None):
        super().__init__()
        factory_kwargs = {"device": device, "dtype": dtype}
        
        self.time_dim = time_dim
        
        # Learnable weight matrix and bias
        self.weight = nn.Parameter(torch.empty((time_dim, 1), **factory_kwargs))
        self.emb_tbl_bias = nn.Parameter(torch.empty(time_dim, **factory_kwargs))
        
        self.reset_parameters()
    
    def reset_parameters(self) -> None:
        # Kaiming uniform initialization
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        nn.init.uniform_(self.emb_tbl_bias, -bound, bound)
    
    def forward(self, tt: torch.Tensor) -> torch.Tensor:
        """
        Encode temporal information.
        
        Args:
            tt: Time tensor [B, L, 1] or [L, B, 1]
            
        Returns:
            Temporal encoding [L, B, time_dim]
        """
        tt = tt.unsqueeze(-1)  # Add feature dimension
        
        # Sinusoidal encoding
        out2 = torch.sin(F.linear(tt, self.weight[1:, :], self.emb_tbl_bias[1:]))
        
        # Linear encoding
        out1 = F.linear(tt, self.weight[0:1, :], self.emb_tbl_bias[0:1])
        
        # Combine encodings
        t = torch.cat([out1, out2], -1)
        t = t.squeeze(-2)  # Remove extra dimension
        t = t.permute(1, 0, 2)  # [L, B, time_dim]
        
        return t
```

### AttentionLayer

Core multi-head attention layer with temporal encoding and MoE support.

```python
class AttentionLayer(nn.Module):
    """
    Multi-head attention layer with temporal encoding and MoE support.
    
    Features:
    - Temporal multi-head attention or standard multi-head attention
    - Mixture of Experts feedforward network
    - Layer normalization and dropout
    - Residual connections
    """
    
    def __init__(
        self,
        dim_embed: int,
        dim_attn_feedforward: int,
        num_heads: int = 1,
        dropout: float = 0.1,
        use_moe: bool = True,
        num_experts: int = 5,
        use_time_seq: bool = True
    ):
        super().__init__()
        
        self.dim_embed = dim_embed
        self.use_time_seq = use_time_seq
        
        # Multi-head attention
        if use_time_seq:
            self.multi_attn = TemporalMultiheadAttention(
                dim_embed, num_heads, dropout=dropout
            )
        else:
            self.multi_attn = nn.MultiheadAttention(
                dim_embed, num_heads, dropout=dropout
            )
        
        # Feedforward network
        if use_moe:
            self.feedforward = MixtureOfExperts(
                dim_embed, num_experts, dim_attn_feedforward
            )
        else:
            self.feedforward = nn.Sequential(
                nn.Linear(dim_embed, dim_attn_feedforward),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(dim_attn_feedforward, dim_embed),
                nn.Dropout(dropout)
            )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(dim_embed)
        self.norm2 = nn.LayerNorm(dim_embed)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self,
        x: torch.Tensor,
        time_seq: Optional[torch.Tensor] = None,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Forward pass for attention layer.
        
        Args:
            x: Input tensor [L, B, E]
            time_seq: Time sequence [L, B, time_dim]
            attn_mask: Attention mask [L, L]
            key_padding_mask: Key padding mask [B, L]
            
        Returns:
            Output tensor [L, B, E]
        """
        # Multi-head attention with residual connection
        if self.use_time_seq and time_seq is not None:
            attn_output, _ = self.multi_attn(
                x, x, x, time_seq, attn_mask=attn_mask, key_padding_mask=key_padding_mask
            )
        else:
            attn_output, _ = self.multi_attn(
                x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask
            )
        
        x = self.norm1(x + self.dropout(attn_output))
        
        # Feedforward with residual connection
        ff_output = self.feedforward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

### FeatureAggregation

Dimensionality reduction module for feature aggregation.

```python
class FeatureAggregation(nn.Module):
    """
    Feature aggregation module for dimensionality reduction.
    
    Reduces feature dimensions using MLP-based aggregation with
    learnable parameters for optimal feature combination.
    """
    
    def __init__(self, n_features: int):
        super().__init__()
        self.n_features = n_features
        self.aggregation = nn.Linear(n_features, 1)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Aggregate features across the feature dimension.
        
        Args:
            x: Input tensor [B, L, E, n_features]
            
        Returns:
            Aggregated tensor [B, L, E, 1]
        """
        return self.aggregation(x)
```

### MixtureOfExperts

Sparse expert routing for improved model capacity.

```python
class MixtureOfExperts(nn.Module):
    """
    Mixture of Experts module for sparse expert routing.
    
    Features:
    - Multiple expert networks for specialized processing
    - Gating network for expert selection
    - Sparse routing for computational efficiency
    - Load balancing for expert utilization
    """
    
    def __init__(self, dim_embed: int, num_experts: int, dim_feedforward: int):
        super().__init__()
        
        self.num_experts = num_experts
        self.dim_embed = dim_embed
        
        # Expert networks
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim_embed, dim_feedforward),
                nn.ReLU(),
                nn.Linear(dim_feedforward, dim_embed)
            )
            for _ in range(num_experts)
        ])
        
        # Gating network
        self.gate = nn.Linear(dim_embed, num_experts)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with expert routing.
        
        Args:
            x: Input tensor [L, B, E]
            
        Returns:
            Expert-processed tensor [L, B, E]
        """
        L, B, E = x.shape
        
        # Compute gate scores
        gate_scores = F.softmax(self.gate(x), dim=-1)  # [L, B, num_experts]
        
        # Process through experts
        expert_outputs = []
        for expert in self.experts:
            expert_outputs.append(expert(x))  # [L, B, E]
        
        expert_outputs = torch.stack(expert_outputs, dim=-1)  # [L, B, E, num_experts]
        
        # Weighted combination of expert outputs
        output = torch.sum(expert_outputs * gate_scores.unsqueeze(-2), dim=-1)  # [L, B, E]
        
        return output
```

## Training Infrastructure

### Lightning Trainer Configuration

The Lightning trainer provides enterprise-grade training capabilities with automatic distributed training, mixed precision, and comprehensive logging.

#### Trainer Setup
```python
def setup_trainer(config: Dict, model_log_path: str) -> pl.Trainer:
    """
    Configure PyTorch Lightning trainer for TSA models.
    
    Features:
    - Automatic DDP/FSDP strategy selection
    - Mixed precision training
    - Comprehensive callbacks
    - TensorBoard logging
    - Model checkpointing
    """
    
    # Strategy selection based on available resources
    if torch.cuda.device_count() > 1:
        if config.get("use_fsdp", False):
            strategy = FSDPStrategy(
                auto_wrap_policy=tsa_auto_wrap_policy,
                mixed_precision=MixedPrecision(
                    param_dtype=torch.float16,
                    reduce_dtype=torch.float16,
                    buffer_dtype=torch.float16
                ) if config.get("use_fp16", False) else None
            )
        else:
            strategy = DDPStrategy(find_unused_parameters=False)
    else:
        strategy = "auto"
    
    # Callbacks
    callbacks = [
        ModelCheckpoint(
            dirpath=os.path.join(model_log_path, "checkpoints"),
            filename="tsa-{epoch:02d}-{val_auroc:.3f}",
            monitor="val/auroc",
            mode="max",
            save_top_k=1,
            save_weights_only=False
        ),
        EarlyStopping(
            monitor="val/auroc",
            patience=config.get("patience", 10),
            mode="max",
            verbose=True
        ),
        LearningRateMonitor(logging_interval="step"),
        DeviceStatsMonitor(cpu_stats=False),
        TQDMProgressBar(refresh_rate=10)
    ]
    
    # Logger
    logger = TensorBoardLogger(
        save_dir=model_log_path,
        name="tsa_lightning_logs"
    )
    
    # Trainer configuration
    trainer = pl.Trainer(
        max_epochs=config.get("max_epochs", 70),
        accelerator="gpu" if torch.cuda.is_available() else "cpu",
        devices="auto",
        strategy=strategy,
        precision=16 if config.get("use_fp16", False) else 32,
        gradient_clip_val=config.get("gradient_clip_val", 1.0),
        accumulate_grad_batches=config.get("accumulate_grad_batches", 1),
        val_check_interval=config.get("val_check_interval", 1.0),
        callbacks=callbacks,
        logger=logger,
        enable_checkpointing=True,
        enable_progress_bar=True,
        enable_model_summary=True,
        sync_batchnorm=True if torch.cuda.device_count() > 1 else False
    )
    
    return trainer
```

### FSDP Auto-Wrap Policy

Custom FSDP wrapping policy for TSA models to optimize memory usage and training efficiency.

```python
def tsa_auto_wrap_policy(
    module: nn.Module,
    recurse: bool,
    unwrapped_params: int,
    min_num_params: int = 1e5
) -> bool:
    """
    Custom FSDP auto-wrap policy for TSA models.
    
    Wraps:
    - OrderAttentionModule (temporal processing)
    - FeatureAttentionModule (feature processing)
    - AttentionLayer (attention computation)
    - Large linear layers and embeddings
    """
    return (
        isinstance(module, (
            OrderAttentionModule,
            FeatureAttentionModule,
            AttentionLayer,
            nn.Linear,
            nn.Embedding
        )) and unwrapped_params >= min_num_params
    )
```

## Metrics and Evaluation

### TSA Metrics Module

Comprehensive metrics computation specifically designed for fraud detection and TSA model evaluation.

#### Key Features
- **Fraud Detection Metrics**: AUC-ROC, PR-AUC, precision, recall optimized for imbalanced data
- **Lightning Integration**: Automatic logging and TensorBoard visualization
- **Score Generation**: Deployment-ready score files with percentile mapping
- **Gate Analysis**: Dual-sequence model gate function evaluation

#### Implementation
```python
class TSAMetrics:
    """
    Comprehensive metrics computation for TSA models.
    
    Provides fraud detection specific metrics with Lightning integration:
    - AUC-ROC (primary metric for model selection)
    - PR-AUC (critical for imbalanced fraud data)
    - Standard classification metrics
    - Score generation for deployment
    - Gate function analysis for dual-sequence models
    """
    
    SUPPORTED_METRICS = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "auroc": auroc,
        "pr_auc": average_precision,
        "specificity": specificity
    }
    
    @staticmethod
    def compute_metrics(
        preds: torch.Tensor,
        targets: torch.Tensor,
        metric_choices: List[str],
        task: str = "binary",
        stage: str = None
    ) -> Dict[str, torch.Tensor]:
        """
        Compute TSA-specific metrics.
        
        Args:
            preds: Model predictions [B, n_classes] or [B] for binary
            targets: Ground truth labels [B]
            metric_choices: List of metrics to compute
            task: "binary" or "multiclass"
            stage: Stage prefix ("train", "val", "test")
            
        Returns:
            Dictionary of computed metrics
        """
        prefix = f"{stage}/" if stage else ""
        metrics = {}
        
        # Convert predictions to probabilities if needed
        if task == "binary":
            if preds.dim() > 1:
                preds = F.softmax(preds, dim=-1)[:, 1]  # Positive class probability
        else:
            if preds.dim() > 1:
                preds = F.softmax(preds, dim=-1)
        
        # Compute each requested metric
        for metric_name in metric_choices:
            if metric_name not in TSAMetrics.SUPPORTED_METRICS:
                continue
                
            metric_fn = TSAMetrics.SUPPORTED_METRICS[metric_name]
            
            try:
                if metric_name == "pr_auc":
                    # Use average_precision for PR-AUC
                    value = metric_fn(preds, targets, task=task)
                else:
                    value = metric_fn(preds, targets, task=task)
                
                metrics[f"{prefix}{metric_name}"] = value
                
            except Exception as e:
                print(f"Warning: Failed to compute {metric_name}: {e}")
                metrics[f"{prefix}{metric_name}"] = torch.tensor(0.0)
        
        return metrics
    
    @staticmethod
    def generate_score_files(
        preds: torch.Tensor,
        targets: torch.Tensor,
        output_dir: str,
        stage: str = "test"
    ):
        """
        Generate TSA-compatible score files for deployment.
        
        Creates:
        - score_file.csv: Predictions with percentile mapping
        - tag_file.csv: Ground truth labels
        - score_distribution.png: Score distribution analysis
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # Convert to numpy for processing
        preds_np = preds.cpu().numpy()
        targets_np = targets.cpu().numpy()
        
        # Generate percentiles for score mapping
        percentiles = np.percentile(preds_np, np.arange(0, 101, 1))
        
        # Create score file
        score_df = pd.DataFrame({
            'score': preds_np,
            'percentile': [
                np.searchsorted(percentiles, score, side='right') - 1
                for score in preds_np
            ]
        })
        
        score_file_path = os.path.join(output_dir, f"{stage}_score_file.csv")
        score_df.to_csv(score_file_path, index=False)
        
        # Create tag file
        tag_df = pd.DataFrame({'label': targets_np})
        tag_file_path = os.path.join(output_dir, f"{stage}_tag_file.csv")
        tag_df.to_csv(tag_file_path, index=False)
        
        print(f"Score files generated: {score_file_path}, {tag_file_path}")
```

### Visualization and Plotting

Enhanced visualization capabilities for TSA model analysis and interpretation.

```python
def plot_tsa_curves(
    y_pred: torch.Tensor,
    y_true: torch.Tensor,
    y_val_pred: torch.Tensor,
    y_val_true: torch.Tensor,
    output_path: str,
    writer: Optional[SummaryWriter] = None,
    global_step: int = 0
):
    """
    Generate TSA-specific ROC and PR curves for fraud detection analysis.
    
    Features:
    - Binary classification ROC/PR curves
    - Test vs validation comparison
    - TensorBoard integration
    - Score distribution analysis
    - Fraud detection specific visualizations
    """
    
    # Ensure tensors are on CPU
    y_pred = y_pred.detach().cpu()
    y_true = y_true.detach().cpu()
    y_val_pred = y_val_pred.detach().cpu()
    y_val_true = y_val_true.detach().cpu()
    
    # ROC Curve
    fpr_test, tpr_test, _ = roc(y_pred, y_true, task="binary")
    fpr_val, tpr_val, _ = roc(y_val_pred, y_val_true, task="binary")
    auc_test = auroc(y_pred, y_true, task="binary")
    auc_val = auroc(y_val_pred, y_val_true, task="binary")
    
    plt.figure(figsize=(12, 5))
    
    # ROC subplot
    plt.subplot(1, 2, 1)
    plt.plot(fpr_test, tpr_test, color='red', lw=2, label=f'Test AUC = {auc_test:.3f}')
    plt.plot(fpr_val, tpr_val, color='blue', lw=2, label=f'Val AUC = {auc_val:.3f}')
    plt.plot([0, 1], [0, 1], 'k--', lw=1)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('TSA ROC Curve (Fraud Detection)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # PR Curve
    precision_test, recall_test, _ = precision_recall_curve(y_pred, y_true, task="binary")
    precision_val, recall_val, _ = precision_recall_curve(y_val_pred, y_val_true, task="binary")
    ap_test = average_precision(y_pred, y_true, task="binary")
    ap_val = average_precision(y_val_pred, y_val_true, task="binary")
    
    plt.subplot(1, 2, 2)
    plt.plot(recall_test, precision_test, color='red', lw=2, label=f'Test AP = {ap_test:.3f}')
    plt.plot(recall_val, precision_val, color='blue', lw=2, label=f'Val AP = {ap_val:.3f}')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('TSA Precision-Recall Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save plot
    plot_path = os.path.join(output_path, "tsa_performance_curves.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    
    # Log to TensorBoard if available
    if writer:
        writer.add_figure("TSA/Performance_Curves", plt.gcf(), global_step=global_step)
    
    plt.close()
    
    print(f"Performance curves saved: {plot_path}")
```

## Custom Loss Functions

### TSA Loss Functions

Specialized loss functions for fraud detection and class imbalance handling.

```python
class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance in fraud detection.
    
    Features:
    - Automatic down-weighting of easy examples
    - Configurable focusing parameter (gamma)
    - Class balancing with alpha parameter
    - Supports both binary and multiclass classification
    """
    
    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, reduction: str = "mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Compute focal loss.
        
        Args:
            inputs: Model logits [B, n_classes]
            targets: Ground truth labels [B]
            
        Returns:
            Focal loss value
        """
        ce_loss = F.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        else:
            return focal_loss


class CyclicalFocalLoss(nn.Module):
    """
    Cyclical Focal Loss with dynamic gamma adjustment during training.
    
    Features:
    - Dynamic focusing parameter based on training progress
    - Cyclical adjustment for improved convergence
    - Automatic class balancing
    - Fraud detection optimization
    """
    
    def __init__(
        self,
        alpha: float = 0.25,
        gamma_min: float = 1.0,
        gamma_max: float = 3.0,
        cycle_length: int = 1000,
        reduction: str = "mean"
    ):
        super().__init__()
        self.alpha = alpha
        self.gamma_min = gamma_min
        self.gamma_max = gamma_max
        self.cycle_length = cycle_length
        self.reduction = reduction
        self.step_count = 0
    
    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Compute cyclical focal loss with dynamic gamma.
        
        Args:
            inputs: Model logits [B, n_classes]
            targets: Ground truth labels [B]
            
        Returns:
            Cyclical focal loss value
        """
        # Compute dynamic gamma
        cycle_position = (self.step_count % self.cycle_length) / self.cycle_length
        gamma = self.gamma_min + (self.gamma_max - self.gamma_min) * (
            0.5 * (1 + math.cos(math.pi * cycle_position))
        )
        
        # Compute focal loss with dynamic gamma
        ce_loss = F.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** gamma * ce_loss
        
        self.step_count += 1
        
        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        else:
            return focal_loss
```

## Data Handling and Utilities

### TSA Data Processing

Utilities for handling TSA-specific data formats and preprocessing requirements.

```python
class TSADataProcessor:
    """
    Data processing utilities for TSA models.
    
    Features:
    - Single and dual sequence processing
    - Temporal sequence handling
    - Padding mask generation
    - Batch processing optimization
    - Memory-efficient data loading
    """
    
    @staticmethod
    def process_single_sequence_batch(batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Process batch for single-sequence TSA model.
        
        Args:
            batch: Raw batch data
            
        Returns:
            Processed batch ready for model input
        """
        processed = {}
        
        # Extract sequence features
        processed['x_cat'] = batch['x_seq_cat'].float()
        processed['x_num'] = batch['x_seq_num'].float()
        processed['x_engineered'] = batch.get('x_engineered', torch.zeros(batch['x_seq_cat'].size(0), 0))
        
        # Extract temporal information
        if 'time_to_last' in batch:
            processed['time_seq'] = batch['time_to_last'].float().unsqueeze(-1)
        else:
            processed['time_seq'] = None
        
        # Extract labels
        processed['labels'] = batch['label'].long()
        
        # Generate padding masks
        processed['key_padding_mask'] = TSADataProcessor.generate_padding_mask(
            processed['x_cat']
        )
        
        return processed
    
    @staticmethod
    def process_dual_sequence_batch(batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Process batch for dual-sequence TSA model.
        
        Args:
            batch: Raw batch data
            
        Returns:
            Processed batch ready for dual-sequence model input
        """
        processed = {}
        
        # CID sequence
        processed['x_seq_cat_cid'] = batch['x_seq_cat_cid'].float()
        processed['x_seq_num_cid'] = batch['x_seq_num_cid'].float()
        processed['time_seq_cid'] = batch.get('time_to_last_cid', torch.zeros_like(batch['x_seq_cat_cid'][:, :, :1])).float()
        
        # CCID sequence
        processed['x_seq_cat_ccid'] = batch['x_seq_cat_ccid'].float()
        processed['x_seq_num_ccid'] = batch['x_seq_num_ccid'].float()
        processed['time_seq_ccid'] = batch.get('time_to_last_ccid', torch.zeros_like(batch['x_seq_cat_ccid'][:, :, :1])).float()
        
        # Engineered features
        processed['x_engineered'] = batch.get('x_engineered', torch.zeros(batch['x_seq_cat_cid'].size(0), 0))
        
        # Labels
        processed['labels'] = batch['label'].long()
        
        # Generate padding masks for both sequences
        processed['key_padding_mask_cid'] = TSADataProcessor.generate_padding_mask(
            processed['x_seq_cat_cid']
        )
        processed['key_padding_mask_ccid'] = TSADataProcessor.generate_padding_mask(
            processed['x_seq_cat_ccid']
        )
        
        return processed
    
    @staticmethod
    def generate_padding_mask(x: torch.Tensor, pad_value: float = 0.0) -> torch.Tensor:
        """
        Generate padding mask for variable-length sequences.
        
        Args:
            x: Input tensor [B, L, F]
            pad_value: Padding value to identify padded positions
            
        Returns:
            Padding mask [B, L] where True indicates padding
        """
        # Check if all features in a position are pad_value
        mask = (x == pad_value).all(dim=-1)  # [B, L]
        return mask
```

## ONNX Export and Deployment

### Model Export

Production-ready model export with ONNX support for deployment optimization.

```python
class TSAModelExporter:
    """
    Model export utilities for TSA Lightning models.
    
    Features:
    - ONNX export with dynamic axes
    - Input/output specification
    - Deployment optimization
    - Model versioning support
    """
    
    @staticmethod
    def export_to_onnx(
        model: pl.LightningModule,
        output_path: str,
        example_input: Dict[str, torch.Tensor],
        opset_version: int = 11
    ):
        """
        Export TSA Lightning model to ONNX format.
        
        Args:
            model: Trained Lightning model
            output_path: Output ONNX file path
            example_input: Example input for tracing
            opset_version: ONNX opset version
        """
        model.eval()
        
        # Prepare input tuple for ONNX export
        if hasattr(model, 'model_type') and model.model_type == 'dual_sequence':
            input_tuple = (
                example_input['x_seq_cat_cid'],
                example_input['x_seq_num_cid'],
                example_input['time_seq_cid'],
                example_input['x_seq_cat_ccid'],
                example_input['x_seq_num_ccid'],
                example_input['time_seq_ccid'],
                example_input['x_engineered']
            )
            input_names = [
                'x_seq_cat_cid', 'x_seq_num_cid', 'time_seq_cid',
                'x_seq_cat_ccid', 'x_seq_num_ccid', 'time_seq_ccid',
                'x_engineered'
            ]
        else:
            input_tuple = (
                example_input['x_cat'],
                example_input['x_num'],
                example_input['x_engineered'],
                example_input['time_seq']
            )
            input_names = ['x_cat', 'x_num', 'x_engineered', 'time_seq']
        
        # Dynamic axes for variable batch size
        dynamic_axes = {name: {0: 'batch_size'} for name in input_names}
        dynamic_axes['output'] = {0: 'batch_size'}
        
        # Export to ONNX
        torch.onnx.export(
            model,
            input_tuple,
            output_path,
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,
            input_names=input_names,
            output_names=['output'],
            dynamic_axes=dynamic_axes,
            verbose=False
        )
        
        print(f"Model exported to ONNX: {output_path}")
        
        # Verify exported model
        TSAModelExporter.verify_onnx_model(output_path, example_input)
    
    @staticmethod
    def verify_onnx_model(onnx_path: str, example_input: Dict[str, torch.Tensor]):
        """
        Verify exported ONNX model functionality.
        
        Args:
            onnx_path: Path to ONNX model
            example_input: Example input for verification
        """
        try:
            import onnxruntime as ort
            
            # Load ONNX model
            session = ort.InferenceSession(onnx_path)
            
            # Prepare input for ONNX Runtime
            ort_inputs = {}
            for input_meta in session.get_inputs():
                input_name = input_meta.name
                if input_name in example_input:
                    ort_inputs[input_name] = example_input[input_name].cpu().numpy()
            
            # Run inference
            ort_outputs = session.run(None, ort_inputs)
            
            print(f"ONNX model verification successful. Output shape: {ort_outputs[0].shape}")
            
        except Exception as e:
            print(f"ONNX model verification failed: {e}")
```

## Training Script

### Comprehensive Training Pipeline

Complete training script that integrates all components for end-to-end TSA model training.

```python
def train_tsa_lightning_model(
    config: Dict[str, Any],
    train_dataloader: DataLoader,
    val_dataloader: DataLoader,
    test_dataloader: Optional[DataLoader] = None,
    model_log_path: str = "./tsa_lightning_logs"
) -> pl.LightningModule:
    """
    Complete training pipeline for TSA Lightning models.
    
    Args:
        config: Model and training configuration
        train_dataloader: Training data loader
        val_dataloader: Validation data loader
        test_dataloader: Optional test data loader
        model_log_path: Path for logs and checkpoints
        
    Returns:
        Trained Lightning model
    """
    
    # Create model based on configuration
    if config.get("model_type", "single_sequence") == "dual_sequence":
        model = DualSequenceTSA(config)
    else:
        model = TemporalSelfAttentionClassification(config)
    
    # Setup trainer
    trainer = setup_trainer(config, model_log_path)
    
    # Train model
    print("Starting TSA Lightning training...")
    trainer.fit(
        model,
        train_dataloaders=train_dataloader,
        val_dataloaders=val_dataloader
    )
    
    # Test model if test data provided
    if test_dataloader is not None:
        print("Running final evaluation...")
        trainer.test(model, dataloaders=test_dataloader)
    
    # Load best checkpoint
    best_model_path = trainer.checkpoint_callback.best_model_path
    if best_model_path:
        print(f"Loading best model from: {best_model_path}")
        if config.get("model_type", "single_sequence") == "dual_sequence":
            model = DualSequenceTSA.load_from_checkpoint(best_model_path)
        else:
            model = TemporalSelfAttentionClassification.load_from_checkpoint(best_model_path)
    
    return model


if __name__ == "__main__":
    # Example usage
    config = {
        "model_type": "single_sequence",
        "n_cat_features": 53,
        "n_num_features": 47,
        "n_classes": 2,
        "n_embedding": 1352,
        "seq_len": 51,
        "dim_embedding_table": 128,
        "num_heads": 1,
        "n_layers_order": 6,
        "n_layers_feature": 6,
        "dropout": 0.1,
        "use_moe": True,
        "num_experts": 5,
        "lr": 1e-5,
        "max_epochs": 70,
        "patience": 10,
        "batch_size": 1024,
        "primary_metric": "auroc",
        "metric_choices": ["accuracy", "precision", "recall", "f1_score", "auroc", "pr_auc"]
    }
    
    # Train model (assuming dataloaders are available)
    # model = train_tsa_lightning_model(config, train_dl, val_dl, test_dl)
```

## Implementation Benefits

### Performance Advantages

#### Distributed Training
- **Automatic Scaling**: DDP/FSDP support for multi-GPU and multi-node training
- **Memory Optimization**: FSDP reduces memory footprint for large models
- **Communication Efficiency**: Optimized gradient synchronization and parameter sharding
- **Load Balancing**: Automatic workload distribution across available resources

#### Training Efficiency
- **Mixed Precision**: 16-bit training reduces memory usage and increases throughput
- **Gradient Accumulation**: Effective larger batch sizes without memory constraints
- **Automatic Optimization**: Lightning handles training loop optimization automatically
- **Checkpointing**: Efficient model state management and resumption capabilities

#### Model Performance
- **Preserved Architecture**: Identical TSA model behavior with Lightning benefits
- **Advanced Metrics**: Comprehensive fraud detection specific evaluation
- **Early Stopping**: Automatic training termination based on validation performance
- **Learning Rate Scheduling**: OneCycleLR integration for optimal convergence

### Development Advantages

#### Code Quality
- **Modular Design**: Clean separation of concerns with reusable components
- **Type Safety**: Comprehensive type hints and documentation
- **Error Handling**: Robust error handling and validation
- **Testing Support**: Individual component testing capabilities

#### Maintainability
- **Clear Architecture**: Well-defined module boundaries and dependencies
- **Configuration Management**: Centralized configuration with validation
- **Logging Integration**: Comprehensive logging and monitoring
- **Documentation**: Extensive inline documentation and examples

#### Extensibility
- **Component Reusability**: Attention modules can be used in other architectures
- **Easy Customization**: Simple configuration changes for different use cases
- **Plugin Architecture**: Easy integration of new loss functions and metrics
- **Version Compatibility**: Backward compatibility with existing TSA models

## Production Deployment

### Model Serving

#### ONNX Export
- **Optimized Inference**: ONNX Runtime optimization for production deployment
- **Cross-Platform**: Deploy on various platforms and frameworks
- **Dynamic Batching**: Support for variable batch sizes
- **Input Validation**: Automatic input/output specification

#### Deployment Pipeline
```python
# Example deployment workflow
def deploy_tsa_model(model_path: str, deployment_config: Dict):
    """
    Complete deployment pipeline for TSA Lightning models.
    
    Steps:
    1. Load trained Lightning model
    2. Export to ONNX format
    3. Optimize for inference
    4. Deploy to serving infrastructure
    """
    
    # Load model
    model = TemporalSelfAttentionClassification.load_from_checkpoint(model_path)
    
    # Export to ONNX
    onnx_path = model_path.replace('.ckpt', '.onnx')
    TSAModelExporter.export_to_onnx(model, onnx_path, example_input)
    
    # Deploy to serving infrastructure
    deploy_to_sagemaker_endpoint(onnx_path, deployment_config)
```

### Monitoring and Observability

#### Model Performance Tracking
- **Real-time Metrics**: Live monitoring of model performance in production
- **Drift Detection**: Automatic detection of data and concept drift
- **Performance Alerts**: Automated alerting for performance degradation
- **A/B Testing**: Support for model comparison and gradual rollout

#### Operational Metrics
- **Inference Latency**: Real-time latency monitoring and optimization
- **Throughput Tracking**: Request volume and processing capacity monitoring
- **Resource Utilization**: CPU, GPU, and memory usage tracking
- **Error Monitoring**: Comprehensive error tracking and analysis

## Migration Guide

### From Original TSA to Lightning

#### Step-by-Step Migration
1. **Configuration Mapping**: Convert existing TSA configs to Lightning format
2. **Data Pipeline**: Adapt data loaders for Lightning compatibility
3. **Model Instantiation**: Replace original models with Lightning modules
4. **Training Script**: Update training scripts to use Lightning trainer
5. **Evaluation**: Verify identical model behavior and performance

#### Compatibility Checklist
- [ ] Model architecture preservation verified
- [ ] Identical forward pass outputs confirmed
- [ ] Training convergence matches original implementation
- [ ] Evaluation metrics consistency validated
- [ ] Checkpoint compatibility ensured

#### Migration Benefits
- **Reduced Code Complexity**: Eliminate boilerplate training code
- **Improved Reliability**: Robust error handling and edge case management
- **Enhanced Performance**: Automatic optimization and distributed training
- **Better Monitoring**: Comprehensive logging and visualization
- **Production Readiness**: Built-in deployment and serving capabilities

## Testing and Validation

### Unit Testing

#### Component Testing
```python
def test_order_attention_module():
    """Test OrderAttentionModule functionality."""
    config = get_test_config()
    module = OrderAttentionModule(config)
    
    # Test forward pass
    x_cat = torch.randint(0, 100, (2, 10, 53))
    x_num = torch.randn(2, 10, 47)
    time_seq = torch.randn(2, 10, 1)
    
    output = module(x_cat, x_num, time_seq)
    
    assert output.shape == (2, 256)  # Expected output shape
    assert not torch.isnan(output).any()  # No NaN values

def test_feature_attention_module():
    """Test FeatureAttentionModule functionality."""
    config = get_test_config()
    module = FeatureAttentionModule(config)
    
    # Test forward pass
    x_cat = torch.randint(0, 100, (2, 10, 53))
    x_num = torch.randn(2, 10, 47)
    x_engineered = torch.randn(2, 100)
    
    output = module(x_cat, x_num, x_engineered)
    
    assert output.shape == (2, 128)  # Expected output shape
    assert not torch.isnan(output).any()  # No NaN values
```

#### Integration Testing
```python
def test_lightning_module_training():
    """Test complete Lightning module training."""
    config = get_test_config()
    model = TemporalSelfAttentionClassification(config)
    
    # Create test data
    train_dl = create_test_dataloader(batch_size=4, num_batches=10)
    val_dl = create_test_dataloader(batch_size=4, num_batches=5)
    
    # Setup trainer
    trainer = pl.Trainer(
        max_epochs=2,
        accelerator="cpu",
        enable_checkpointing=False,
        logger=False
    )
    
    # Train model
    trainer.fit(model, train_dl, val_dl)
    
    # Verify training completed successfully
    assert trainer.state.finished
```

### Performance Testing

#### Benchmark Comparison
```python
def benchmark_original_vs_lightning():
    """Compare performance between original TSA and Lightning implementation."""
    
    # Setup identical configurations
    config = get_benchmark_config()
    
    # Original TSA model
    original_model = create_original_tsa_model(config)
    
    # Lightning TSA model
    lightning_model = TemporalSelfAttentionClassification(config)
    
    # Compare forward pass outputs
    test_input = create_test_input(config)
    
    with torch.no_grad():
        original_output = original_model(**test_input)
        lightning_output = lightning_model(**test_input)
    
    # Verify outputs are identical (within tolerance)
    assert torch.allclose(original_output, lightning_output, atol=1e-6)
    
    # Benchmark inference speed
    original_time = benchmark_inference_speed(original_model, test_input)
    lightning_time = benchmark_inference_speed(lightning_model, test_input)
    
    print(f"Original TSA inference time: {original_time:.4f}s")
    print(f"Lightning TSA inference time: {lightning_time:.4f}s")
```

## Future Enhancements

### Planned Features

#### Advanced Architectures
- **Transformer Integration**: Full transformer encoder-decoder architectures
- **Graph Neural Networks**: Integration with graph-based fraud detection
- **Multi-Modal Fusion**: Enhanced multi-modal learning capabilities
- **Continual Learning**: Online learning and model adaptation

#### Performance Optimizations
- **Model Compression**: Quantization and pruning for deployment optimization
- **Dynamic Batching**: Adaptive batch size optimization
- **Caching Strategies**: Intelligent caching for repeated inference
- **Hardware Acceleration**: TPU and specialized hardware support

#### Monitoring and Observability
- **Advanced Metrics**: Custom fraud detection metrics and KPIs
- **Explainability**: Enhanced model interpretability and explanation
- **Fairness Monitoring**: Bias detection and fairness metrics
- **Automated Retraining**: Trigger-based model retraining pipelines

### Research Directions

#### Attention Mechanisms
- **Sparse Attention**: More efficient attention patterns for long sequences
- **Hierarchical Attention**: Multi-scale temporal attention mechanisms
- **Cross-Modal Attention**: Enhanced attention across different data modalities
- **Adaptive Attention**: Dynamic attention span based on input characteristics

#### Fraud Detection Specific
- **Adversarial Training**: Robustness against adversarial attacks
- **Few-Shot Learning**: Rapid adaptation to new fraud patterns
- **Federated Learning**: Privacy-preserving distributed training
- **Real-Time Adaptation**: Online learning for evolving fraud patterns

## Conclusion

The PyTorch Lightning implementation of Temporal Self-Attention models represents a significant advancement in both architectural sophistication and engineering excellence. By preserving the exact TSA model behavior while adding enterprise-grade training infrastructure, this implementation provides:

### Key Achievements
- **Architecture Preservation**: Identical model behavior with enhanced capabilities
- **Modular Design**: Reusable components for flexible model composition
- **Production Readiness**: Comprehensive deployment and monitoring support
- **Performance Optimization**: Distributed training and inference optimization
- **Developer Experience**: Simplified training with robust error handling

### Impact on Fraud Detection
- **Improved Accuracy**: Enhanced model performance through better training
- **Faster Deployment**: Streamlined path from research to production
- **Scalable Training**: Support for large-scale distributed training
- **Operational Excellence**: Comprehensive monitoring and observability
- **Future-Proof Architecture**: Extensible design for evolving requirements

### Technical Excellence
- **Code Quality**: High-quality, well-documented, and tested implementation
- **Performance**: Optimized for both training and inference efficiency
- **Reliability**: Robust error handling and edge case management
- **Maintainability**: Clear architecture with separation of concerns
- **Extensibility**: Plugin architecture for easy customization

This Lightning implementation establishes a new standard for TSA model development and deployment, providing a solid foundation for current fraud detection needs while enabling future innovations in temporal attention mechanisms and multi-modal learning.

## References

### Technical Documentation
- **[PyTorch Lightning Documentation](https://pytorch-lightning.readthedocs.io/)** - Official Lightning framework documentation
- **[FSDP Documentation](https://pytorch.org/docs/stable/fsdp.html)** - Fully Sharded Data Parallel training
- **[TensorBoard Integration](https://pytorch.org/docs/stable/tensorboard.html)** - Logging and visualization
- **[ONNX Export Guide](https://pytorch.org/docs/stable/onnx.html)** - Model export for deployment

### Research Papers
- **"Attention Is All You Need"** - Transformer architecture foundations
- **"Focal Loss for Dense Object Detection"** - Class imbalance handling
- **"Mixture of Experts"** - Sparse expert routing mechanisms
- **"Temporal Convolutional Networks"** - Temporal sequence modeling

### Implementation References
- **[Original TSA Implementation](../../dockers/tsa/scripts/)** - Reference TSA model implementation
- **[PyTorch BSM Lightning Models](../../dockers/pytorch_bsm/lightning_models/)** - Lightning implementation patterns
- **[TSA Analysis Document](../4_analysis/2025-10-20_tsa_cursus_step_equivalency_analysis.md)** - Integration analysis

### Related Design Documents
- **[Original TSA Model Design](./temporal_self_attention_model_design.md)** - Core TSA architecture
- **[Config-Driven Design](./config_driven_design.md)** - Configuration management patterns
- **[Model Registry Design](./registry_manager.md)** - Model versioning and management

---

*This design document serves as the comprehensive specification for the PyTorch Lightning implementation of Temporal Self-Attention models, providing both high-level architectural guidance and detailed implementation specifications for enterprise-grade fraud detection systems.*
