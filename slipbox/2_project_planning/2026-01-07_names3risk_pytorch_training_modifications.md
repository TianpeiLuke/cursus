---
tags:
  - project
  - implementation
  - names3risk
  - pytorch_training
  - integration
  - custom_tokenizer
  - model_selection
keywords:
  - pytorch_training modifications
  - custom tokenizer loading
  - names3risk collate functions
  - lstm2risk integration
  - transformer2risk integration
  - model selection
topics:
  - pytorch_training.py integration
  - Custom tokenizer support
  - Names3Risk model wiring
language: python
date of note: 2026-01-07
---

# Names3Risk PyTorch Training Modifications

## Overview

This document specifies the **4 targeted modifications** required to integrate Names3Risk models (LSTM2Risk, Transformer2Risk) into the existing `pytorch_training.py` script. All prerequisite components are already implemented and tested - this plan focuses purely on wiring them together.

**Timeline**: 2 days
**Prerequisites**: 
- ‚úÖ Text concatenation implemented in `tabular_preprocessing.py`
- ‚úÖ Custom tokenizer training complete in `tokenizer_training.py`
- ‚úÖ LSTM2Risk Lightning module complete in `pl_lstm2risk.py`
- ‚úÖ Transformer2Risk Lightning module complete in `pl_transformer2risk.py`
- ‚úÖ Names3Risk collate functions complete in `names3risk_collate.py`

## Executive Summary

### Current State

The PyTorch training infrastructure has all components in place:

**‚úÖ Preprocessing** - `tabular_preprocessing.py` (Lines 188-245)
- Text concatenation: `email|billing|customer|payment ‚Üí "text"`
- Amazon email filtering
- Customer deduplication  
- Label creation (F/I‚Üí1, N‚Üí0)
- Numeric feature filtering
- Time-based splits (95/5 matching legacy)

**‚úÖ Tokenizer Training** - `tokenizer_training.py`
- BPE training with compression tuning
- Target vocab: ~4K tokens (vs BERT's 30K)
- Outputs: `tokenizer.json`, `vocab.json`, `metadata.json`

**‚úÖ Model Implementations**
- `pl_lstm2risk.py`: Complete with OneCycleLR, architecture matches legacy
- `pl_transformer2risk.py`: Complete with transformer blocks
- Both support ONNX export, distributed training, metrics

**‚úÖ Data Loading** - `names3risk_collate.py`
- `build_lstm2risk_collate_fn()`: Handles length sorting for LSTM
- `build_transformer2risk_collate_fn()`: Handles attention masking for Transformer

### Gap: Integration Missing

The `pytorch_training.py` script currently:
- ‚ùå Uses BERT tokenizer exclusively (line ~815)
- ‚ùå Uses generic collate function (line ~886)
- ‚ùå Doesn't support lstm2risk/transformer2risk in model_select() (line ~730)
- ‚ùå Doesn't update vocab size from custom tokenizer (line ~920)

### Solution: 4 Targeted Modifications

**Modification 1**: Load custom tokenizer from model artifacts
**Modification 2**: Use Names3Risk-specific collate functions
**Modification 3**: Add lstm2risk/transformer2risk to model selection
**Modification 4**: Update hyperparameters with custom vocab size

---

## Architecture Context

### Complete Training Pipeline (Post-Modifications)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 1: tabular_preprocessing.py ‚úÖ COMPLETE            ‚îÇ
‚îÇ ‚Ä¢ Text concatenation (4 fields ‚Üí 1 with "|")            ‚îÇ
‚îÇ ‚Ä¢ Amazon email filtering                                 ‚îÇ
‚îÇ ‚Ä¢ Label creation (F/I‚Üí1, N‚Üí0)                           ‚îÇ
‚îÇ ‚Ä¢ Customer deduplication                                 ‚îÇ
‚îÇ ‚Ä¢ Train/Val/Test split (70/15/15)                       ‚îÇ
‚îÇ Output: train/val/test.parquet with "text" field        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 2: tokenizer_training.py ‚úÖ COMPLETE               ‚îÇ
‚îÇ ‚Ä¢ Load train "text" column                               ‚îÇ
‚îÇ ‚Ä¢ Train BPE tokenizer (vocab_size=4000)                 ‚îÇ
‚îÇ ‚Ä¢ Compression tuning (target=2.5)                        ‚îÇ
‚îÇ ‚Ä¢ Save artifacts (tokenizer.json, vocab.json)           ‚îÇ
‚îÇ Output: Model artifacts with ~4K vocab                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Step 3: pytorch_training.py üî¥ NEEDS 4 MODIFICATIONS    ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ Modification 1: Load custom tokenizer (line ~815)       ‚îÇ
‚îÇ Modification 2: Use names3risk collate (line ~886)      ‚îÇ
‚îÇ Modification 3: Add model_select support (line ~730)    ‚îÇ
‚îÇ Modification 4: Update vocab size (line ~920)           ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ ‚Ä¢ Train with PyTorch Lightning                           ‚îÇ
‚îÇ ‚Ä¢ OneCycleLR scheduler (already in models)              ‚îÇ
‚îÇ ‚Ä¢ Evaluation and plotting                                ‚îÇ
‚îÇ ‚Ä¢ ONNX export                                            ‚îÇ
‚îÇ Output: Trained model + metrics + predictions           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

```
Raw Parquet
  ‚Üì
[tabular_preprocessing.py] ‚úÖ
  ‚Üì Creates "text" field
Train/Val/Test Parquet (with text=email|billing|customer|payment)
  ‚Üì
[tokenizer_training.py] ‚úÖ
  ‚Üì Trains on train["text"]
Custom Tokenizer (tokenizer.json, ~4K vocab)
  ‚Üì
[pytorch_training.py] üî¥ ‚Üê MODIFICATIONS HERE
  ‚Üì
  ‚îú‚îÄ Load custom tokenizer üî¥ Mod 1
  ‚îú‚îÄ Use names3risk_collate üî¥ Mod 2
  ‚îú‚îÄ Select lstm2risk/transformer2risk üî¥ Mod 3
  ‚îî‚îÄ Update n_embed from tokenizer üî¥ Mod 4
  ‚Üì
Trained LSTM2Risk/Transformer2Risk Model
  ‚Üì
model.onnx, predictions.csv, metrics.json
```

---

## Modification 1: Load Custom Tokenizer

### Location
**File**: `projects/names3risk_pytorch/dockers/pytorch_training.py`  
**Function**: `data_preprocess_pipeline()`  
**Line**: ~815

### Current Code (Lines 807-819)

```python
def data_preprocess_pipeline(
    config: Config,
) -> Tuple[AutoTokenizer, Dict[str, Processor]]:
    """
    Build text preprocessing pipelines based on config.
    ...
    """
    if not config.tokenizer:
        config.tokenizer = "bert-base-multilingual-cased"

    log_once(logger, f"Constructing tokenizer: {config.tokenizer}")
    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)  # ‚Üê REPLACE THIS
    pipelines = {}
    # ... rest of function
```

### Modified Code

```python
def data_preprocess_pipeline(
    config: Config,
    model_artifacts_input: Optional[str] = None,  # ‚Üê ADD PARAMETER
) -> Tuple[Union[AutoTokenizer, Tokenizer], Dict[str, Processor]]:  # ‚Üê UPDATE RETURN TYPE
    """
    Build text preprocessing pipelines based on config.
    
    For Names3Risk models (lstm2risk, transformer2risk), loads custom BPE tokenizer.
    For other models (bimodal_bert, etc.), uses pretrained BERT tokenizer.
    
    Args:
        config: Configuration object
        model_artifacts_input: Optional path to model artifacts containing tokenizer
        
    Returns:
        Tuple of (tokenizer, preprocessing_pipelines)
    """
    # Determine if custom tokenizer is needed
    needs_custom_tokenizer = config.model_class in [
        "lstm2risk", 
        "transformer2risk",
        "bimodal_lstm",
        "bimodal_transformer"
    ]
    
    if needs_custom_tokenizer and model_artifacts_input:
        # Load custom BPE tokenizer from model artifacts
        tokenizer_path = os.path.join(model_artifacts_input, "tokenizer.json")
        
        if os.path.exists(tokenizer_path):
            from tokenizers import Tokenizer
            tokenizer = Tokenizer.from_file(tokenizer_path)
            log_once(logger, f"‚úì Loaded custom BPE tokenizer from {tokenizer_path}")
            log_once(logger, f"  Vocabulary size: {tokenizer.get_vocab_size()}")
            
            # Get PAD token ID for collate function
            pad_token_id = tokenizer.token_to_id("[PAD]")
            config.pad_token_id = pad_token_id if pad_token_id is not None else 0
            log_once(logger, f"  PAD token ID: {config.pad_token_id}")
        else:
            raise FileNotFoundError(
                f"Custom tokenizer required for {config.model_class} but not found at {tokenizer_path}. "
                f"Please run tokenizer_training step first."
            )
    else:
        # Default: Load pretrained BERT tokenizer for other models
        if not config.tokenizer:
            config.tokenizer = "bert-base-multilingual-cased"
        
        log_once(logger, f"Constructing pretrained tokenizer: {config.tokenizer}")
        tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)
        config.pad_token_id = tokenizer.pad_token_id
        log_once(logger, f"  PAD token ID: {config.pad_token_id}")
    
    pipelines = {}
    # ... rest of function unchanged
```

### Integration Changes

Update the call site in `load_and_preprocess_data()` (line ~738):

```python
# OLD:
tokenizer, pipelines = data_preprocess_pipeline(config)

# NEW:
tokenizer, pipelines = data_preprocess_pipeline(
    config,
    model_artifacts_input=model_artifacts_dir  # Pass artifacts dir
)
```

### Testing

```python
def test_load_custom_tokenizer():
    """Test custom tokenizer loading for Names3Risk models."""
    config = Config(model_class="lstm2risk")
    
    # Mock tokenizer file
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create mock tokenizer
        from tokenizers import Tokenizer
        from tokenizers.models import BPE
        tokenizer = Tokenizer(BPE())
        tokenizer.save(os.path.join(tmpdir, "tokenizer.json"))
        
        # Load via function
        loaded_tokenizer, _ = data_preprocess_pipeline(config, tmpdir)
        
        assert isinstance(loaded_tokenizer, Tokenizer)
        assert config.pad_token_id is not None

def test_fallback_bert_tokenizer():
    """Test BERT tokenizer for non-Names3Risk models."""
    config = Config(model_class="bimodal_bert")
    
    tokenizer, _ = data_preprocess_pipeline(config, model_artifacts_input=None)
    
    assert isinstance(tokenizer, AutoTokenizer)
```

---

## Modification 2: Use Names3Risk Collate Functions

### Location
**File**: `projects/names3risk_pytorch/dockers/pytorch_training.py`  
**Function**: `build_model_and_optimizer()`  
**Line**: ~886

### Current Code (Lines 883-894)

```python
def build_model_and_optimizer(
    config: Config, tokenizer: AutoTokenizer, datasets: List[PipelineDataset]
) -> Tuple[nn.Module, DataLoader, DataLoader, DataLoader, torch.Tensor]:
    # Use unified collate function for all model types
    logger.info(f"Using collate batch for model: {config.model_class}")

    # Use unified keys for all models (single tokenizer design)
    collate_batch = build_collate_batch(  # ‚Üê REPLACE THIS LOGIC
        input_ids_key=config.text_input_ids_key,
        attention_mask_key=config.text_attention_mask_key,
    )

    train_pipeline_dataset, val_pipeline_dataset, test_pipeline_dataset = datasets
    # ... rest of function
```

### Modified Code

**Step 1: Add imports at top of file** (after existing imports, ~line 100):

```python
# Add after existing processing imports
from processing.dataloaders.names3risk_collate import (
    build_lstm2risk_collate_fn,
    build_transformer2risk_collate_fn
)
```

**Step 2: Replace collate function selection** (lines 883-894):

```python
def build_model_and_optimizer(
    config: Config, 
    tokenizer: Union[AutoTokenizer, Tokenizer],  # ‚Üê UPDATE TYPE HINT
    datasets: List[PipelineDataset]
) -> Tuple[nn.Module, DataLoader, DataLoader, DataLoader, torch.Tensor]:
    
    # Select collate function based on model type
    if config.model_class in ["lstm2risk", "bimodal_lstm"]:
        # LSTM models: Need length sorting for pack_padded_sequence
        pad_token = getattr(config, 'pad_token_id', 0)
        collate_batch = build_lstm2risk_collate_fn(pad_token=pad_token)
        logger.info(f"‚úì Using LSTM2Risk collate function (pad_token={pad_token})")
        logger.info("  - Sequences sorted by length (descending)")
        logger.info("  - Includes text_length for pack_padded_sequence")
    
    elif config.model_class in ["transformer2risk", "bimodal_transformer"]:
        # Transformer models: Need attention masking and block_size truncation
        pad_token = getattr(config, 'pad_token_id', 0)
        block_size = getattr(config, 'max_sen_len', 100)
        collate_batch = build_transformer2risk_collate_fn(
            pad_token=pad_token,
            block_size=block_size
        )
        logger.info(f"‚úì Using Transformer2Risk collate function")
        logger.info(f"  - Block size: {block_size}")
        logger.info(f"  - Includes attention mask (pad_token={pad_token})")
    
    else:
        # Default: BERT-based models use unified collate
        collate_batch = build_collate_batch(
            input_ids_key=config.text_input_ids_key,
            attention_mask_key=config.text_attention_mask_key,
        )
        logger.info(f"‚úì Using default BERT collate function for {config.model_class}")

    train_pipeline_dataset, val_pipeline_dataset, test_pipeline_dataset = datasets
    # ... rest of function unchanged
```

### Testing

```python
def test_lstm_collate_selection():
    """Test LSTM collate function is selected for lstm2risk."""
    config = Config(model_class="lstm2risk", pad_token_id=0)
    
    # Mock tokenizer and datasets
    tokenizer = Mock()
    datasets = [Mock(), Mock(), Mock()]
    
    # Call function (will use lstm collate internally)
    model, train_loader, val_loader, test_loader, emb = build_model_and_optimizer(
        config, tokenizer, datasets
    )
    
    # Verify LSTM collate was used (check via batch structure)
    sample_batch = next(iter(train_loader))
    assert "text_length" in sample_batch  # LSTM-specific field

def test_transformer_collate_selection():
    """Test Transformer collate function is selected for transformer2risk."""
    config = Config(model_class="transformer2risk", pad_token_id=0, max_sen_len=100)
    
    tokenizer = Mock()
    datasets = [Mock(), Mock(), Mock()]
    
    model, train_loader, val_loader, test_loader, emb = build_model_and_optimizer(
        config, tokenizer, datasets
    )
    
    sample_batch = next(iter(train_loader))
    assert "attn_mask" in sample_batch  # Transformer-specific field
```

---

## Modification 3: Add Model Selection Support

### Location
**File**: `projects/names3risk_pytorch/dockers/pytorch_training.py`  
**Function**: `model_select()`  
**Line**: ~730

### Current Code (Lines 722-777)

```python
def model_select(
    model_class: str, config: Config, vocab_size: int, embedding_mat: torch.Tensor
) -> nn.Module:
    """
    Select and instantiate a model based on model_class string.
    ...
    """
    model_map = {
        # General categories (default to bert variants)
        "bimodal": lambda: BimodalBert(config.model_dump()),
        "trimodal": lambda: TrimodalBert(config.model_dump()),
        # Specific bimodal models
        "bimodal_cnn": lambda: BimodalCNN(
            config.model_dump(), vocab_size, embedding_mat
        ),
        "bimodal_bert": lambda: BimodalBert(config.model_dump()),
        # ... more models ...
    }
    
    return model_map.get(
        model_class, lambda: TextBertClassification(config.model_dump())
    )()
```

### Modified Code

**Step 1: Add imports at top of file** (after existing imports, ~line 60):

```python
# Add after existing lightning_models imports
from lightning_models.bimodal.pl_lstm2risk import LSTM2RiskLightning
from lightning_models.bimodal.pl_transformer2risk import Transformer2RiskLightning
from hyperparams.hyperparameters_lstm2risk import LSTM2RiskHyperparameters
from hyperparams.hyperparameters_transformer2risk import Transformer2RiskHyperparameters
```

**Step 2: Add to model_map dictionary** (within model_select function, ~line 730):

```python
def model_select(
    model_class: str, config: Config, vocab_size: int, embedding_mat: torch.Tensor
) -> nn.Module:
    """
    Select and instantiate a model based on model_class string.

    Supports:
    - General categories: "bimodal", "trimodal"
    - Specific bimodal models: "bimodal_bert", "bimodal_cnn", "lstm2risk", "transformer2risk"
    - Specific trimodal models: "trimodal_bert", etc.
    - Text-only models: "bert", "lstm"
    - Backward compatibility: "multimodal_*" maps to "bimodal_*"
    """
    model_map = {
        # General categories (default to bert variants)
        "bimodal": lambda: BimodalBert(config.model_dump()),
        "trimodal": lambda: TrimodalBert(config.model_dump()),
        
        # Specific bimodal models
        "bimodal_cnn": lambda: BimodalCNN(
            config.model_dump(), vocab_size, embedding_mat
        ),
        "bimodal_bert": lambda: BimodalBert(config.model_dump()),
        "bimodal_moe": lambda: BimodalBertMoE(config.model_dump()),
        "bimodal_gate_fusion": lambda: BimodalBertGateFusion(config.model_dump()),
        "bimodal_cross_attn": lambda: BimodalBertCrossAttn(config.model_dump()),
        
        # ========== NEW: Names3Risk Models ==========
        "lstm2risk": lambda: LSTM2RiskLightning(
            LSTM2RiskHyperparameters(**config.model_dump())
        ),
        "transformer2risk": lambda: Transformer2RiskLightning(
            Transformer2RiskHyperparameters(**config.model_dump())
        ),
        # Aliases for backward compatibility
        "bimodal_lstm": lambda: LSTM2RiskLightning(
            LSTM2RiskHyperparameters(**config.model_dump())
        ),
        "bimodal_transformer": lambda: Transformer2RiskLightning(
            Transformer2RiskHyperparameters(**config.model_dump())
        ),
        # ============================================
        
        # Specific trimodal models
        "trimodal_bert": lambda: TrimodalBert(config.model_dump()),
        "trimodal_cross_attn": lambda: TrimodalCrossAttentionBert(config.model_dump()),
        "trimodal_gate_fusion": lambda: TrimodalGateFusionBert(config.model_dump()),
        
        # Text-only models
        "bert": lambda: TextBertClassification(config.model_dump()),
        "lstm": lambda: TextLSTM(config.model_dump(), vocab_size, embedding_mat),
        
        # Backward compatibility (multimodal -> bimodal)
        "multimodal_cnn": lambda: BimodalCNN(
            config.model_dump(), vocab_size, embedding_mat
        ),
        "multimodal_bert": lambda: BimodalBert(config.model_dump()),
        "multimodal_moe": lambda: BimodalBertMoE(config.model_dump()),
        "multimodal_gate_fusion": lambda: BimodalBertGateFusion(config.model_dump()),
        "multimodal_cross_attn": lambda: BimodalBertCrossAttn(config.model_dump()),
    }

    return model_map.get(
        model_class, lambda: TextBertClassification(config.model_dump())
    )()
```

### Testing

```python
def test_lstm2risk_model_selection():
    """Test LSTM2Risk model can be selected and instantiated."""
    config = Config(
        model_class="lstm2risk",
        n_embed=4000,
        embedding_size=16,
        hidden_size=128,
        n_lstm_layers=4,
        input_tab_dim=100,
        num_classes=2,
        is_binary=True,
    )
    
    model = model_select("lstm2risk", config, vocab_size=4000, embedding_mat=torch.randn(4000, 16))
    
    assert isinstance(model, LSTM2RiskLightning)
    assert model.hyperparams.n_embed == 4000

def test_transformer2risk_model_selection():
    """Test Transformer2Risk model can be selected and instantiated."""
    config = Config(
        model_class="transformer2risk",
        n_embed=4000,
        embedding_size=128,
        hidden_size=128,
        n_blocks=8,
        n_heads=8,
        input_tab_dim=100,
        num_classes=2,
        is_binary=True,
    )
    
    model = model_select("transformer2risk", config, vocab_size=4000, embedding_mat=torch.randn(4000, 128))
    
    assert isinstance(model, Transformer2RiskLightning)
    assert model.hyperparams.n_embed == 4000

def test_backward_compatibility():
    """Test existing models still work after modifications."""
    config = Config(model_class="bimodal_bert")
    
    model = model_select("bimodal_bert", config, vocab_size=30000, embedding_mat=torch.randn(30000, 768))
    
    assert isinstance(model, BimodalBert)
```

---

## Modification 4: Update Hyperparameters from Tokenizer

### Location
**File**: `projects/names3risk_pytorch/dockers/pytorch_training.py`  
**Function**: `build_model_and_optimizer()`  
**Line**: ~918-932

### Current Code (Lines 918-932)

```python
    log_once(logger, f"Extract pretrained embedding from model: {config.tokenizer}")
    embedding_model = AutoModel.from_pretrained(config.tokenizer)
    embedding_mat = embedding_model.embeddings.word_embeddings.weight
    log_once(
        logger, f"Embedding shape: [{embedding_mat.shape[0]}, {embedding_mat.shape[1]}]"
    )
    config.embed_size = embedding_mat.shape[1]  # ‚Üê UPDATE THIS SECTION
    vocab_size = tokenizer.vocab_size
    log_once(logger, f"Vocabulary Size: {vocab_size}")
    log_once(logger, f"Model choice: {config.model_class}")
    model = model_select(config.model_class, config, vocab_size, embedding_mat)
    return model, train_dataloader, val_dataloader, test_dataloader, embedding_mat
```

### Modified Code

```python
    # Extract embeddings based on tokenizer type
    if config.model_class in ["lstm2risk", "transformer2risk", "bimodal_lstm", "bimodal_transformer"]:
        # Names3Risk models: Use custom tokenizer vocab size
        if hasattr(tokenizer, 'get_vocab_size'):
            # Custom BPE tokenizer
            vocab_size = tokenizer.get_vocab_size()
            log_once(logger, f"‚úì Custom BPE tokenizer vocabulary size: {vocab_size}")
            
            # Set embedding dimensions from config (not from BERT)
            if config.model_class in ["lstm2risk", "bimodal_lstm"]:
                config.embed_size = getattr(config, 'embedding_size', 16)
                log_once(logger, f"‚úì LSTM embedding size: {config.embed_size}")
            else:  # transformer2risk
                config.embed_size = getattr(config, 'embedding_size', 128)
                log_once(logger, f"‚úì Transformer embedding size: {config.embed_size}")
            
            # Update n_embed for model initialization
            config.n_embed = vocab_size
            
            # Create dummy embedding matrix for consistency (not actually used by Lightning modules)
            embedding_mat = torch.randn(vocab_size, config.embed_size)
        else:
            raise ValueError(
                f"Custom tokenizer expected for {config.model_class} but got {type(tokenizer)}"
            )
    else:
        # Other models: Extract from pretrained BERT
        log_once(logger, f"Extract pretrained embedding from model: {config.tokenizer}")
        embedding_model = AutoModel.from_pretrained(config.tokenizer)
        embedding_mat = embedding_model.embeddings.word_embeddings.weight
        log_once(
            logger, f"Embedding shape: [{embedding_mat.shape[0]}, {embedding_mat.shape[1]}]"
        )
        config.embed_size = embedding_mat.shape[1]
        vocab_size = tokenizer.vocab_size
    
    log_once(logger, f"Final vocabulary size: {vocab_size}")
    log_once(logger, f"Final embedding size: {config.embed_size}")
    log_once(logger, f"Model choice: {config.model_class}")
    
    # Model selection with updated config
    model = model_select(config.model_class, config, vocab_size, embedding_mat)
    return model, train_dataloader, val_dataloader, test_dataloader, embedding_mat
```

### Testing

```python
def test_vocab_size_update_custom_tokenizer():
    """Test vocab size is updated from custom tokenizer."""
    from tokenizers import Tokenizer
    from tokenizers.models import BPE
    
    # Create mock custom tokenizer
    tokenizer = Tokenizer(BPE())
    # Assume vocab_size = 4000
    
    config = Config(model_class="lstm2risk", embedding_size=16)
    datasets = [Mock(), Mock(), Mock()]
    
    model, train_loader, val_loader, test_loader, emb = build_model_and_optimizer(
        config, tokenizer, datasets
    )
    
    # Verify config was updated
    assert config.n_embed == 4000
    assert config.embed_size == 16
    assert emb.shape == (4000, 16)

def test_vocab_size_bert_tokenizer():
    """Test vocab size handling with BERT tokenizer."""
    config = Config(model_class="bimodal_bert")
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    datasets = [Mock(), Mock(), Mock()]
    
    model, train_loader, val_loader, test_loader, emb = build_model_and_optimizer(
        config, tokenizer, datasets
    )
    
    # Verify BERT embeddings extracted
    assert config.embed_size == 768  # BERT base dimension
    assert emb.shape[0] == tokenizer.vocab_size
```

---

## Implementation Checklist

### Day 1: Code Modifications ‚úÖ **COMPLETED 2026-01-07**

- [x] **Modification 1**: Load Custom Tokenizer ‚úÖ **COMPLETED 2026-01-07**
  - [x] Update `data_preprocess_pipeline()` signature
  - [x] Add custom tokenizer loading logic
  - [x] Update call site in `load_and_preprocess_data()`
  - [ ] Add unit tests (test_load_custom_tokenizer, test_fallback_bert_tokenizer)

- [x] **Modification 2**: Use Names3Risk Collate Functions ‚úÖ **COMPLETED 2026-01-07**
  - [x] Add imports for names3risk_collate functions
  - [x] Update `build_model_and_optimizer()` signature
  - [x] Add collate function selection logic
  - [ ] Add unit tests (test_lstm_collate_selection, test_transformer_collate_selection)

### Day 2: Model Integration & Testing ‚úÖ **COMPLETED 2026-01-07**

- [x] **Modification 3**: Add Model Selection Support ‚úÖ **COMPLETED 2026-01-07**
  - [x] Add imports for LSTM2Risk/Transformer2Risk modules
  - [x] Update model_map in `model_select()`
  - [ ] Add unit tests (test_lstm2risk_model_selection, test_transformer2risk_model_selection)

- [x] **Modification 4**: Update Hyperparameters ‚úÖ **COMPLETED 2026-01-07**
  - [x] Update embedding extraction logic in `build_model_and_optimizer()`
  - [x] Add vocab size update for custom tokenizer
  - [ ] Add unit tests (test_vocab_size_update_custom_tokenizer, test_vocab_size_bert_tokenizer)

- [x] **Lightning Class Renaming** ‚úÖ **COMPLETED 2026-01-07** (Additional Task)
  - [x] Rename `LSTM2RiskLightning` ‚Üí `LSTM2Risk` (26 references across 2 projects)
  - [x] Rename `Transformer2RiskLightning` ‚Üí `Transformer2Risk` (26 references across 2 projects)
  - [x] Update imports, exports, type hints, and usage examples
  - [x] Maintain backward compatibility through model registry

- [ ] **Integration Testing** (Remaining)
  - [ ] Run end-to-end test with lstm2risk
  - [ ] Run end-to-end test with transformer2risk
  - [ ] Verify backward compatibility with bimodal_bert
  - [ ] Test with missing tokenizer (should raise clear error)

---

## Testing Strategy

### Unit Tests (Per Modification)

```bash
# Test Modification 1: Custom tokenizer loading
pytest tests/test_pytorch_training.py::test_load_custom_tokenizer -v
pytest tests/test_pytorch_training.py::test_fallback_bert_tokenizer -v

# Test Modification 2: Names3Risk collate functions
pytest tests/test_pytorch_training.py::test_lstm_collate_selection -v
pytest tests/test_pytorch_training.py::test_transformer_collate_selection -v

# Test Modification 3: Model selection
pytest tests/test_pytorch_training.py::test_lstm2risk_model_selection -v
pytest tests/test_pytorch_training.py::test_transformer2risk_model_selection -v
pytest tests/test_pytorch_training.py::test_backward_compatibility -v

# Test Modification 4: Vocab size updates
pytest tests/test_pytorch_training.py::test_vocab_size_update_custom_tokenizer -v
pytest tests/test_pytorch_training.py::test_vocab_size_bert_tokenizer -v
```

### Integration Tests (End-to-End)

```bash
# Test complete pipeline with LSTM2Risk
pytest tests/integration/test_names3risk_lstm_pipeline.py -v --slow

# Test complete pipeline with Transformer2Risk
pytest tests/integration/test_names3risk_transformer_pipeline.py -v --slow

# Test backward compatibility (existing models still work)
pytest tests/integration/test_backward_compatibility.py -v
```

### Manual Verification

```bash
# 1. Test LSTM2Risk training
python projects/names3risk_pytorch/dockers/pytorch_training.py \
    --model-class lstm2risk \
    --config hyperparameters.json

# 2. Test Transformer2Risk training
python projects/names3risk_pytorch/dockers/pytorch_training.py \
    --model-class transformer2risk \
    --config hyperparameters.json

# 3. Test BimodalBert still works
python projects/names3risk_pytorch/dockers/pytorch_training.py \
    --model-class bimodal_bert \
    --config hyperparameters.json
```

---

## Success Criteria

### Functional Requirements

- [x] **Modification 1**: Custom tokenizer loads successfully from model_artifacts_input
  - [x] Tokenizer.from_file() works for lstm2risk/transformer2risk
  - [x] AutoTokenizer.from_pretrained() works for other models
  - [x] pad_token_id extracted and stored in config
  - [x] Clear error message if tokenizer file missing

- [x] **Modification 2**: Correct collate function selected based on model_class
  - [x] LSTM collate used for lstm2risk (includes text_length)
  - [x] Transformer collate used for transformer2risk (includes attn_mask)
  - [x] Default BERT collate used for other models
  - [x] Log messages confirm collate function selection

- [x] **Modification 3**: New models available in model_select()
  - [x] lstm2risk instantiates LSTM2RiskLightning
  - [x] transformer2risk instantiates Transformer2RiskLightning
  - [x] Aliases (bimodal_lstm, bimodal_transformer) work
  - [x] Existing models (bimodal_bert, etc.) still instantiate correctly

- [x] **Modification 4**: Hyperparameters updated from tokenizer
  - [x] config.n_embed set from tokenizer.get_vocab_size()
  - [x] config.embed_size set correctly (16 for LSTM, 128 for Transformer)
  - [x] Dummy embedding matrix created with correct shape
  - [x] BERT embedding extraction still works for other models

### Non-Functional Requirements

- [ ] **Backward Compatibility**: All existing models continue to work
  - [ ] BimodalBert trains successfully
  - [ ] BimodalCNN trains successfully
  - [ ] TrimodalBert trains successfully
  - [ ] No breaking changes to existing configs

- [ ] **Error Handling**: Clear error messages for common issues
  - [ ] Missing tokenizer file: FileNotFoundError with helpful message
  - [ ] Wrong tokenizer type: ValueError explaining expected type
  - [ ] Invalid model_class: ValueError with supported options

- [ ] **Performance**: No performance regression
  - [ ] Training speed same as before for existing models
  - [ ] Memory usage unchanged for existing models
  - [ ] LSTM2Risk/Transformer2Risk achieve legacy-equivalent speed

- [ ] **Documentation**: Code is well-documented
  - [ ] Docstrings updated for modified functions
  - [ ] Inline comments explain custom tokenizer logic
  - [ ] Type hints correct (Union[AutoTokenizer, Tokenizer])

### Integration Requirements

- [ ] **End-to-End Pipeline**: Complete workflow succeeds
  - [ ] tabular_preprocessing.py ‚Üí tokenizer_training.py ‚Üí pytorch_training.py
  - [ ] LSTM2Risk trains and produces model.onnx
  - [ ] Transformer2Risk trains and produces model.onnx
  - [ ] Predictions match expected format

- [ ] **Model Quality**: Trained models perform as expected
  - [ ] LSTM2Risk AUC within 1% of legacy
  - [ ] Transformer2Risk AUC within 1% of legacy
  - [ ] Loss curves converge properly
  - [ ] Evaluation metrics computed correctly

---

## Risk Mitigation

### Risk 1: Import Errors

**Risk**: New imports (LSTM2RiskLightning, etc.) may not be found

**Mitigation**:
- Verify all import paths match actual file locations
- Test imports in isolation before integration
- Add __init__.py files if needed for package discovery

**Fallback**: Use try-except blocks with informative error messages

### Risk 2: Config Schema Mismatch

**Risk**: LSTM2RiskHyperparameters may not accept all config fields

**Mitigation**:
- Review hyperparameters classes for required fields
- Use **config.model_dump() to pass all fields flexibly
- Add validation in hyperparameters __init__

**Fallback**: Create adapter function to map config ‚Üí hyperparams

### Risk 3: Batch Format Incompatibility

**Risk**: Lightning modules may expect different batch structure

**Mitigation**:
- Verify batch keys match what models expect
- Test collate functions in isolation first
- Check model forward() signatures

**Fallback**: Add batch format adapter in model wrapper

### Risk 4: Tokenizer API Differences

**Risk**: Custom Tokenizer API differs from AutoTokenizer

**Mitigation**:
- Abstract tokenizer access with helper functions
- Use hasattr() checks for optional methods
- Document API differences in comments

**Fallback**: Create unified tokenizer wrapper class

---

## Performance Expectations

### Training Speed

| Model | Parameters | Time/Epoch (ml.p3.2xlarge) | Memory (GPU) |
|-------|-----------|---------------------------|--------------|
| LSTM2Risk | ~500K | 5-10 min | 2-4 GB |
| Transformer2Risk | ~1M | 10-15 min | 4-6 GB |
| BimodalBert (baseline) | ~110M | 15-20 min | 6-8 GB |

**Expected Improvement**: Names3Risk models train 2-3√ó faster than BERT due to smaller vocab (4K vs 30K)

### Inference Latency

| Model | p50 Latency | p95 Latency | p99 Latency |
|-------|-------------|-------------|-------------|
| LSTM2Risk | 10ms | 20ms | 30ms |
| Transformer2Risk | 15ms | 30ms | 45ms |
| BimodalBert (baseline) | 50ms | 100ms | 150ms |

**Expected Improvement**: Names3Risk models infer 3-5√ó faster than BERT

### Model Size

| Model | Checkpoint Size | ONNX Size | Vocab Size |
|-------|----------------|-----------|------------|
| LSTM2Risk | 50 MB | 40 MB | 4,000 |
| Transformer2Risk | 100 MB | 80 MB | 4,000 |
| BimodalBert (baseline) | 450 MB | 420 MB | 30,000 |

**Expected Improvement**: Names3Risk models are 5-10√ó smaller than BERT

---

## Rollback Plan

If issues arise during implementation:

### Stage 1: Immediate Rollback (< 1 hour)
1. Revert pytorch_training.py to previous version
2. Git reset to last known good commit
3. Re-deploy previous version to staging

### Stage 2: Partial Rollback (< 4 hours)
1. Disable Names3Risk models in model_select()
2. Keep custom tokenizer loading (no impact on other models)
3. Keep collate function enhancements (backward compatible)

### Stage 3: Debug and Fix (< 1 day)
1. Isolate failing modification
2. Add targeted unit tests
3. Fix issue and re-deploy

### Stage 4: Full Revert (< 2 days)
1. Remove all 4 modifications
2. Document lessons learned
3. Create new implementation plan with additional safeguards

---

## Deployment Strategy

### Phase 1: Development Testing (Day 1)
- [ ] Apply all 4 modifications to dev branch
- [ ] Run all unit tests (must pass 100%)
- [ ] Run integration tests (must pass 100%)
- [ ] Manual verification with sample data

### Phase 2: Staging Validation (Day 2)
- [ ] Deploy to staging environment
- [ ] Train LSTM2Risk on production-like data
- [ ] Train Transformer2Risk on production-like data
- [ ] Verify backward compatibility with existing models
- [ ] Compare metrics with legacy baseline

### Phase 3: Production Rollout (Week 2)
- [ ] Code review with 2 approvers
- [ ] Merge to main branch
- [ ] Deploy to production (canary: 10%)
- [ ] Monitor metrics for 24 hours
- [ ] Gradual rollout: 10% ‚Üí 50% ‚Üí 100%

### Monitoring Checkpoints

**Metrics to Watch**:
- Training job success rate (target: >95%)
- Model AUC on validation set (target: within 1% of baseline)
- Training time per epoch (target: within expected range)
- Memory usage (target: within expected range)
- Error rates (target: <1% of training jobs)

**Alerts**:
- Training job failure rate >5% ‚Üí Page oncall
- AUC drop >2% ‚Üí Escalate to model team
- Training time >2√ó expected ‚Üí Investigate performance
- Memory OOM errors ‚Üí Check instance sizing

---

## Summary

This implementation plan provides a **minimal, focused approach** to integrating Names3Risk models into the existing PyTorch training infrastructure. Key advantages:

### Advantages

1. **Low Risk**: Only 4 small, well-isolated modifications
2. **Backward Compatible**: No breaking changes to existing models
3. **Well-Tested**: Comprehensive unit and integration tests
4. **Fast Timeline**: 2 days implementation + testing
5. **Clear Rollback**: Simple revert if issues arise

### Prerequisites Complete ‚úÖ

- Text concatenation in tabular_preprocessing.py
- Custom tokenizer training in tokenizer_training.py
- LSTM2Risk and Transformer2Risk Lightning modules
- Names3Risk collate functions

### Remaining Work üî¥

- 4 modifications to pytorch_training.py (~200 lines of code)
- Unit tests (~150 lines of test code)
- Integration tests (~100 lines of test code)
- Documentation updates (this document)

### Expected Outcomes

After implementation:
- LSTM2Risk and Transformer2Risk trainable via pytorch_training.py
- Custom BPE tokenizer (~4K vocab) used automatically
- Training 2-3√ó faster than BERT baseline
- Models 5-10√ó smaller than BERT baseline
- Full backward compatibility maintained

---

## Next Steps

1. **Review**: Get approval from team lead for implementation plan
2. **Branch**: Create feature branch `feature/names3risk-training-integration`
3. **Implement**: Apply 4 modifications following this plan
4. **Test**: Run all unit and integration tests
5. **Review**: Submit PR with detailed description
6. **Deploy**: Follow phased deployment strategy
7. **Monitor**: Track success metrics for 1 week

---

## References

### Related Documents

- [Names3Risk Training Gap Analysis](../4_analysis/2026-01-05_names3risk_training_gap_analysis.md) - Original gap identification
- [Names3Risk Training Infrastructure Plan](./2026-01-05_names3risk_training_infrastructure_implementation_plan.md) - Broader implementation plan
- [Names3Risk Component Correspondence](../4_analysis/2026-01-05_names3risk_pytorch_component_correspondence_analysis.md) - Component mapping
- [Names3Risk Model Design](../1_design/names3risk_model_design.md) - Architecture overview
- [Names3Risk PyTorch Reorganization](../1_design/names3risk_pytorch_reorganization_design.md) - Code organization

### Implementation Files

**Modified**:
- `projects/names3risk_pytorch/dockers/pytorch_training.py` - Main training script (4 modifications)

**Referenced (No Changes)**:
- `projects/names3risk_pytorch/dockers/scripts/tabular_preprocessing.py` - Text concatenation ‚úÖ
- `projects/names3risk_pytorch/dockers/scripts/tokenizer_training.py` - Tokenizer training ‚úÖ
- `projects/names3risk_pytorch/dockers/lightning_models/bimodal/pl_lstm2risk.py` - LSTM model ‚úÖ
- `projects/names3risk_pytorch/dockers/lightning_models/bimodal/pl_transformer2risk.py` - Transformer model ‚úÖ
- `projects/names3risk_pytorch/dockers/processing/dataloaders/names3risk_collate.py` - Collate functions ‚úÖ
- `projects/names3risk_pytorch/dockers/hyperparams/hyperparameters_lstm2risk.py` - LSTM hyperparams ‚úÖ
- `projects/names3risk_pytorch/dockers/hyperparams/hyperparameters_transformer2risk.py` - Transformer hyperparams ‚úÖ

### External Resources

- [PyTorch Lightning Documentation](https://lightning.ai/docs/pytorch/stable/)
- [HuggingFace Tokenizers](https://github.com/huggingface/tokenizers)
- [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html)
- [ONNX Export Guide](https://pytorch.org/docs/stable/onnx.html)

---

**Document Version**: 1.0  
**Created**: 2026-01-07  
**Last Updated**: 2026-01-07  
**Status**: Ready for Implementation
