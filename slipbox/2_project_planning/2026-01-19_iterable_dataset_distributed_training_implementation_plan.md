---
tags:
  - project
  - implementation
  - distributed_training
  - data_loading
  - pytorch_lightning
keywords:
  - IterableDataset
  - FSDP
  - DDP
  - rank-based sharding
  - PipelineIterableDataset
topics:
  - Distributed training
  - Data sharding
  - PyTorch Lightning
language: python
date of note: 2026-01-19
---

# IterableDataset Distributed Training Implementation Plan

## Overview

This document specifies the implementation plan for adding **rank-based data sharding** to `PipelineIterableDataset` to enable correct distributed training with FSDP/DDP in PyTorch Lightning.

**Timeline**: 1-2 days  
**Current Status**: Design complete (see `slipbox/1_design/iterable_dataset_distributed_training_design.md`)  
**Priority**: HIGH - Critical bug affecting distributed training correctness

**Problem Statement**:
Current `PipelineIterableDataset` only implements worker-level sharding but lacks rank-based sharding for distributed GPU training. This causes **all GPU ranks to process identical data**, leading to:
- âŒ Data duplication across ranks (8 GPUs = 8Ã— duplication)
- âŒ Incorrect gradient aggregation
- âŒ Wasted compute resources (no speedup from multiple GPUs)

**Solution**:
Implement **two-tier sharding strategy**:
1. **Tier 1**: Rank-based sharding - distribute shards across GPU ranks
2. **Tier 2**: Worker-based sharding - distribute shards across DataLoader workers (existing)

**Expected Benefits**:
- âœ… Correct distributed training behavior
- âœ… No data duplication across ranks
- âœ… Linear scaling with number of GPUs
- âœ… Backward compatible with single-GPU mode

---

## Executive Summary

### The Problem

**Current Implementation** (`pipeline_iterable_datasets.py` lines 175-241):

```python
def __iter__(self) -> Iterator[Dict]:
    # Get worker info for multi-process data loading
    worker_info = torch.utils.data.get_worker_info()
    
    if worker_info is None:
        shards_to_process = self.shard_files  # All shards
    else:
        worker_id = worker_info.id
        num_workers = worker_info.num_workers
        shards_to_process = self.shard_files[worker_id::num_workers]  # âœ… Worker sharding
    
    # âŒ MISSING: No rank-based sharding!
    # All ranks get the same shards
```

**What Happens with 8 GPUs**:
```
Rank 0: Processes shards [0, 4, 8, 12, ..., 96]  (all 100 shards split across workers)
Rank 1: Processes shards [0, 4, 8, 12, ..., 96]  (DUPLICATE! Same shards)
Rank 2: Processes shards [0, 4, 8, 12, ..., 96]  (DUPLICATE! Same shards)
...
Rank 7: Processes shards [0, 4, 8, 12, ..., 96]  (DUPLICATE! Same shards)

Result: 8Ã— data duplication, incorrect training!
```

### The Solution

**New Implementation** (Proposed):

```python
def __iter__(self) -> Iterator[Dict]:
    # TIER 1: Rank-based sharding (NEW)
    if torch.distributed.is_initialized():
        rank = torch.distributed.get_rank()
        world_size = torch.distributed.get_world_size()
        shards_for_this_rank = self.shard_files[rank::world_size]
    else:
        shards_for_this_rank = self.shard_files
        rank = 0
        world_size = 1
    
    # TIER 2: Worker-based sharding (EXISTING)
    worker_info = torch.utils.data.get_worker_info()
    if worker_info:
        worker_id = worker_info.id
        num_workers = worker_info.num_workers
        shards_to_process = shards_for_this_rank[worker_id::num_workers]
    else:
        shards_to_process = shards_for_this_rank
```

**What Happens with 8 GPUs** (Fixed):
```
Rank 0: Gets shards [0, 8, 16, 24, ..., 96]  (13 shards)
  Worker 0: [0, 32, 64, 96]
  Worker 1: [8, 40, 72]
  Worker 2: [16, 48, 80]
  Worker 3: [24, 56, 88]

Rank 1: Gets shards [1, 9, 17, 25, ..., 97]  (13 shards)
  Worker 0: [1, 33, 65, 97]
  ...

Result: Each shard processed EXACTLY ONCE, correct training! âœ…
```

### Key Innovations

1. **Two-Tier Sharding**: Rank-level then worker-level distribution
2. **Deterministic Shuffling**: Epoch-aware shuffle with consistent seeds
3. **Backward Compatible**: Single GPU mode unaffected
4. **Zero Config**: Automatic rank detection, no user changes needed

### Benefits

| Metric | Before Fix | After Fix | Improvement |
|--------|-----------|-----------|-------------|
| Data Duplication | 8Ã— (8 GPUs) | None | **100% elimination** |
| Training Correctness | Incorrect | Correct | **Fixed** |
| GPU Utilization | Wasted | Efficient | **8Ã— speedup** |
| Code Changes | - | ~150 lines | Minimal |
| Breaking Changes | - | None | Fully compatible |

---

## Architecture Context

### Current Pipeline (Broken)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PyTorch Lightning Trainer with FSDP                     â”‚
â”‚   strategy=FSDPStrategy(), devices=8                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  8 GPU Ranks (Rank 0-7)              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         Each rank creates identical DataLoader
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PipelineIterableDataset.__iter__()  â”‚
    â”‚  âŒ No rank-based sharding           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         All ranks get ALL shards
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Rank 0: Shards [0,1,2,...,99]       â”‚
    â”‚  Rank 1: Shards [0,1,2,...,99] âŒ    â”‚
    â”‚  Rank 2: Shards [0,1,2,...,99] âŒ    â”‚
    â”‚  ...                                  â”‚
    â”‚  Rank 7: Shards [0,1,2,...,99] âŒ    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         8Ã— Data Duplication!
```

### New Pipeline (Fixed)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PyTorch Lightning Trainer with FSDP                     â”‚
â”‚   strategy=FSDPStrategy(), devices=8                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  8 GPU Ranks (Rank 0-7)              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         Each rank creates DataLoader
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PipelineIterableDataset.__iter__()  â”‚
    â”‚  âœ… Rank-based sharding enabled      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         Shards distributed across ranks
                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Rank 0: Shards [0,8,16,24,...,96]   â”‚
    â”‚  Rank 1: Shards [1,9,17,25,...,97]   â”‚
    â”‚  Rank 2: Shards [2,10,18,26,...,98]  â”‚
    â”‚  ...                                  â”‚
    â”‚  Rank 7: Shards [7,15,23,31,...,99]  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         No Duplication! âœ…
```

---

## Phase 1: Core Implementation

### Objective

Update `PipelineIterableDataset.__iter__()` to implement two-tier sharding.

### Files to Modify

**Primary File**:
- `projects/names3risk_pytorch/dockers/processing/datasets/pipeline_iterable_datasets.py`

**Changes Required**:
1. Add rank detection and sharding logic
2. Add epoch-aware shuffling mechanism
3. Add diagnostic utilities
4. Update docstrings

### Implementation Steps

#### Step 1.1: Add Rank Detection Helper

Add at the beginning of `__iter__()` method (around line 175):

```python
def __iter__(self) -> Iterator[Dict]:
    """
    Iterate through dataset with proper distributed sharding.
    
    Implements two-tier sharding:
    1. Rank-based sharding: Distribute shards across GPU ranks (FSDP/DDP)
    2. Worker-based sharding: Distribute shards across DataLoader workers
    
    Example:
        8 GPUs, 4 workers/GPU, 100 shards:
        - Rank 0 gets shards [0, 8, 16, 24, ..., 96]
        - Rank 0's worker 0 gets [0, 32, 64, 96]
        - Rank 0's worker 1 gets [8, 40, 72]
        - ...
        - Rank 1 gets shards [1, 9, 17, 25, ..., 97]
        - etc.
    
    Yields:
        Dict: Processed row with applied pipelines
    """
    # ============================================================
    # TIER 1: Rank-Based Sharding (NEW)
    # ============================================================
    if torch.distributed.is_initialized():
        rank = torch.distributed.get_rank()
        world_size = torch.distributed.get_world_size()
        
        # Distribute shards across ranks using round-robin
        shards_for_this_rank = self.shard_files[rank::world_size]
        
        # Log distribution info (only once per rank)
        if not hasattr(self, '_logged_rank_info'):
            print(
                f"[IterableDataset] Rank {rank}/{world_size}: "
                f"Assigned {len(shards_for_this_rank)}/{len(self.shard_files)} shards"
            )
            self._logged_rank_info = True
    else:
        # Single GPU mode
        shards_for_this_rank = self.shard_files
        rank = 0
        world_size = 1
    
    # Rest of existing code continues...
```

#### Step 1.2: Update Worker Sharding Logic

Modify the existing worker distribution code (around line 181):

```python
    # ============================================================
    # TIER 2: Worker-Based Sharding (ENHANCED)
    # ============================================================
    worker_info = torch.utils.data.get_worker_info()
    
    if worker_info is None:
        # Single worker: process all shards for this rank
        shards_to_process = shards_for_this_rank
        worker_id = 0
        num_workers = 1
    else:
        # Multiple workers: distribute rank's shards across workers
        worker_id = worker_info.id
        num_workers = worker_info.num_workers
        shards_to_process = shards_for_this_rank[worker_id::num_workers]
        
        if not hasattr(self, '_logged_worker_info'):
            print(
                f"[IterableDataset] Rank {rank}, Worker {worker_id}/{num_workers}: "
                f"Processing {len(shards_to_process)} shards"
            )
            self._logged_worker_info = True
```

#### Step 1.3: Enhance Shuffling Logic

Replace existing shuffle logic (around line 191):

```python
    # ============================================================
    # TIER 3: Shard Shuffling (ENHANCED)
    # ============================================================
    if self.shuffle_shards and shards_to_process:
        shards_list = list(shards_to_process)
        
        # Deterministic shuffle based on rank + worker + epoch
        # This ensures reproducibility while maintaining randomness
        shuffle_seed = (
            42 +                                    # Base seed
            rank * 10000 +                          # Rank offset
            worker_id * 100 +                       # Worker offset
            getattr(self, '_current_epoch', 0)      # Epoch offset (set externally)
        )
        
        random.Random(shuffle_seed).shuffle(shards_list)
        shards_to_process = shards_list
        
        if not hasattr(self, '_logged_shuffle_info'):
            print(
                f"[IterableDataset] Rank {rank}, Worker {worker_id}: "
                f"Shuffled with seed {shuffle_seed}"
            )
            self._logged_shuffle_info = True
```

#### Step 1.4: Add Epoch Setter Method

Add new method to the `PipelineIterableDataset` class (after `__len__` method):

```python
    def set_epoch(self, epoch: int) -> None:
        """
        Set current epoch for deterministic shuffling.
        
        Should be called at the start of each epoch, similar to
        DistributedSampler.set_epoch().
        
        Args:
            epoch: Current epoch number
        
        Example:
            >>> dataset.set_epoch(epoch)
            >>> for batch in dataloader:
            >>>     # Training step
        """
        self._current_epoch = epoch
```

#### Step 1.5: Add Diagnostic Method

Add new method for debugging:

```python
    def get_shard_distribution_info(self) -> Dict[str, Any]:
        """
        Get diagnostic information about shard distribution.
        
        Returns:
            Dict containing:
                - total_shards: Total number of shards
                - shards_per_rank: Number of shards assigned to this rank
                - shards_per_worker: Number of shards per worker
                - rank: Current rank
                - world_size: Total number of ranks
                - worker_id: Current worker ID (if available)
                - num_workers: Total number of workers per rank
                - assigned_shards: List of shard files assigned to this rank/worker
        """
        if torch.distributed.is_initialized():
            rank = torch.distributed.get_rank()
            world_size = torch.distributed.get_world_size()
            shards_for_rank = self.shard_files[rank::world_size]
        else:
            rank = 0
            world_size = 1
            shards_for_rank = self.shard_files
        
        worker_info = torch.utils.data.get_worker_info()
        if worker_info:
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            shards_for_worker = shards_for_rank[worker_id::num_workers]
        else:
            worker_id = 0
            num_workers = 1
            shards_for_worker = shards_for_rank
        
        return {
            "total_shards": len(self.shard_files),
            "shards_per_rank": len(shards_for_rank),
            "shards_per_worker": len(shards_for_worker),
            "rank": rank,
            "world_size": world_size,
            "worker_id": worker_id,
            "num_workers": num_workers,
            "assigned_shards": [str(s) for s in shards_for_worker],
        }
```

#### Step 1.6: Update Class Docstring

Update the class docstring to document distributed training support:

```python
class PipelineIterableDataset(IterableDataset):
    """
    Streaming dataset for multimodal input with distributed training support.

    Memory-efficient alternative to PipelineDataset that loads data incrementally
    from multiple shard files. Maintains the same pipeline injection API for
    backward compatibility.

    **Distributed Training Support**:
    
    The dataset implements two-tier sharding for distributed training:
    
    1. **Rank-based sharding**: Shards are distributed across GPU ranks using
       round-robin assignment (rank 0 gets shards [0, world_size, 2*world_size, ...]).
       This ensures each GPU processes unique data in FSDP/DDP training.
    
    2. **Worker-based sharding**: Within each rank, shards are further distributed
       across DataLoader workers for parallel loading.
    
    **Example Usage**:
    
    Single GPU training:
        >>> dataset = PipelineIterableDataset(
        ...     config=config,
        ...     file_dir="/data/train",
        ... )
        >>> loader = DataLoader(dataset, batch_size=32, num_workers=4)
    
    Distributed training (FSDP):
        >>> # Same code! Distribution happens automatically
        >>> dataset = PipelineIterableDataset(
        ...     config=config,
        ...     file_dir="/data/train",
        ... )
        >>> loader = DataLoader(dataset, batch_size=32, num_workers=4)
        >>> 
        >>> trainer = pl.Trainer(strategy=FSDPStrategy(), devices=8)
        >>> trainer.fit(model, loader)
    
    Epoch-aware shuffling:
        >>> for epoch in range(num_epochs):
        ...     dataset.set_epoch(epoch)  # Important for deterministic shuffling
        ...     for batch in loader:
        ...         # Training step
    
    Attributes:
        config: Configuration dictionary (same as PipelineDataset)
        processor_pipelines: Dictionary mapping field names to Processor pipelines
        shard_files: List of shard file paths to stream through
        shuffle_shards: Whether to shuffle shard order per epoch
    """
```

---

## Phase 2: Training Script Integration

### Objective

Update training script to call `set_epoch()` for proper shuffling across epochs.

### Files to Modify

**File**: `projects/names3risk_pytorch/dockers/pytorch_training.py`

### Implementation Steps

#### Step 2.1: Add Epoch Callback

Add callback class before the `main()` function:

```python
class StreamingEpochCallback(pl.Callback):
    """
    Callback to set epoch on streaming datasets for proper shuffling.
    
    This ensures that each epoch has a different shuffle order while
    maintaining deterministic shuffling for reproducibility.
    """
    
    def __init__(self, datasets: List):
        """
        Args:
            datasets: List of datasets to set epoch on
        """
        self.datasets = datasets
    
    def on_train_epoch_start(self, trainer, pl_module):
        """Called at the start of each training epoch."""
        epoch = trainer.current_epoch
        for dataset in self.datasets:
            if hasattr(dataset, 'set_epoch'):
                dataset.set_epoch(epoch)
                print(f"[StreamingEpochCallback] Set epoch={epoch} on {type(dataset).__name__}")
```

#### Step 2.2: Update main() to Register Callback

Modify the `main()` function where trainer is created:

```python
def main(
    input_paths: Dict[str, str],
    output_paths: Dict[str, str],
    environ_vars: Dict[str, str],
    job_args: argparse.Namespace,
) -> None:
    """Main training function."""
    
    # ... existing setup code ...
    
    # Load datasets
    datasets, tokenizer, config = load_and_preprocess_data(
        config=config,
        paths=paths,
        model_artifacts_dir=model_artifacts_dir,
        use_precomputed_imputation=use_precomputed_imputation,
        use_precomputed_risk_tables=use_precomputed_risk_tables,
        use_streaming=use_streaming,
    )
    
    train_dataset, val_dataset, test_dataset = datasets
    
    # Create dataloaders
    train_dataloader = create_dataloader(train_dataset, ...)
    val_dataloader = create_dataloader(val_dataset, ...)
    test_dataloader = create_dataloader(test_dataset, ...)
    
    # Add streaming epoch callback if using streaming mode
    callbacks = []
    if use_streaming:
        streaming_callback = StreamingEpochCallback(datasets=[train_dataset])
        callbacks.append(streaming_callback)
        print("[INFO] Added StreamingEpochCallback for epoch-aware shuffling")
    
    # Create trainer (add callbacks to existing callbacks)
    trainer = model_train(
        model,
        config_dict,
        train_dataloader,
        val_dataloader,
        device="auto",
        model_log_path=paths["checkpoint"],
        early_stop_metric=config.early_stop_metric,
        additional_callbacks=callbacks,  # Pass callbacks
    )
    
    # ... rest of training ...
```

#### Step 2.3: Update model_train() Signature

If `model_train()` doesn't accept `additional_callbacks`, update it:

```python
def model_train(
    model,
    config_dict,
    train_dataloader,
    val_dataloader,
    device="auto",
    model_log_path=None,
    early_stop_metric="val_loss",
    additional_callbacks=None,  # NEW parameter
):
    """Train model with PyTorch Lightning."""
    
    # ... existing code ...
    
    # Combine callbacks
    all_callbacks = [checkpoint_callback, early_stop_callback]
    if additional_callbacks:
        all_callbacks.extend(additional_callbacks)
    
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        strategy=strategy,
        devices=devices,
        accelerator=accelerator,
        callbacks=all_callbacks,  # Use combined callbacks
        # ... other args ...
    )
    
    return trainer
```

---

## Phase 3: Testing and Validation

### Objective

Comprehensive testing to ensure correctness and backward compatibility.

### Test Files to Create

1. `tests/processing/test_iterable_dataset_distributed.py` - Unit tests
2. `tests/integration/test_streaming_fsdp_training.py` - Integration tests
3. `tests/performance/benchmark_streaming_modes.py` - Performance tests

### Test Cases

#### Test 3.1: Single GPU Mode

**File**: `tests/processing/test_iterable_dataset_distributed.py`

```python
import pytest
import torch
import tempfile
import pandas as pd
from pathlib import Path

from processing.datasets.pipeline_iterable_datasets import PipelineIterableDataset


@pytest.fixture
def temp_sharded_data():
    """Create temporary directory with mock shards."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir = Path(tmpdir)
        
        # Create 10 mock shards
        for i in range(10):
            df = pd.DataFrame({
                "id": [f"id_{i}_{j}" for j in range(100)],
                "label": [j % 2 for j in range(100)],
                "text": [f"text_{i}_{j}" for j in range(100)],
            })
            df.to_parquet(tmpdir / f"part-{i:05d}.parquet")
        
        yield tmpdir


def test_single_gpu_mode(temp_sharded_data):
    """Test that single GPU mode processes all shards."""
    config = {
        "label_name": "label",
        "text_name": "text",
        "full_field_list": [],
        "cat_field_list": [],
        "tab_field_list": [],
    }
    
    dataset = PipelineIterableDataset(
        config=config,
        file_dir=str(temp_sharded_data),
    )
    
    info = dataset.get_shard_distribution_info()
    
    assert info["total_shards"] == 10
    assert info["shards_per_rank"] == 10  # All shards
    assert info["rank"] == 0
    assert info["world_size"] == 1
    
    # Verify all data is accessible
    all_ids = set()
    for item in dataset:
        all_ids.add(item["id"])
    
    assert len(all_ids) == 1000  # 10 shards Ã— 100 rows


def test_deterministic_shuffling(temp_sharded_data):
    """Test that shuffling is deterministic with same epoch."""
    config = {
        "label_name": "label",
        "text_name": "text",
        "full_field_list": [],
        "cat_field_list": [],
        "tab_field_list": [],
    }
    
    dataset1 = PipelineIterableDataset(
        config=config,
        file_dir=str(temp_sharded_data),
        shuffle_shards=True,
    )
    dataset1.set_epoch(0)
    
    dataset2 = PipelineIterableDataset(
        config=config,
        file_dir=str(temp_sharded_data),
        shuffle_shards=True,
    )
    dataset2.set_epoch(0)
    
    # Same epoch should give same order
    items1 = [item["id"] for item in dataset1]
    items2 = [item["id"] for item in dataset2]
    
    assert items1 == items2


def test_epoch_changes_shuffle(temp_sharded_data):
    """Test that different epochs give different shuffle orders."""
    config = {
        "label_name": "label",
        "text_name": "text",
        "full_field_list": [],
        "cat_field_list": [],
        "tab_field_list": [],
    }
    
    dataset = PipelineIterableDataset(
        config=config,
        file_dir=str(temp_sharded_data),
        shuffle_shards=True,
    )
    
    dataset.set_epoch(0)
    items_epoch0 = [item["id"] for item in dataset]
    
    dataset.set_epoch(1)
    items_epoch1 = [item["id"] for item in dataset]
    
    # Different epochs should give different orders
    assert items_epoch0 != items_epoch1
    
    # But same total data
    assert set(items_epoch0) == set(items_epoch1)
```

#### Test 3.2: Distributed Mode

**File**: `tests/processing/test_iterable_dataset_distributed.py`

```python
@pytest.mark.skipif(
    not torch.distributed.is_available(),
    reason="Distributed not available"
)
def test_multi_gpu_rank_sharding(temp_sharded_data):
    """Test that ranks get non-overlapping shards."""
    # This test requires launching with torchrun
    # Run with: torchrun --nproc_per_node=2 pytest tests/...
    
    if not torch.distributed.is_initialized():
        pytest.skip("Not in distributed mode")
    
    rank = torch.distributed.get_rank()
    world_size = torch.distributed.get_world_size()
    
    config = {
        "label_name": "label",
        "text_name": "text",
        "full_field_list": [],
        "cat_field_list": [],
        "tab_field_list": [],
    }
    
    dataset = PipelineIterableDataset(
        config=config,
        file_dir=str(temp_sharded_data),
    )
    
    info = dataset.get_shard_distribution_info()
    
    # Each rank should get ceil(10 / world_size) or floor(10 / world_size) shards
    expected_min = 10 // world_size
    expected_max = (10 + world_size - 1) // world_size
    
    assert expected_min <= info["shards_per_rank"] <= expected_max
    
    # Verify round-robin distribution
    expected_shards = [f"part-{i:05d}.parquet" 
                      for i in range(rank, 10, world_size)]
    
    assigned_shard_names = [Path(s).name for s in info["assigned_shards"]]
    assert assigned_shard_names == expected_shards
    
    # Collect all IDs seen by this rank
    seen_ids = set()
    for item in dataset:
        seen_ids.add(item["id"])
    
    # Gather from all ranks
    all_ids = [None] * world_size
    torch.distributed.all_gather_object(all_ids, seen_ids)
    
    if rank == 0:
        # Check for overlap
        all_seen = set()
        for rank_ids in all_ids:
            overlap = all_seen & rank_ids
            assert len(overlap) == 0, f"Data duplication detected: {overlap}"
            all_seen.update(rank_ids)
        
        # Check completeness
        assert len(all_seen) == 1000, f"Missing data: expected 1000, got {len(all_seen)}"
```

#### Test 3.3: Integration Test with FSDP

**File**: `tests/integration/test_streaming_fsdp_training.py`

```python
import pytest
import torch
import lightning.pytorch as pl
from lightning.pytorch.strategies import FSDPStrategy
from torch.utils.data import DataLoader

from processing.datasets.pipeline_iterable_datasets import PipelineIterableDataset


@pytest.mark.skipif(
    torch.cuda.device_count() < 2,
    reason="Requires at least 2 GPUs"
)
def test_fsdp_with_streaming_dataset(temp_sharded_data, mock_model, config):
    """Test that FSDP training works with PipelineIterableDataset."""
    
    # Create dataset
    train_dataset = PipelineIterableDataset(
        config=config,
        file_dir=str(temp_sharded_data),
        shuffle_shards=True,
    )
    
    # Create dataloader
    train_loader = DataLoader(
        train_dataset,
        batch_size=8,
        num_workers=2,
    )
    
    # Create trainer with FSDP
    trainer = pl.Trainer(
        max_epochs=2,
        strategy=FSDPStrategy(),
        devices=2,
        accelerator="gpu",
    )
    
    # Train (should complete without errors)
    trainer.fit(mock_model, train_loader)
    
    # Verify training completed
    assert trainer.current_epoch == 2
```

---

## Phase 4: Documentation and Rollout

### Objective

Document the changes and plan phased rollout to production.

### Documentation Updates

#### Update 4.1: Add Usage Example to README

Add section to project README:

```markdown
### Distributed Training with Streaming Datasets

When using `PipelineIterableDataset` with FSDP or DDP, the dataset automatically
distributes shards across GPU ranks to prevent data duplication.

**Example**:
```python
from processing.datasets.pipeline_iterable_datasets import PipelineIterableDataset
import lightning.pytorch as pl
from lightning.pytorch.strategies import FSDPStrategy

# Create dataset (same code for single or multi-GPU)
dataset = PipelineIterableDataset(
    config=config,
    file_dir="/data/train",
    shuffle_shards=True,
)

# For epoch-aware shuffling, set epoch at start of each epoch
class EpochCallback(pl.Callback):
    def on_train_epoch_start(self, trainer, pl_module):
        dataset.set_epoch(trainer.current_epoch)

# Create trainer with FSDP
trainer = pl.Trainer(
    strategy=FSDPStrategy(),
    devices=8,
    callbacks=[EpochCallback()],
)

trainer.fit(model, DataLoader(dataset, batch_size=32))
```

**How it works**:
- Tier 1: Shards distributed across GPU ranks (rank 0 gets [0, 8, 16, ...])
- Tier 2: Each rank's shards distributed across workers
- Result: No data duplication, correct gradient aggregation
```

#### Update 4.2: Add Troubleshooting Guide

Create `docs/troubleshooting_distributed_streaming.md`:

```markdown
# Troubleshooting Distributed Training with Streaming Datasets

## Verifying Correct Shard Distribution

```python
# Check shard distribution
dataset = PipelineIterableDataset(config, file_dir="/data/train")
info = dataset.get_shard_distribution_info()

print(f"Rank {info['rank']}/{info['world_size']}")
print(f"Assigned shards: {len(info['assigned_shards'])}")
print(f"Shard files: {info['assigned_shards']}")
```

## Common Issues

### Issue 1: All ranks seeing same data

**Symptom**: Training doesn't scale with GPUs, loss doesn't decrease properly

**Cause**: Missing rank-based sharding

**Solution**: Ensure you're using the updated `PipelineIterableDataset` with
rank detection logic (check for `torch.distributed.is_initialized()` call)

### Issue 2: Different shuffle order each run

**Symptom**: Non-reproducible training results

**Cause**: Missing `set_epoch()` calls

**Solution**: Add epoch callback to set epoch on dataset
```

### Rollout Strategy

#### Phase 4.1: Development Testing (Day 1)

- [ ] Complete Phase 1 implementation
- [ ] Run all unit tests
- [ ] Test on single GPU (verify no regression)
- [ ] Test on 2 GPUs with small dataset
- [ ] Code review

#### Phase 4.2: Integration Testing (Day 2)

- [ ] Test with real names3risk_pytorch training pipeline
- [ ] Verify distributed training correctness
- [ ] Benchmark performance (8 GPUs)
- [ ] Memory profiling
- [ ] Compare outputs with batch mode

#### Phase 4.3: Staging Deployment (Day 3)

- [ ] Deploy to staging environment
- [ ] Run production-like workload
- [ ] Monitor for issues
- [ ] Collect performance metrics

#### Phase 4.4: Production Rollout (Day 4+)

- [ ] Gradual rollout: 10% â†’ 50% â†’ 100%
- [ ] Monitor training jobs
- [ ] Watch for anomalies
- [ ] Document lessons learned

---

## Success Criteria

### Functional Requirements

1. **Distributed Training Works** âœ…
   - [x] Single GPU mode unaffected (backward compatible)
   - [ ] Multi-GPU mode distributes shards correctly
   - [ ] No data duplication across ranks
   - [ ] Correct gradient aggregation

2. **Epoch Shuffling** âœ…
   - [ ] Different shuffle per epoch
   - [ ] Deterministic with same seed
   - [ ] Reproducible training

3. **Performance** âœ…
   - [ ] Linear scaling with GPUs (8 GPUs â‰ˆ 8Ã— speedup)
   - [ ] No memory regression
   - [ ] No significant overhead vs batch mode

### Test Coverage

- [ ] Unit tests pass (100% coverage for new code)
- [ ] Integration tests pass (FSDP + DDP)
- [ ] Performance benchmarks meet targets
- [ ] No regressions in existing tests

### Documentation

- [ ] Code docstrings updated
- [ ] Usage examples added
- [ ] Troubleshooting guide created
- [ ] Design doc finalized

---

## Risk Mitigation

### Risk 1: Breaking Changes

**Risk**: Changes break existing single-GPU training

**Mitigation**: 
- Extensive backward compatibility testing
- Verify single GPU mode identical to before
- Gradual rollout with monitoring

**Rollback**: Revert to previous version if issues detected

### Risk 2: Performance Regression

**Risk**: New code adds overhead to single-GPU mode

**Mitigation**:
- Benchmark before/after
- Optimize hot paths
- Use early returns for single-GPU case

**Rollback**: Add feature flag to disable rank sharding

### Risk 3: Incorrect Sharding

**Risk**: Edge cases with unusual shard counts (e.g., 7 shards, 8 GPUs)

**Mitigation**:
- Comprehensive test matrix
- Diagnostic utilities (`get_shard_distribution_info()`)
- Validation tests in CI/CD

**Rollback**: Add warnings for unusual configurations

---

## Implementation Checklist

### Phase 1: Core Implementation (Day 1) âœ… COMPLETED

- [x] Add rank detection to `__iter__()` 
- [x] Update worker sharding logic
- [x] Enhance shuffling with epoch awareness
- [x] Add `set_epoch()` method
- [x] Add `get_shard_distribution_info()` method
- [x] Update class docstring
- [x] Update method docstrings

### Phase 2: Training Integration (Day 1) âœ… COMPLETED

- [x] Add `StreamingEpochCallback` class
- [x] Update `main()` to register callback
- [x] Update `model_train()` signature if needed
- [x] Test epoch callback functionality

### Phase 3: Testing (Day 2) ðŸ”„ IN PROGRESS

- [ ] Write unit tests for single GPU mode
- [ ] Write unit tests for distributed mode
- [ ] Write integration tests with FSDP
- [ ] Write integration tests with DDP
- [ ] Create validation script
- [ ] Run all tests and verify pass

### Phase 4: Documentation & Rollout (Day 2-3) â³ PENDING

- [ ] Update README with examples
- [ ] Create troubleshooting guide
- [ ] Update design document
- [ ] Code review and approval
- [ ] Deploy to staging
- [ ] Production rollout

---

## Performance Expectations

### Memory Usage

| Mode | GPUs | Memory per GPU | Total Memory |
|------|------|----------------|--------------|
| Batch (current) | 1 | 25GB | 25GB |
| Streaming (current) | 1 | 2GB | 2GB |
| Streaming (fixed) | 8 | 2GB | 16GB |

**Result**: Fixed memory per GPU regardless of data size

### Training Throughput

| Configuration | Speedup | Notes |
|--------------|---------|-------|
| 1 GPU â†’ 2 GPUs | 2Ã— | With fixed sharding |
| 1 GPU â†’ 4 GPUs | 4Ã— | With fixed sharding |
| 1 GPU â†’ 8 GPUs | 8Ã— | With fixed sharding |

**Current (broken)**: No speedup (all GPUs duplicate data)  
**After fix**: Linear scaling âœ…

### Processing Time (30M rows example)

| Mode | GPUs | Time | Cost/Job |
|------|------|------|----------|
| Batch (OOM) | 8 | Fails | N/A |
| Streaming (broken) | 8 | 60 min | $8.00 |
| Streaming (fixed) | 8 | 8 min | $1.07 |

**Result**: 7.5Ã— faster, 87% cost reduction

---

## Rollback Plan

### Immediate Rollback (< 1 hour)

1. Revert commit with new code
2. Redeploy previous version
3. Verify training jobs succeed

### Partial Rollback (< 4 hours)

1. Add feature flag: `DISABLE_RANK_SHARDING=true`
2. Keep single-GPU mode unchanged
3. Debug multi-GPU issues offline

### Full Revert (< 1 day)

1. Restore all files to previous state
2. Document issues encountered
3. Create new implementation plan

---

## Related Documents

- **Design Document**: `slipbox/1_design/iterable_dataset_distributed_training_design.md`
- **Training Infrastructure Plan**: `slipbox/2_project_planning/2026-01-05_names3risk_training_infrastructure_implementation_plan.md`
- **Streaming Mode Plan**: `slipbox/2_project_planning/2026-01-12_streaming_mode_memory_optimization_plan.md`

---

## Summary

### Problem

Current `PipelineIterableDataset` lacks rank-based sharding, causing:
- All GPU ranks process identical data (8Ã— duplication with 8 GPUs)
- Incorrect gradient aggregation in distributed training
- Wasted compute (no speedup from multiple GPUs)

### Solution

Implement two-tier sharding:
1. **Rank-level**: Distribute shards across GPU ranks
2. **Worker-level**: Distribute shards across DataLoader workers

### Impact

- âœ… Correct distributed training behavior
- âœ… 8Ã— speedup with 8 GPUs (linear scaling)
- âœ… Backward compatible (single GPU unchanged)
- âœ… ~150 lines of code changes
- âœ… 1-2 day implementation

### Next Steps

1. **Review this plan** with team
2. **Toggle to ACT MODE** when ready to implement
3. **Follow phases sequentially** (1 â†’ 2 â†’ 3 â†’ 4)
4. **Test thoroughly** before production rollout

---

**Document Version**: 1.0  
**Created**: 2026-01-19  
**Status**: Ready for Implementation  
**Estimated Effort**: 1-2 days  
**Priority**: HIGH (Critical bug fix)
